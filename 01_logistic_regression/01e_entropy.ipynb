{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Entropy? (Bonus)\n",
    "\n",
    "Entropy is a measure of randomness. It is defined as:\n",
    "\n",
    "\\\\[\n",
    "H(X) = \\sum_x - \\prob{X = x} \\log \\prob{X = x}\n",
    "\\\\]\n",
    "\n",
    "What is this? It is the *expected* negative log probability from a sample of the random variable $X$.\n",
    "\n",
    "So what is the negative log probability?\n",
    "\n",
    "Well, it turns out that the negative log probability is the length of the ideal length of a code to encode samples from $X$. Let me explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding, Huffman Codes\n",
    "\n",
    "Words in English that are very common tend to be short. Words that are less common are longer. That is efficient. That means that the average English text tends to be shorter.\n",
    "\n",
    "It would be great if *every* word could be just a few letters, but there aren't enough short sequences of letters. For instance, if we're encoding words into binary, there are only $2^8 = 256$ words that are 8 bits long. Therefore, some need to be longer than others. So making the common ones short makes sense.\n",
    "\n",
    "What is the *ideal* coding of English? That is, how would you assign English words binary codes so that a random sample of English text would be written down with the shortest possible length?\n",
    "\n",
    "One such way to build a coding is the [Huffman Coding][huffman-coding] approach. I won't explain how that is done here, but you can look it up and read about it on Wikipedia.\n",
    "\n",
    "Imagine that for every word in the English language $w_i$, that $\\prob{w_i} = 2^{-k_i}$. That is, the probability of every word is the inverse of a power of two.\n",
    "\n",
    "Then the Huffman code would give every word $w_i$ a code of length $k_i$. You can verify this for yourself. Note that $k_i = -\\log_2 \\prob{w_i}$. Oooh...\n",
    "\n",
    "### Optimal Code Length\n",
    "\n",
    "Now, when the probabilities of words aren't always an inverse power of two, the Huffman coding will not always assign codes of length $-\\log_2 \\prob{w_i}$. That makes sense, because when $\\prob{w_i}$ isn't an inverse power of two, then this isn't an integer, and a code length must always be an integer.\n",
    "\n",
    "Still, it turns out that if you build multi-word Huffman Codes codes, which encode pairs of words together with a code, then the Huffman code for pairs will be closer to $-\\log_2 \\prob{w_i w_j}$ (the probability a pair of words appears one after the other).\n",
    "\n",
    "If you then do triples, you get closer. And the more words you encode together, the more the Huffman code length approaches that negative log probability.\n",
    "\n",
    "Therefore, I will say that the negative log probability is the best code length, even when it isn't an integer.\n",
    "\n",
    "[huffman-coding]: https://en.wikipedia.org/wiki/Huffman_coding#Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Is Expected Code Length\n",
    "\n",
    "Returning to entropy, since it is the expected negative log probability of an event (like a word), when randomly chosen, then I say this is the expected length of the code for a sample from the variable.\n",
    "\n",
    "Which is to say, that if I sample 100 events from $X$, I expect to be able to encode this with $100 H(X)$ bits. By the law of large numbers, this should converge as I draw more samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "Now that we know what Entropy is, then what is Cross Entropy?\n",
    "\n",
    "Cross entropy is:\n",
    "\n",
    "\\\\[\n",
    "\\sum_x \\pprob{p}{X = x} -\\log \\pprob{q}{X = x}\n",
    "\\\\]\n",
    "\n",
    "Here, I'm talking about two *probability distributions* over events from $X$.\n",
    "\n",
    "Let's call $p$ the *true* distribution. Let's call $q$ the *learned* distribution. That is, let's say that I *think* that the probability distribution is $q$, but in reality it is $p$.\n",
    "\n",
    "We know what the entropy would be if the true distribution were really $q$:\n",
    "\n",
    "\\\\[\n",
    "\\sum_x - \\pprob{q}{X = x} \\log \\pprob{q}{X = x}\n",
    "\\\\]\n",
    "\n",
    "That's because in the best coding for a distribution $q$, we'd use codes of length $-\\log \\pprob{q}{X = x}$ for each possible sample $x$. The average code length comes from doing a weighted sum over those code lengths.\n",
    "\n",
    "The problem is that if we build the best distribution for $q$, the events will still come from the *true* distribution $p$. Therefore, the expected value is:\n",
    "\n",
    "\\\\[\n",
    "\\sum_x - \\pprob{p}{X = x} \\log \\pprob{q}{X = x}\n",
    "\\\\]\n",
    "\n",
    "This says: it's the expected code length when drawing from a distribution $p$ and using the best coding for the distribution $q$.\n",
    "\n",
    "This is the cross-entropy. It is often written $H(p, q)$. By definition, $H(p, q) \\geq H(p)$. That's because if you know the true distribution, you'll use the best encoding for that distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy vs KL Divergence\n",
    "\n",
    "Cross Entropy can be high for two reasons:\n",
    "\n",
    "1. The learned distribution $q$ might be very different than the true distribution $p$.\n",
    "2. Perhaps $q = p$, but the problem is that the distribution is intrinsically very random. That is, $H(p)$ is itself very high.\n",
    "\n",
    "Therefore, cross entropy isn't a totally fair measure of how much $q$ misunderstands $p$. To measure this, we subtract out the entropy of $p$:\n",
    "\n",
    "\\\\[\n",
    "D_{\\text{KL}}(p, q) := H(p, q) - H(p)\n",
    "\\\\]\n",
    "\n",
    "This is called the *KL Divergence*. It is the average number of *extra bits* required to encode a sample from the true distribution $p$ using the best encoding for the learned distribution $q$.\n",
    "\n",
    "Now, when $p = q$, then $H(p, q) = H(p)$ per the above, so $D_{\\text{KL}}(p, q) = 0$. That makes sense: when you use the best encoding for the right distribution, then by definition you use no extra bits you don't truly need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there a way to explain in terms of likelihood or something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
