{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Entropy? (Bonus)\n",
    "\n",
    "Entropy is a measure of randomness. It is defined as:\n",
    "\n",
    "\\\\[\n",
    "H(X) = \\sum_x - \\prob{X = x} \\log \\prob{X = x}\n",
    "\\\\]\n",
    "\n",
    "What is this? It is the *expected* negative log probability from a sample of the random variable $X$.\n",
    "\n",
    "So what is the negative log probability?\n",
    "\n",
    "Well, it turns out that the negative log probability is the length of the ideal length of a code to encode samples from $X$. Let me explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding, Huffman Codes\n",
    "\n",
    "Words in English that are very common tend to be short. Words that are less common are longer. That is efficient. That means that the average English text tends to be shorter.\n",
    "\n",
    "It would be great if *every* word could be just a few letters, but there aren't enough short sequences of letters. For instance, if we're encoding words into binary, there are only $2^8 = 256$ words that are 8 bits long. Therefore, some need to be longer than others. So making the common ones short makes sense.\n",
    "\n",
    "What is the *ideal* coding of English? That is, how would you assign English words binary codes so that a random sample of English text would be written down with the shortest possible length?\n",
    "\n",
    "One such way to build a coding is the [Huffman Coding][huffman-coding] approach. I won't explain how that is done here, but you can look it up and read about it on Wikipedia.\n",
    "\n",
    "Imagine that for every word in the English language $w_i$, that $\\prob{w_i} = 2^{-k_i}$. That is, the probability of every word is the inverse of a power of two.\n",
    "\n",
    "Then the Huffman code would give every word $w_i$ a code of length $k_i$. You can verify this for yourself. Note that $k_i = -\\log_2 \\prob{w_i}$. Oooh...\n",
    "\n",
    "### Optimal Code Length\n",
    "\n",
    "Now, when the probabilities of words aren't always an inverse power of two, the Huffman coding will not always assign codes of length $-\\log_2 \\prob{w_i}$. That makes sense, because when $\\prob{w_i}$ isn't an inverse power of two, then this isn't an integer, and a code length must always be an integer.\n",
    "\n",
    "Still, it turns out that if you build multi-word Huffman Codes codes, which encode pairs of words together with a code, then the Huffman code for pairs will be closer to $-\\log_2 \\prob{w_i w_j}$ (the probability a pair of words appears one after the other).\n",
    "\n",
    "If you then do triples, you get closer. And the more words you encode together, the more the Huffman code length approaches that negative log probability.\n",
    "\n",
    "Therefore, I will say that the negative log probability is the best code length, even when it isn't an integer.\n",
    "\n",
    "[huffman-coding]: https://en.wikipedia.org/wiki/Huffman_coding#Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Is Expected Code Length\n",
    "\n",
    "Returning to entropy, since it is the expected negative log probability of an event (like a word), when randomly chosen, then I say this is the expected length of the code for a sample from the variable (using the best coding for that probability distribution).\n",
    "\n",
    "Which is to say, that if I sample 100 events from $X$, I expect to be able to encode this with $100 H(X)$ bits. By the law of large numbers, this should converge as I draw more samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "Now that we know what Entropy is, then what is Cross Entropy?\n",
    "\n",
    "Cross entropy is:\n",
    "\n",
    "\\\\[\n",
    "\\sum_x -\\pprob{p}{X = x} \\log \\pprob{q}{X = x}\n",
    "\\\\]\n",
    "\n",
    "Here, I'm talking about two *probability distributions* over events from $X$.\n",
    "\n",
    "Let's call $p$ the *true* distribution. Let's call $q$ the *learned* distribution. That is, let's say that I *think* that the probability distribution is $q$, but in reality it is $p$.\n",
    "\n",
    "We know what the entropy would be if the true distribution were really $q$:\n",
    "\n",
    "\\\\[\n",
    "\\sum_x - \\pprob{q}{X = x} \\log \\pprob{q}{X = x}\n",
    "\\\\]\n",
    "\n",
    "That's because in the best coding for a distribution $q$, we'd use codes of length $-\\log \\pprob{q}{X = x}$ for each possible sample $x$. The average code length comes from doing a weighted sum over those code lengths.\n",
    "\n",
    "The problem is that if we build the best distribution for $q$, the events will still come from the *true* distribution $p$. Therefore, the expected value is:\n",
    "\n",
    "\\\\[\n",
    "\\sum_x - \\pprob{p}{X = x} \\log \\pprob{q}{X = x}\n",
    "\\\\]\n",
    "\n",
    "This says: it's the expected code length when drawing from a distribution $p$ and using the best coding for the distribution $q$.\n",
    "\n",
    "This is the cross-entropy. It is often written $H(p, q)$. By definition, $H(p, q) \\geq H(p)$. That's because if you know the true distribution, you'll use the best encoding for that distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy vs KL Divergence\n",
    "\n",
    "Cross Entropy can be high for two reasons:\n",
    "\n",
    "1. The learned distribution $q$ might be very different than the true distribution $p$.\n",
    "2. Perhaps $q = p$, but the problem is that the distribution is intrinsically very random. That is, $H(p)$ is itself very high.\n",
    "\n",
    "Therefore, cross entropy isn't a totally fair measure of how much $q$ misunderstands $p$. To measure this, we subtract out the entropy of $p$:\n",
    "\n",
    "\\\\[\n",
    "D_{\\text{KL}}(p, q) := H(p, q) - H(p)\n",
    "\\\\]\n",
    "\n",
    "This is called the *KL Divergence*. It is the average number of *extra bits* required to encode a sample from the true distribution $p$ using the best encoding for the learned distribution $q$.\n",
    "\n",
    "Now, when $p = q$, then $H(p, q) = H(p)$ per the above, so $D_{\\text{KL}}(p, q) = 0$. That makes sense: when you use the best encoding for the right distribution, then by definition you use no extra bits you don't truly need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy And Maximum Likelihood\n",
    "\n",
    "The nature of a *probabilistic* model is that it learns to estimate a probability distribution. For instance, in the case of Logistic Regression, the model is learning to estimate:\n",
    "\n",
    "\\\\[\n",
    "\\prob{Y = 1 \\condbar X = x}\n",
    "\\\\]\n",
    "\n",
    "Even linear regression has an interpretation where it is learning a conditional probability distribution:\n",
    "\n",
    "\\\\[\n",
    "\\prob{Y = y \\condbar X = x}\n",
    "=\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n",
    "e^{\n",
    "    \\frac{\n",
    "        (y - y\\hat)^2\n",
    "    }{\n",
    "        2\\sigma^2\n",
    "    }\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "As we know, choosing the $\\theta$ that maximzes the probability of the dataset $\\prob{\\theta}{\\mathcal{D}}$ is the same as the $\\theta$ that maximizes $\\log\\pprob{\\theta}{\\mathcal{D}}$ is the same as *minimizing* $-\\log\\pprob{\\theta}{\\mathcal{D}}$.\n",
    "\n",
    "Now that we know about entropy, we know that this negative log probability of the dataset is the same as the length of the encoding of the results of $\\mathcal{D}$ using the very best coding for the probability distribution $\\Pr_\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative log probability can be considered as an *estimate* of the cross entropy between the true and learned distributions. That is:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    -\\log\\pprob{\\theta}{\\mathcal{D}}\n",
    "}{\n",
    "    N\n",
    "}\n",
    "\\approx\n",
    "H(\\text{true distribution}, \\text{learned distribution})\n",
    "\\\\]\n",
    "\n",
    "Therefore, the negative log probability is in a sense *always* an estimate of the cross entropy. However, it is common to reserve the term *cross entropy error* for only the error used for classification problems:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log\n",
    "\\pprob{\\theta}{\\mathcal{D}}\n",
    "&=\n",
    "\\sum_i^N\n",
    "    y^i\n",
    "    \\log\n",
    "    \\pprob{\\theta}{Y = 1 \\condbar X = x^i}\n",
    "+\n",
    "    (1 - y^i)\n",
    "    \\pprob{\\theta}{Y = 0 \\condbar X = x^i}\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Not Minimize KL Divergence?\n",
    "\n",
    "You might ask why we would want to minimize an estimate of cross entropy rather than the KL divergence. Isn't cross entropy an \"unfair\" measure of the difference between two distributions? Isn't KL divergence \"fairer?\"\n",
    "\n",
    "But remember: $H(p, q) = H(p) + D_\\mathcal{KL}(p, q)$. So the choice of $q$ that minimizes cross entropy must by definition minimize the KL divergence, since nothing can change $H(p)$ which is fixed.\n",
    "\n",
    "Let me make a few other points.\n",
    "\n",
    "As we gain more data, we get a better approximation of the cross entropy. That's because the cross entropy is the expected code length of samples from $p$ using the best encoding for $q$. The more samples from $p$, the better we can approximate this.\n",
    "\n",
    "The cross entropy is an upper bound on the entropy of the true distribution $p$. As we gain more data, we should converge to a learned distribution $q$ which is the *best* $q$. However, the space of models we are considering is always restricted. For instance, with logistic regression we consider only models which can be written as $\\prob{Y = y \\condbar X = x} = \\sigma\\left(\\theta_0 + \\sum_i \\theta_i x_i\\right)$. Not every distribution may be written this way.\n",
    "\n",
    "Therefore, the true distribution $p$ may not be amongst our class of models under consideration. If $p$ *is* in the true model class, the cross entropy should converge to the true entropy as the KL divergence falls to zero. That's because $q$ will converge to $p$.\n",
    "\n",
    "On the other hand, if $p$ is not in the model class, then $q$ will *not* converge to $q$. It will instead converge to the *best approximation* $q^*$, which is the $q*$ that minimizes the KL divergence to $p$. The KL divergence *will not* be zero, since these distributions are not the same.\n",
    "\n",
    "This shows that if $p$ is not amongst the model class, then we cannot assume that the cross-entropy will converge to the true entropy.\n",
    "\n",
    "In this sense, we can typically never know the true entropy of the underlying distribution, since any sufficiently sophisticated will not really be in the simple class of models we restrict ourselves to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
