{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reparametrization And Cross Entropy Error\n",
    "\n",
    "From the previous notebook, I said that we want to try to find a better model than Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations From Last Time\n",
    "\n",
    "Here is our Naive Bayes model equation:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\cdots\n",
    "\\frac{\n",
    "    \\prob{W_M = w_M \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_M = w_M \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "And we saw we could rewrite this as:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}\n",
    "=\n",
    "\\phi_0\n",
    "\\prod_{i = 1}^M\n",
    "\\phi_i^{w_i}\n",
    "\\phi_i^{\\prime 1 - w_i}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Finally, we saw that we could then write:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{i = 1}^M\n",
    "    \\phi_i^{w_i}\n",
    "    \\phi_i^{\\prime 1 - w_i}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "        \\phi_i^{\\prime 1 - w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\\\\n",
    "\\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "        \\phi_i^{\\prime 1 - w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating Absence Features\n",
    "\n",
    "Writing in the absence features $\\phi_i^{\\prime 1 - w_i}$ is driving me crazy. We used presence and absence features with the Naive Bayes model because the Naive Bayes models calculates each one seperately as a feature probability ratio.\n",
    "\n",
    "This is how Naive Bayes works, but it isn't strictly necessary to do things this way.\n",
    "\n",
    "Let me choose some new parameters that will work *the same* as Naive Bayes. I will use these to simplify my equations above.\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\omega_0\n",
    "&:=\n",
    "\\phi_0\n",
    "\\prod_{i = 1}^M\n",
    "\\phi_i^\\prime\n",
    "\\\\\n",
    "\\omega_i\n",
    "&:=\n",
    "\\frac{\n",
    "    \\phi_i\n",
    "}{\n",
    "    \\phi_i^\\prime\n",
    "}\n",
    "\\\\\n",
    "\\omega_i^\\prime\n",
    "&:=\n",
    "1.0\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you this is this calculates the same Naive Bayes probabilities, even though the absence feature factor is always 1.0:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\frac{\n",
    "    \\pprob{\\omega}{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}{\n",
    "    \\pprob{\\omega}{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}\n",
    "&=\n",
    "\\omega_0\n",
    "\\prod_{i = 1}^M\n",
    "\\omega_i^{w_i}\n",
    "\\omega_i^{\\prime (1 - w_i)}\n",
    "\\\\\n",
    "&=\n",
    "\\left(\n",
    "    \\phi_0\n",
    "    \\prod_{i = 1}^M\n",
    "        \\phi_i^\\prime\n",
    "\\right)\n",
    "\\prod_{i = 1}^M\n",
    "\\frac{\n",
    "    \\phi_i\n",
    "}{\n",
    "    \\phi_i^\\prime\n",
    "}^{w_i}\n",
    "1^{1 - w_i}\n",
    "\\\\\n",
    "&=\n",
    "\\phi_0\n",
    "\\prod_{i = 1}^M\n",
    "\\phi_i^{w_i}\n",
    "\\left(\n",
    "    \\phi_i^{\\prime -1}\n",
    "\\right)^{w_i}\n",
    "\\left(\n",
    "    \\phi_i^\\prime\n",
    "\\right)^1\n",
    "\\\\\n",
    "&=\n",
    "\\phi_0\n",
    "\\prod_{i = 1}^M\n",
    "\\phi_i^{w_i}\n",
    "\\phi_i^{\\prime (1 - w_i)}\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What have I shown here?\n",
    "\n",
    "There is always a choice of $\\omega$ that is *equivalent* to the $\\phi$ that are chosen by Naive Bayes. Since these $\\omega$ always have $\\omega_i^\\prime = 1.0$, those parameters are redundant and unnecessary. I will drop all mention of $\\omega_i^\\prime$ from any future equations.\n",
    "\n",
    "Likewise, for any setting of $\\omega$, I can always go back to a setting of the $\\phi$ values:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\phi_0\n",
    "&:=\n",
    "\\omega_0\n",
    "\\\\\n",
    "\\phi_i\n",
    "&:=\n",
    "\\omega_i\n",
    "\\\\\n",
    "\\phi_i^\\prime\n",
    "&:=\n",
    "1.0\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I am showing is that $\\phi$ and $\\omega$ are *interchangeable*. Any model in terms of $\\phi$ is equal to a model in terms of some $\\omega$, and vice versa.\n",
    "\n",
    "This is just like the interchangability of feet and meters. It doesn't really matter which one I use. If I find it easier to solve a problem about distance in terms of meters, then there is no necessity for me to use feet for my calculations, right?\n",
    "\n",
    "Likewise, using my $\\omega$ values is easier than using $\\phi$ values. The equations will be simpler, because the $\\omega$ version will not have any absence features.\n",
    "\n",
    "Instead of trying to find the very best $\\phi$ values, I will try to find the best $\\omega$ values. This is like finding the answer in meters rather than in feet. If for *some* reason you really want me to give you the answer in $\\phi$ values, I can convert back and give it to you like that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing $\\omega$ everywhere I used to write $\\phi$, I'm just going to keep using the $\\phi$ variable name, but with no more $\\phi_i^\\prime$ variables. I showed those were unnecessary.\n",
    "\n",
    "Meet the new equations!\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{i = 1}^M\n",
    "    \\phi_i^{w_i}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\\\\n",
    "\\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I do this, my Naive Bayes settings won't work:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    \\phi_0\n",
    "&:=\n",
    "    \\frac{\n",
    "        \\prob{\\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{\\text{S} = 0}\n",
    "    }\n",
    "    \\quad\\quad\\text{(THESE NOT CORRECT ANYMORE)}\n",
    "\\\\\n",
    "    \\phi_i\n",
    "&:=\n",
    "    \\frac{\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These old definitions wouldn't work, because there are no longer any $\\phi_i^\\prime$ variables, and our Naive Bayes formulas were using that. But sure enough, we can find a new setting that works for our Naive Bayes model without using any $\\phi_i^\\prime$:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    \\phi_0\n",
    "&:=\n",
    "    \\frac{\n",
    "        \\prob{\\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{\\text{S} = 0}\n",
    "    }\n",
    "    \\prod_{i = 1}^M\n",
    "        \\frac{\n",
    "            \\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "        }{\n",
    "            \\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "        }\n",
    "\\\\\n",
    "    \\phi_i\n",
    "&:=\n",
    "        \\frac{\n",
    "            \\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "        }{\n",
    "            \\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "        }\n",
    "    \\Big/\n",
    "        \\frac{\n",
    "            \\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "        }{\n",
    "            \\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "        }\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That would give you the same old Naive Bayes probability calculations. And that is the whole point of the parameterization change: you never needed those $\\phi_i^\\prime$ parameters.\n",
    "\n",
    "However, I no longer really care about the Naive Bayes model values for $\\phi_0, \\phi_i$. I want to choose my own values. It doesn't really matter any more what Naive Bayes thinks these should be set to, because I'm going to set them for myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing Likelihood\n",
    "\n",
    "From the last notebook, we saw that we want to maximize:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\pprob{\\phi}{\\mathcal{D}}\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\pprob{\\phi}{\\text{S} = s_i \\condbar W_1 = w_{i, 1}, \\ldots, W_M = w_{i, M}}\n",
    "\\\\\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\left(\n",
    "    \\pprob{\\phi}{\\text{S} = 1 \\condbar W_1 = w_{i, 1}, \\ldots, W_M = w_{i, M}}\n",
    "\\right)^{s_i}\n",
    "\\left(\n",
    "    \\pprob{\\phi}{\\text{S} = 0 \\condbar W_1 = w_{i, 1}, \\ldots, W_M = w_{i, M}}\n",
    "\\right)^{1 - s_i}\n",
    "\\\\\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{i, j}}\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{s_i}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{1 - s_i}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "I've written the $\\phi$ as a subscript of $\\Pr$ to try to emphasize that the \"probability\" is what our model thinks, and it depends on our choice of $\\phi$. Our job is going to be pick the $\\phi$ that maximizes this probability of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing Log Likelihood\n",
    "\n",
    "We may have thousands of datapoints. Each datapoint will be assigned some probability $\\pprob{\\phi}{S = s_i \\condbar W = w} < 1.0$.\n",
    "\n",
    "Multiplying many numbers less than zero quickly yields a very, very small number. Computers have difficulty representing numbers like $\\frac{1}{2}^{2048}$ in floating point representation. A number like this will end up being rounded to zero. Regardless, a great deal of precision is lost.\n",
    "\n",
    "To avoid this problem, it is common to work in the *log space* to avoid multiplying many probabilities. Here we go:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log\n",
    "\\pprob{\\phi}{\\mathcal{D}}\n",
    "&=\n",
    "\\log\n",
    "\\prod_i^N\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{i, j}}\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{s_i}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{1 - s_i}\n",
    "\\\\\n",
    "&=\n",
    "\\sum_i^N\n",
    "s_i\n",
    "\\left(\n",
    "    \\log\n",
    "    \\frac{\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{i, j}}\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)\n",
    "+\n",
    "(1 - s_i)\n",
    "\\left(\n",
    "    \\log\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Since the $\\log$ function is *[monotonic][monotonic]*, maximizing the log probability is the same as maximizing the probability.\n",
    "\n",
    "[monotonic]: https://en.wikipedia.org/wiki/Monotonic_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability Calculation Is Still Troublesome\n",
    "\n",
    "When we go to maximize the log likelihood of the dataset, this is a sum of log probabilities. Each log probability is like this:\n",
    "\n",
    "\\\\[\n",
    "\\log\n",
    "\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "=\n",
    "\\log\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{j = 1}^M\n",
    "    \\phi_j^{w_{j}}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{j}}\n",
    "    \\right)\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "There are a couple problems with this. Once again, we see a product of many $\\phi_i$ values. We know computers don't do well with that, because they can lose precision.\n",
    "\n",
    "**Bonus**: There is another problem. This formula will work very poorly with Gradient Ascent. Gradient Ascent works best when the second partial derivative is the same with respect to all parameters. This has to do with the relationship between Gradient Ascent and Newton's Method. If the second derivatives are very different with respect to different parameters, no learning rate will work well for all the parameters.\n",
    "\n",
    "Our parameterization has this problem. The second partial derivatives of our formula above will be very different when taking with respect to different parameters.\n",
    "\n",
    "One way to intuit the problem is like this. Consider the same change of adding $\\epsilon$ to a parameter $\\phi_i$ and a parameter $\\phi_j$. If $\\epsilon \\ll \\phi_i$, then this change has a big impact on the product $\\phi_0 \\prod \\phi_i w_i$. On the other hand, if $\\epsilon \\gg \\phi_j$, then this will have very little impact on the product.\n",
    "\n",
    "This isn't quite exactly what screws up Gradient Ascent, but it is closely related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparameterizing the model (again)\n",
    "\n",
    "We've already reparameterized the model once before by dropping a bunch of unnecessary parameters (the absence features).\n",
    "\n",
    "Let's do it all over again to avoid multiplying a bunch of terms together. Here's what I'm going to do:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\theta_0\n",
    "&:=\n",
    "\\log \\phi_0\n",
    "\\\\\n",
    "\\theta_i\n",
    "&:=\n",
    "\\log \\phi_i\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this definition, we can see that to go back we do like so:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\phi_0\n",
    "&=\n",
    "\\exp(\\theta_0)\n",
    "\\\\\n",
    "\\phi_i\n",
    "&=\n",
    "\\exp(\\theta_i)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This is because exponentation and logarithms are inverse operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this reparameterization to write all our equation for log probability in $\\theta$:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log\n",
    "\\pprob{\\phi}{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\log\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{j = 1}^M\n",
    "    \\phi_j^{w_{j}}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{j}}\n",
    "    \\right)\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\log\n",
    "\\frac{\n",
    "    \\exp(\\theta_0)\n",
    "    \\prod_{j = 1}^M\n",
    "    \\exp(\\theta_j)^{w_j}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\exp(\\theta_0)\n",
    "        \\prod_{j = 1}^M\n",
    "        \\exp(\\theta_j)^{w_j}\n",
    "    \\right)\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\log\n",
    "\\frac{\n",
    "    \\exp(\\theta_0)\n",
    "    \\prod_{j = 1}^M\n",
    "    \\exp(\\theta_j w_j)\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\exp(\\theta_0)\n",
    "        \\prod_{j = 1}^M\n",
    "        \\exp(\\theta_j w_j)\n",
    "    \\right)\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\log\n",
    "\\frac{\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what I have done is really nothing at all. Every setting of the $\\phi$ parameters corresponds to a setting of the $\\theta$ parameters, and vice versa. This is another reparameterization.\n",
    "\n",
    "The equations using this reparameterization are convenient because they do not have any multiplications in them. It has turned the multiplications into one sum, plus one exponentiation operation. This plays a lot better with how computers work. It is faster and it is more numerically accurate.\n",
    "\n",
    "Though I won't explain exactly why, this form also results in better behavior when applying Gradient Ascent. One suggestive difference from before is that changing any parameter by $\\epsilon$ has the same result as changing any other parameter by $\\epsilon$. (You can verify that.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a handy function called the *logistic function*. It is $\\sigma(z) = \\frac{e^z}{1 + e^z}$. So we can write:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}\n",
    "=\n",
    "\\sigma\\left(\n",
    "    \\theta_0\n",
    "    +\n",
    "    \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "\\right)\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy fact about the sigmoid function:\n",
    "\n",
    "\\\\[\n",
    "1 - \\sigma(z) = \\sigma(-z)\n",
    "\\\\]\n",
    "\n",
    "This is because:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\frac{\n",
    "    e^{-z}\n",
    "}{\n",
    "    1 + e^{-z}\n",
    "}\n",
    "&=\n",
    "\\frac{\n",
    "    e^z e^{-z}\n",
    "}{\n",
    "    e^z (1 + e^{-z})\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    e^z + 1\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "1\n",
    "-\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    e^z + 1\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "1 - \\sigma(z)\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reparameterized Likelihood Function\n",
    "\n",
    "So here is our new problem!\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log\n",
    "\\pprob{\\theta}{\\mathcal{D}}\n",
    "&=\n",
    "\\sum_i^N\n",
    "s_i\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M \\theta_j w_{i, j}\n",
    "    \\right)\n",
    "\\right)\n",
    "+\n",
    "(1 - s_i)\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(\n",
    "        -\\theta_0\n",
    "        -\n",
    "        \\sum_{j = 1}^M \\theta_j w_{i, j}\n",
    "    \\right)\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This is the reparameterized problem! We are ready to try to apply Gradient Ascent to it!\n",
    "\n",
    "Before we do so, let's turn this into an error function!\n",
    "\n",
    "\\\\[\n",
    "E(\\theta) = - \\log \\pprob{\\theta}{\\mathcal{D}}\n",
    "\\\\]\n",
    "\n",
    "I didn't do anything exciting here. I just flipped things around so I'll do Gradient Descent now.\n",
    "\n",
    "This error function is called the *cross entropy error*. Minimizing the cross entropy error is equivalent to maximizing the likelihood.\n",
    "\n",
    "There is a cool interpretation of the cross entropy that comes out of a field called [Information Theory][itheory], but I will leave that as a bonus notebook.\n",
    "\n",
    "[itheory]: https://en.wikipedia.org/wiki/Information_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
