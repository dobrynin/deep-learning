{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reparametrization And Cross Entropy Error\n",
    "\n",
    "From the previous notebook, I said that we want to try to find a better model than Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations From Last Time\n",
    "\n",
    "Here is our Naive Bayes model equation:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\cdots\n",
    "\\frac{\n",
    "    \\prob{W_M = w_M \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_M = w_M \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "And we saw we could rewrite this as:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}\n",
    "=\n",
    "\\phi_0\n",
    "\\prod_{i = 1}^M\n",
    "\\phi_i^{w_i}\n",
    "\\phi_i^{\\prime 1 - w_i}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Finally, we saw that we could then write:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{i = 1}^M\n",
    "    \\phi_i^{w_i}\n",
    "    \\phi_i^{\\prime 1 - w_i}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "        \\phi_i^{\\prime 1 - w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\\\\n",
    "\\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "        \\phi_i^{\\prime 1 - w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating Absence Features\n",
    "\n",
    "Writing in the absence features $\\phi_i^{\\prime 1 - w_i}$ is driving me crazy. Can I show you how to get rid of them?\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\omega_0\n",
    "&:=\n",
    "\\phi_0\n",
    "\\prod_{i = 1}^M\n",
    "\\phi_i^\\prime\n",
    "\\\\\n",
    "\\omega_i\n",
    "&:=\n",
    "\\frac{\n",
    "    \\phi_i\n",
    "}{\n",
    "    \\phi_i^\\prime\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I do this, then:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{i = 1}^M\n",
    "    \\phi_i^{w_i}\n",
    "    \\phi_i^{\\prime 1 - w_i}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "        \\phi_i^{\\prime 1 - w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    \\omega_0\n",
    "    \\prod_{i = 1}^M\n",
    "    \\omega_i^{w_i}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\omega_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\omega_i^{w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Instead of switching over to $\\omega$, I'm going to keep using $\\phi$. But I'm going to not use those absence features anymore. They were unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing Likelihood\n",
    "\n",
    "From the last notebook, we saw that we want to maximize:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\pprob{\\phi}{\\mathcal{D}}\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\pprob{\\phi}{\\text{S} = s_i \\condbar W_1 = w_{i, 1}, \\ldots, W_M = w_{i, M}}\n",
    "\\\\\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\left(\n",
    "    \\pprob{\\phi}{\\text{S} = 1 \\condbar W_1 = w_{i, 1}, \\ldots, W_M = w_{i, M}}\n",
    "\\right)^{s_i}\n",
    "\\left(\n",
    "    \\pprob{\\phi}{\\text{S} = 0 \\condbar W_1 = w_{i, 1}, \\ldots, W_M = w_{i, M}}\n",
    "\\right)^{1 - s_i}\n",
    "\\\\\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{i, j}}\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{s_i}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{1 - s_i}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "I've written the $\\phi$ as a subscript of $\\Pr$ to try to emphasize that the \"probability\" is what our model thinks, and it depends on our choice of $\\phi$. Our job is going to be pick the $\\phi$ that maximizes this probability of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing Log Likelihood\n",
    "\n",
    "We may have thousands of datapoints. Each datapoint will be assigned some probability $\\pprob{\\phi}{S = s_i \\condbar W = w} < 1.0$.\n",
    "\n",
    "Multiplying many numbers less than zero quickly yields a very, very small number. Computers have difficulty representing numbers like $\\frac{1}{2}^{2048}$ in floating point representation. A number like this will end up being rounded to zero. Regardless, a great deal of precision is lost.\n",
    "\n",
    "To avoid this problem, it is common to work in the *log space* to avoid multiplying many probabilities. Here we go:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log\n",
    "\\pprob{\\phi}{\\mathcal{D}}\n",
    "&=\n",
    "\\log\n",
    "\\prod_i^N\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{i, j}}\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{s_i}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{1 - s_i}\n",
    "\\\\\n",
    "&=\n",
    "\\sum_i^N\n",
    "s_i\n",
    "\\left(\n",
    "    \\log\n",
    "    \\frac{\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{i, j}}\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)\n",
    "+\n",
    "(1 - s_i)\n",
    "\\left(\n",
    "    \\log\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{j = 1}^M\n",
    "            \\phi_j^{w_{i, j}}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Since the $\\log$ function is *[monotonic][monotonic]*, maximizing the log probability is the same as maximizing the probability.\n",
    "\n",
    "[monotonic]: https://en.wikipedia.org/wiki/Monotonic_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability Calculation Is Still Troublesome\n",
    "\n",
    "When we go to maximize the log likelihood of the dataset, this is a sum of log probabilities. Each log probability is like this:\n",
    "\n",
    "\\\\[\n",
    "\\log\n",
    "\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "=\n",
    "\\log\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{j = 1}^M\n",
    "    \\phi_j^{w_{j}}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{j}}\n",
    "    \\right)\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "There are a couple problems with this. Once again, we see a product of many $\\phi_i$ values. We know computers don't do well with that, because they can lose precision.\n",
    "\n",
    "There is another problem. This formula will work very poorly with Gradient Ascent. Gradient Ascent works best when the second partial derivative is the same with respect to all parameters. This has to do with the relationship between Gradient Ascent and Newton's Method. If the second derivatives are very different with respect to different parameters, no learning rate will work well for all the parameters.\n",
    "\n",
    "Our parameterization has this problem. The second partial derivatives of our formula above will be very different when taking with respect to different parameters.\n",
    "\n",
    "One way to intuit the problem is like this. Consider the same change of adding $\\epsilon$ to a parameter $\\phi_i$ and a parameter $\\phi_j$. If $\\epsilon \\ll \\phi_i$, then this change has a big impact on the product $\\phi_0 \\prod \\phi_i w_i$. On the other hand, if $\\epsilon \\gg \\phi_j$, then this will have very little impact on the product.\n",
    "\n",
    "This isn't quite exactly what screws up Gradient Ascent, but it is closely related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparameterizing the model\n",
    "\n",
    "Let's change the way we calculate the log probability. We'll avoid multiplying a bunch of terms, but come out with effectively the same model.\n",
    "\n",
    "Let's define a bunch of values:\n",
    "\n",
    "\\\\[\n",
    "\\theta_i := \\log \\phi_i\n",
    "\\\\]\n",
    "\n",
    "Then, we have that:\n",
    "\n",
    "\\\\[\n",
    "\\log\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{j = 1}^M\n",
    "    \\phi_j^{w_{j}}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "        \\phi_j^{w_{j}}\n",
    "    \\right)\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I have done is called *reparameterization*. Instead of defining my model in terms of $\\phi$, I will define it in terms of $\\theta$. But It's obvious how to go from $\\phi$ to $\\theta$ (take the log), and back again (exponentiate).\n",
    "\n",
    "What I have done is really nothing at all. But it has the important benefit of avoiding a bunch of multiplications by turning them into one sum, plus one exponentiation operation. This plays a lot better with how computers work. It is faster and it is more numerically accurate.\n",
    "\n",
    "Though I won't explain exactly why, this form also results in better behavior when applying Gradient Ascent. One suggestive difference from before is that changing any parameter by $\\epsilon$ has the same result as changing any other parameter by $\\epsilon$. (You can verify that.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a handy function called the *logistic function*. It is $\\sigma(z) = \\frac{e^z}{1 + e^z}$. So we can write:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\exp\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "    \\right)\n",
    "}\n",
    "=\n",
    "\\sigma\\left(\n",
    "    \\theta_0\n",
    "    +\n",
    "    \\sum_{j = 1}^M\n",
    "        \\theta_j w_j\n",
    "\\right)\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy fact about the sigmoid function:\n",
    "\n",
    "\\\\[\n",
    "1 - \\sigma(z) = \\sigma(-z)\n",
    "\\\\]\n",
    "\n",
    "You should prove this yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reparameterized Likelihood Function\n",
    "\n",
    "So here is our new problem!\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log\n",
    "\\pprob{\\theta}{\\mathcal{D}}\n",
    "&=\n",
    "\\sum_i^N\n",
    "s_i\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M \\phi_j w_{i, j}\n",
    "    \\right)\n",
    "\\right)\n",
    "+\n",
    "(1 - s_i)\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(\n",
    "        -\\theta_0\n",
    "        -\n",
    "        \\sum_{j = 1}^M \\phi_j w_{i, j}\n",
    "    \\right)\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This is the reparameterized problem! We are ready to try to apply Gradient Ascent to it!\n",
    "\n",
    "Before we do so, let's turn this into an error function!\n",
    "\n",
    "\\\\[\n",
    "E(\\theta) = - \\log \\pprob{\\theta}{\\mathcal{D}}\n",
    "\\\\]\n",
    "\n",
    "I didn't do anything exciting here. I just flipped things around so I'll do Gradient Descent now.\n",
    "\n",
    "This error function is called the *cross entropy error*. Minimizing the cross entropy error is equivalent to maximizing the likelihood.\n",
    "\n",
    "There is a cool interpretation of the cross entropy that comes out of a field called [Information Theory][itheory], but I will leave that as a bonus notebook.\n",
    "\n",
    "[itheory]: https://en.wikipedia.org/wiki/Information_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
