{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Naive Bayes\n",
    "\n",
    "I want to try to evaluate the performance of our Naive Bayes model. We used false positive rates and recall to look at the performance, but I want to talk about some other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Parameters\n",
    "\n",
    "Consider an email with a word vector like $(w_1, w_2, \\ldots, w_M)$. That means if $w_1 = 1$ the email contains the word assigned the numeric code of 1. If there are $M$ words in the vocabulary, then there are $M$ entries in the vector.\n",
    "\n",
    "We know how to compute the Naive Bayes' calculation about the odds that this email is spam. It is:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\cdots\n",
    "\\frac{\n",
    "    \\prob{W_M = w_M \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_M = w_M \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Let's define some parameters to maybe make this equation simpler:\n",
    "\n",
    "\\\\[\n",
    "\\phi_i\n",
    ":=\n",
    "\\frac{\n",
    "    \\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "And one for the absence of the feature:\n",
    "\n",
    "\\\\[\n",
    "\\phi'_i\n",
    ":=\n",
    "\\frac{\n",
    "    \\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "There is also a special\n",
    "\n",
    "\\\\[\n",
    "\\phi_0\n",
    ":=\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Using this, we have:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "}\n",
    "=\n",
    "\\phi_0\n",
    "\\phi_1^{w_1}\n",
    "\\phi_1^{\\prime 1 - w_1}\n",
    "\\cdots\n",
    "\\phi_M^{w_M}\n",
    "\\phi_M^{\\prime 1 - w_M}\n",
    "=\n",
    "\\phi_0\n",
    "\\prod_{i = 1}^M\n",
    "\\phi_i^{w_i}\n",
    "\\phi_i^{\\prime 1 - w_i}\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds To Probability\n",
    "\n",
    "We have a formula for the odds. Let's turn it into a formula for the probability that an email is spam. We'll use the rule that probability is equal to $\\frac{odds}{1 + odds}$:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    \\frac{\n",
    "        \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "    }{\n",
    "        \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "    }\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\frac{\n",
    "        \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "    }{\n",
    "        \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "    }\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    \\phi_0\n",
    "    \\prod_{i = 1}^M\n",
    "    \\phi_i^{w_i}\n",
    "    \\phi_i^{\\prime 1 - w_i}\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "        \\phi_i^{\\prime 1 - w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the same reasoning, we have:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1\n",
    "    +\n",
    "    \\left(\n",
    "        \\phi_0\n",
    "        \\prod_{i = 1}^M\n",
    "        \\phi_i^{w_i}\n",
    "        \\phi_i^{\\prime 1 - w_i}\n",
    "    \\right)\n",
    "}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing The Likelihood Of An Example\n",
    "\n",
    "What does it mean if our Naive Bayes model does a good job at predicting the class of an email?\n",
    "\n",
    "It means that if the email truly is spam, then the Naive Bayes model should calculate $\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}$ to be very high.\n",
    "\n",
    "On the other hand, if the email is not spam, then we want the Naive Bayes model to calculate $\\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_M = w_M}$.\n",
    "\n",
    "Basically: the model is doing a good job if it thinks the right answers are very likely. The more likely the model thinks the right answer is the better.\n",
    "\n",
    "Here is another way to think of the same idea. The model calculates a probability that the email is spam. If it then *randomly guessed* whether the email was spam/ham using that probability, we want the model to be most likely to guess correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing Likelihood Of A Dataset\n",
    "\n",
    "I showed you what we want for a single example: we want the model to think that the right answer is very likely. The more likely the better.\n",
    "\n",
    "The same principle applies across the entire dataset. You want the model to think the results of the entire dataset are as likely as possible. The probability of the dataset is denoted $\\prob{\\mathcal{D}}$.\n",
    "\n",
    "Here is another way to say the same thing. Let's say the model went through each of the training emails one-by-one. For each email, it randomly guesses whether the email is spam/ham based on $\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M}$. For instance, if $\\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_M = w_M} = 0.90$, then the model will randomly choose \"spam\" or \"ham\" with a 90%/10% probability.\n",
    "\n",
    "If we use this strategy, what is the probability that the model is correct about *every single email*? The way to calculate is by *multiplying* all the probabilities that it is correct for each individual email.\n",
    "\n",
    "This is probably very small, because even if the model is very good at guessing, there are lots of emails that we must get correct. All the same, we call this $\\prob{\\mathcal{D}}$.\n",
    "\n",
    "Our goal is that $\\prob{\\mathcal{D}}$ be as high as possible. Therefore the probability we want to maximize is:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\prob{\\mathcal{D}}\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\prob{\\text{S} = s_i \\condbar W_1 = w_{i, 1}, \\ldots, W_M = w_{i, M}}\n",
    "\\\\\n",
    "&=\n",
    "\\prod_i^N\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "            \\phi_{i, j}^{w_j}\n",
    "            \\phi_{i, j}^{\\prime 1 - w_j}\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\phi_0\n",
    "        \\prod_{j = 1}^M\n",
    "            \\phi_{i, j}^{w_j}\n",
    "            \\phi_{i, j}^{\\prime 1 - w_j}\n",
    "    }\n",
    "\\right)^{s_i}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1\n",
    "        +\n",
    "        \\left(\n",
    "            \\phi_0\n",
    "            \\prod_{i = 1}^M\n",
    "            \\phi_i^{w_i}\n",
    "            \\phi_i^{\\prime 1 - w_i}\n",
    "        \\right)\n",
    "    }\n",
    "\\right)^{1 - s_i}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This assumes there are $N$ emails. $s_i$ is 1 when the $i$th email is spam; it is 0 otherwise. The $(w_{i, 1}, w_{i, 2}, \\ldots, w_{i, M})$ is the word vector for the $i$th email.\n",
    "\n",
    "The higher $\\prob{\\mathcal{D}}$, the more likely the model thinks the dataset is. It makes sense to want the model to think the dataset is likely. The dataset is the only way we can learn the model; why should the model learn things that make it think the observed results are unusual or weird or not revealing of the true mechanism of the system?\n",
    "\n",
    "This principle that the best model is the one that assigns the greatest probability to the dataset is called the *maximum likelihood principle*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is Naive The Best We Can Do?\n",
    "\n",
    "Now that we have a measure of how good a job a model is doing, this opens up a new possible question: is the Naive Bayes model doing the best job possible?\n",
    "\n",
    "In particular, could I tweak the values of $\\phi_0, \\phi_i, \\phi_i^\\prime$ so that do a better job? Could some other values for the parameters assign a higher likelihood to the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Naive Bayes May Not Choose The Best $\\phi$\n",
    "\n",
    "We have seen that Naive Bayes makes an assumption. It assumes:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\wedge W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\wedge W_2 = w_2 \\condbar \\text{S} = 0}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_2 = w_2 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Basically, Naive Bayes assumes that the feature probability ratio for a pair of words is the same as the product of the feature probability ratios for each of the individual words.\n",
    "\n",
    "We saw that this derives from the naive conditional independence assumption:\n",
    "\n",
    "\\\\[\n",
    "\\prob{W_1 = w_1 \\wedge W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "=\n",
    "\\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "\\prob{W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "\\\\]\n",
    "\n",
    "Basically, the naive assumption is that there is no relationship between the pair of words $W_1$ and $W_2$ given that you know whether the email is spam (or not spam).\n",
    "\n",
    "When the naive assumption is wrong, then Naive Bayes will not calculate the correct probability. The more wrong the naive assumption is, the worse job Naive Bayes will tend to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Features\n",
    "\n",
    "Let's see the most extreme example of where Naive Bayes can go wrong.\n",
    "\n",
    "Say that $W_1$ always occurs if $W_2$ occurs, and vice versa. This is extreme, but there are definitely pairs of words that tend to co-occur: for instance, \"limited\" and \"time\" (as in \"limited time offer\").\n",
    "\n",
    "If two words always appear together, than we know:\n",
    "\n",
    "\\\\[\n",
    "\\prob{W_1 = w_1 \\wedge W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "=\n",
    "\\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "=\n",
    "\\prob{W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "\\\\]\n",
    "\n",
    "Our independence assumption could not be more wrong! If we use the formula:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\wedge W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\wedge W_2 = w_2 \\condbar \\text{S} = 0}\n",
    "}\n",
    "&=\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_2 = w_2 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\right)^2\n",
    "= \\phi_1^2\n",
    "\\\\\n",
    "&=\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\prob{W_2 = w_2 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_2 = w_2 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\right)^2\n",
    "=\n",
    "\\phi_2^2\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "then we are effectively *double counting* the effect of $W_1, W_2$. In this case, we really ought to change $\\phi_2 = 1.0$ (no change in odds ratio) if we're going to keep $\\phi_1$ the same.\n",
    "\n",
    "Or we can change $\\phi_1$ and keep $\\phi_2$. Or we can split the difference and set:\n",
    "\n",
    "\\\\[\n",
    "\\phi_1 := \\phi_2 := \\sqrt{\n",
    "    \\frac{\n",
    "        \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When You Can Beat Naive Bayes\n",
    "\n",
    "We see now when you can set the $\\phi$ values differently from Naive Bayes and do better at predicting the training dataset.\n",
    "\n",
    "That happens when the conditional independence assumption is not true. It happens when there are correlations (either positive or negative correlations) between the occurence of one feature and the occurence of another.\n",
    "\n",
    "If the conditional independence assumption really is true, then the $\\phi$ calculated for Naive Bayes really are optimal. It's only because the naive assumption is typically false that we can typically do a better job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Find A Better $\\phi$\n",
    "\n",
    "We now know that Naive Bayes doesn't always give you the best choice of $\\phi$. How can we find a better setting of $\\phi$?\n",
    "\n",
    "The answer is to use gradient descent to maximize our likelihood function. This basically starts out the $\\phi$ values randomly. For each step of the gradient descent algorithms, it looks at every $\\phi_i$ value, and asks: \"Would I do better if I were to decrease $\\phi_i$ a little bit? Would I do betetr if I increase $\\phi_i$ a little bit?\"\n",
    "\n",
    "It repeats this for many steps, and hopefully ends up with a better setting of $\\phi$.\n",
    "\n",
    "In the next notebook, we'll see how to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
