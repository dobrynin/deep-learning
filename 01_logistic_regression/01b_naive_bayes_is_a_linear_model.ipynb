{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive Bayes Discriminator Is A Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we learned about linear regression, we learned coefficients $\\theta_i$ to parameterize a linear model.\n",
    "\n",
    "I want to show you how we can turn our Naive Bayes discriminator into a linear model with an intercept term and coefficients. We'll see why this is useful very soon.\n",
    "\n",
    "First, let's return to our standard definition:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1 \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\cdots\n",
    "\\frac{\n",
    "    \\prob{W_k = w_k \\condbar \\text{S} = 1}\n",
    "}{\n",
    "    \\prob{W_k = w_k \\condbar \\text{S} = 0}\n",
    "}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write this in a form where the $w_i$ don't appear inside any of the probabilities.\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0}\n",
    "}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\prob{W_1 = 1 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_1 = 1 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\right)^{w_1}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\prob{W_1 = 0 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_1 = 0 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\right)^{1 - w_1}\n",
    "\\cdots\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\prob{W_k = 1 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_k = 1 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\right)^{w_k}\n",
    "\\left(\n",
    "    \\frac{\n",
    "        \\prob{W_k = 0 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_k = 0 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\right)^{1 - w_k}\n",
    "\\\\]\n",
    "\n",
    "Remember, either $w_i = 0$, or $w_i = 1$. So for each pair of feature probability ratios, only one remains, and the other is raised to the zeroth power and becomes one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's bring the $w_i, 1 - w_i$ terms down from the exponent by taking the log.\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    &\n",
    "    \\left(\n",
    "        \\log\\prob{\\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{\\text{S} = 0}\n",
    "    \\right)\n",
    "\\\\\n",
    "    &+\n",
    "    w_1\n",
    "    \\left(\n",
    "        \\log\\prob{W_1 = 1 \\condbar \\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{W_1 = 1 \\condbar \\text{S} = 0}\n",
    "    \\right)\n",
    "    +\n",
    "    (1 - w_1)\n",
    "    \\left(\n",
    "        \\log\\prob{W_1 = 0 \\condbar \\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{W_1 = 0 \\condbar \\text{S} = 0}\n",
    "    \\right)\n",
    "\\\\\n",
    "    &\\cdots\n",
    "\\\\\n",
    "    &+\n",
    "    w_k\n",
    "    \\left(\n",
    "        \\log\\prob{W_k = 1 \\condbar \\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{W_k = 1 \\condbar \\text{S} = 0}\n",
    "    \\right)\n",
    "    +\n",
    "    (1 - w_k)\n",
    "    \\left(\n",
    "        \\log\\prob{W_k = 0 \\condbar \\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{W_k = 0 \\condbar \\text{S} = 0}\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's simplify slightly by multiplying out the $1 - w_i$ factors and rearranging slightly.\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "    &\n",
    "    \\left(\n",
    "        \\log\\prob{\\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{\\text{S} = 0}\n",
    "    \\right)\n",
    "    +\n",
    "    \\sum_{i = 1}^k\n",
    "        \\left(\n",
    "            \\log\\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "            -\n",
    "            \\log\\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "        \\right)\n",
    "\\\\\n",
    "    &+\n",
    "    \\sum_{i = 1}^k\n",
    "    w_i\n",
    "    \\Big[\n",
    "        \\left(\n",
    "            \\log\\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "            -\n",
    "            \\log\\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "        \\right)\n",
    "        -\n",
    "        \\left(\n",
    "            \\log\\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "            -\n",
    "            \\log\\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "        \\right)\n",
    "    \\Big]\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a $\\theta_0$ for an intercept and $\\theta_i$ for the coefficients to the $w_i$ variables.\n",
    "\n",
    "\\\\[\n",
    "\\theta_0\n",
    "=\n",
    "    \\left(\n",
    "        \\log\\prob{\\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{\\text{S} = 0}\n",
    "    \\right)\n",
    "    +\n",
    "    \\sum_{i = 1}^k\n",
    "        \\left(\n",
    "            \\log\\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "            -\n",
    "            \\log\\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "        \\right)\n",
    "\\\\\n",
    "\\theta_i\n",
    "=\n",
    "    \\left(\n",
    "        \\log\\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "    \\right)\n",
    "    -\n",
    "    \\left(\n",
    "        \\log\\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "        -\n",
    "        \\log\\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "    \\right)\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can substitute back in. Remember, that we started out with an equation for the odds of spam versus not spam, but when we took the log this became the *log odds*.\n",
    "\n",
    "\\\\[\n",
    "\\log\n",
    "\\frac{\n",
    "    \\prob{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}{\n",
    "    \\prob{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}\n",
    "=\n",
    "\\theta_0\n",
    "+\n",
    "\\sum_{i=1}^k\n",
    "\\theta_i w_i\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the $\\theta_i$ are called a *log odds ratio*. Let me show you why. First step:\n",
    "\n",
    "\\\\[\n",
    "\\theta_i\n",
    "=\n",
    "\\log\n",
    "    \\frac{\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "-\n",
    "\\log\n",
    "    \\frac{\n",
    "        \\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "=\n",
    "\\log\n",
    "    \\frac{\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "        \\big/\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "    }{\n",
    "        \\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "        \\big/\n",
    "        \\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\\\]\n",
    "\n",
    "I just used the fact that the difference of logs is the log of the ratio. That's the same rule I've been using all along. I used this rule twice.\n",
    "\n",
    "Next, since I have a ratio of ratios, I can move the denominator of the top down, and bring the numerator of the bottom up. See:\n",
    "\n",
    "\\\\[\n",
    "\\theta_i\n",
    "=\n",
    "\\log\n",
    "    \\frac{\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 1}\n",
    "        \\big/\n",
    "        \\prob{W_i = 0 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_i = 1 \\condbar \\text{S} = 0}\n",
    "        \\big/\n",
    "        \\prob{W_i = 0 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\\\]\n",
    "\n",
    "So now I have the log of the ratio of two odds. The top is the odds that a spam email contains the word $W_i$, while the bottom is the odds that a non-spam email contains the word $W_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these the best choice of $\\theta_i$?\n",
    "\n",
    "We see that the Naive Bayes suggests a way to define a linear function to estimate the log odds that an email is spam.\n",
    "\n",
    "The Naive Bayes classifier assumes that features are conditionally independent, but we know that isn't strictly true. The less this is true, the worse a job the choice of $\\theta_0, \\theta_i$ as specified by the Naive Bayes assumption will do at predicting the log odds.\n",
    "\n",
    "For example, consider if $w_{123}$ is *always* present in an email when $w_{456}$ is, and vice versa. Then if we set\n",
    "\n",
    "\\\\[\n",
    "\\theta_{123}\n",
    "=\n",
    "\\log\n",
    "    \\frac{\n",
    "        \\prob{W_{123} = 1 \\condbar \\text{S} = 1}\n",
    "        \\big/\n",
    "        \\prob{W_{123} = 0 \\condbar \\text{S} = 1}\n",
    "    }{\n",
    "        \\prob{W_{123} = 1 \\condbar \\text{S} = 0}\n",
    "        \\big/\n",
    "        \\prob{W_{123} = 0 \\condbar \\text{S} = 0}\n",
    "    }\n",
    "\\\\]\n",
    "\n",
    "then we should set\n",
    "\n",
    "\\\\[\n",
    "\\theta_{456} = 0.0\n",
    "\\\\]\n",
    "\n",
    "The reason is that the feature $w_{456}$ adds no new information beyond $w_{123}$. But Naive Bayes won't know to do this, because it is assuming $w_{456}$ is independent of $w_{123}$. In fact, the exact opposite true: the presence of one depends *entirely* on the presence of the other.\n",
    "\n",
    "Basically, Naive Bayes fails insofar as the occurence of words is not conditionally independent given the class spam/ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toward An Error Function\n",
    "\n",
    "This suggests that we might try to pick the $\\theta_0, \\theta_i$ more freely, without assumption. But if we do that, we need to define an error function so that we can have a criterion that defines a \"best\" choice of $\\hat\\theta_0, \\hat\\theta_i$. What could that be?\n",
    "\n",
    "Well, to start, it makes sense that if in our training dataset we have a spam email, we want:\n",
    "\n",
    "\\\\[\n",
    "\\theta_0 + \\sum w_i \\theta_i\n",
    "\\\\]\n",
    "\n",
    "to be as large as possible. That's because this number is supposed to be the log odds:\n",
    "\n",
    "\\\\[\n",
    "\\log\n",
    "\\frac{\n",
    "    \\pprob{\\theta}{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}{\n",
    "    \\pprob{\\theta}{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "The higher the log odds are, the more likely the model thinks this email is spam, which is the correct answer.\n",
    "\n",
    "Likewise, if the email weren't spam, we would want the estimated log odds to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Odds to Probability\n",
    "\n",
    "These log odds are making my head hurt. More simply, I want $\\pprob{\\theta}{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_k = w_k}$ to be as close to 1.0 as possible when the email really is spam, and as close to 0.0 when the email isn't.\n",
    "\n",
    "So, as a first step, let's turn log odds into probability. The key is to remember how to convert odds to probability: $\\frac{\\text{odds}}{1 + \\text{odds}}$. So let's call \n",
    "\n",
    "\\\\[\n",
    "z := \\theta_0 + \\sum w_i \\theta_i\n",
    "\\\\]\n",
    "\n",
    "Since $z$ is thus the log odds ratio, we can undo by exponentiating:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\pprob{\\theta}{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}{\n",
    "    \\pprob{\\theta}{\\text{S} = 0 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "}\n",
    "=\n",
    "e^z\n",
    "\\\\]\n",
    "\n",
    "Then\n",
    "\n",
    "\\\\[\n",
    "\\pprob{\\theta}{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "=\n",
    "\\frac{\n",
    "e^z\n",
    "}{\n",
    "1 + e^z\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "1\n",
    "}{\n",
    "1 + e^{-z}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "I like the form $\\frac{e^z}{1 + e^z}$. However, you will often see the equivalent form $\\frac{1}{e^{-z} + 1}$. Because of some subtleties of round-off error, this second way is better for computers.\n",
    "\n",
    "The function $f(z) = \\frac{e^z}{1 + e^z} = \\frac{1}{e^{-z} + 1}$ is called the *logistic function*. It's the function for turning log odds into a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared Errors For Probabilities (not ideal)\n",
    "\n",
    "So we now know that\n",
    "\n",
    "\\\\[\n",
    "\\pprob{\\theta}{\\text{S} = 1 \\condbar W_1 = w_1, \\ldots, W_k = w_k}\n",
    "=\n",
    "\\frac{1}{1 + e^{-z}}\n",
    "=\n",
    "\\frac{1}{1 + e^{\n",
    "    -\\left(\\theta_0 + \\sum \\theta_i w_i\\right)\n",
    "}}\n",
    "\\\\]\n",
    "\n",
    "As discussed, if the email in question truly is spam, we want this to be as close to 1.0 as possible. On the other hand, we want it to be close to 0.0 if the email is not spam.\n",
    "\n",
    "We're now in a similar spot to when we did linear regression. We know what we want our output to be close to, but we don't know exactly how to sum up our errors.\n",
    "\n",
    "One (not very good) way to make an error function would be:\n",
    "\n",
    "\\\\[\n",
    "E(\\theta)\n",
    "=\n",
    "\\sum_i^N\n",
    "    \\left(\n",
    "        \\frac{1}{1 + e^{-z_i}}\n",
    "        -\n",
    "        y_i\n",
    "    \\right)\n",
    "    ^2\n",
    "\\\\]\n",
    "\n",
    "That is, use the sum of squared errors loss on the probabilities calculated by the model.\n",
    "\n",
    "But there is a better way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Error\n",
    "\n",
    "Here is the primary idea. A good choice of $\\theta$ should make the observed dataset *likely*. This is basically synonymous with saying: choose the $\\theta$ that explain the observed dataset best, leaving the least possible to random chance.\n",
    "\n",
    "This principle is called the *maximum likelihood principle*. Let's apply it to our classification problem. We want to maximize\n",
    "\n",
    "\\\\[\n",
    "\\prod_{i = 1}^N \\pprob{\\theta}{Y = y^i \\condbar X = x^i}\n",
    "\\\\]\n",
    "\n",
    "I'm slightly shifting up my notation. $y^i$ means the $i$th label; I'm not taking the \"power\" of $y$. $Y$ is the variable \"spam or not spam.\"\n",
    "\n",
    "$x^i$ is the vector of observed predictor values for the $i$th example. We can use a subscript to number each of these features:\n",
    "\n",
    "\\\\[\n",
    "\\pprob{\\theta}{Y = y^i \\condbar X = x^i}\n",
    "=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1 + \\exp \\left(\n",
    "        -\\theta_0 - \\sum_j \\theta_j x^i_j\n",
    "    \\right)\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Here $x^i_j$ is 1 if for training example number $i$, the feature $j$ is present.\n",
    "\n",
    "Okay, let's use my fancy exponentiation trick:\n",
    "\n",
    "\\\\[\n",
    "\\pprob{\\theta}{\\mathcal{D}}\n",
    "=\n",
    "\\prod_{i = 1}^N \\pprob{\\theta}{Y = y^i \\condbar X = x^i}\n",
    "=\n",
    "\\prod_{i = 1}^N\n",
    "    \\left(\n",
    "        \\pprob{\\theta}{Y = 1 \\condbar X = x^i}\n",
    "    \\right)\n",
    "    ^{y^i}\n",
    "    \\cdot\n",
    "    \\left(\n",
    "        1 - \\pprob{\\theta}{Y = 1 \\condbar X = x^i}\n",
    "    \\right)\n",
    "    ^{1 - y^i}\n",
    "\\\\]\n",
    "\n",
    "This is the *likelihood* of the dataset for the model parameterized by $\\theta$. I denoted it by $\\pprob{\\theta}{\\mathcal{D}}$. $\\mathcal{D}$ is supposed to mean \"the dataset.\"\n",
    "\n",
    "Okay, it is very typical for us to turn products of probabilities into sums of log probabilities. This is very practical, because multiplying many probabilities means you are dealing with a result that is very close to zero. The way floating point arithmetic works, it is hard for computers to deal with numbers with very small magnitude.\n",
    "\n",
    "Thus, a common trick in computing is to work with sums of log probabilities. The log of a number close to zero is a large negative number; computers are happy with this. The log of a number close to 1.0 is still close to zero, but this is being *added*, not *multiplied*. So that is also good.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\log \\pprob{\\theta}{\\mathcal{D}}\n",
    "&=\n",
    "\\log\n",
    "\\prod_{i = 1}^N\n",
    "    \\left(\n",
    "        \\log \\pprob{\\theta}{Y = 1 \\condbar X = x^i}\n",
    "    \\right)\n",
    "    ^{y^i}\n",
    "    \\cdot\n",
    "    \\left(\n",
    "        1 - \\pprob{\\theta}{Y = 1 \\condbar X = x^i}\n",
    "    \\right)\n",
    "    ^{1 - y^i}\n",
    "\\\\\n",
    "&=\n",
    "\\sum_{i = 1}^N\n",
    "    y^i\n",
    "    \\log \\left( \\pprob{\\theta}{Y = 1 \\condbar X = x^i} \\right)\n",
    "    +\n",
    "    (1 - y^i)\n",
    "    \\log \\left( 1 - \\pprob{\\theta}{Y = 1 \\condbar X = x^i} \\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Because this is a log probability, we want it to be as large (close to zero) as possible. Or we can flip things around:\n",
    "\n",
    "\\\\[\n",
    "-\\log \\pprob{\\theta}{\\mathcal{D}}\n",
    "=\n",
    "\\sum_{i = 1}^N\n",
    "    y^i\n",
    "    \\left(\n",
    "        -\\log \\left( \\pprob{\\theta}{Y = 1 \\condbar X = x^i} \\right)\n",
    "    \\right)\n",
    "    +\n",
    "    (1 - y^i)\n",
    "    \\left(\n",
    "        -\\log \\left( 1 - \\pprob{\\theta}{Y = 1 \\condbar X = x^i} \\right)\n",
    "    \\right)\n",
    "\\\\]\n",
    "\n",
    "Because this is a negative of a log probability, we want it to be as *small* as possible. We want it to be close to zero. That means it is suitable as an error function to minimize.\n",
    "\n",
    "This is the *cross-entropy error*. In the next notebook, we'll train a model to minimize this error by using Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
