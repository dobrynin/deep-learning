{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    os.getcwd(),\n",
    "    \"data/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Simple email class. All it does is really allow you to read an email's text.\n",
    "class Email:\n",
    "    def __init__(self, path, label):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "\n",
    "    def text_content(self):\n",
    "        return type(self).read_text_content(self.path)\n",
    "    \n",
    "    def word_counts(self):\n",
    "        counts = {}\n",
    "        for word in self.text_content().split():\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "            \n",
    "        return counts\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, path, label):\n",
    "        return Email(\n",
    "            path = path,\n",
    "            label = label\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def read_text_content(cls, path):\n",
    "        full_path = os.path.join(DATA_DIR, path)\n",
    "        # Grr! Emails are encoded in Latin-1, not UTF-8. Python\n",
    "        # (rightly) freaks out.\n",
    "        with open(full_path, \"r\", encoding = \"iso-8859-1\") as f:\n",
    "            try:\n",
    "                return f.read()\n",
    "            except:\n",
    "                print(f\"Error with: {path}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "# This counts how many emails each word occurs in.\n",
    "def build_word_reaches(ham_emails, spam_emails):\n",
    "    word_reaches = {}\n",
    "    for emails in (ham_emails, spam_emails):\n",
    "        for email in emails:\n",
    "            for (word, _) in email.word_counts().items():\n",
    "                if word not in word_reaches:\n",
    "                    word_reaches[word] = 0\n",
    "                # No matter how frequent in the email, only counts once per email.\n",
    "                word_reaches[word] += 1\n",
    "                \n",
    "    return word_reaches\n",
    "\n",
    "# Throw away those emails that don't occur in at least 100 emails.\n",
    "# Throwing out low reach features means:\n",
    "# (1) Less chance for overfitting\n",
    "# (2) Smaller feature vectors, faster, less memory use.\n",
    "def filter_words(word_reaches, limit = 100):\n",
    "    filtered_words = SortedSet()\n",
    "    for (word, word_reach) in word_reaches.items():\n",
    "        if word_reach >= limit:\n",
    "            filtered_words.add(word)\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "# Bidirectional map. Limits to just filtered words, though.\n",
    "class FilteredWordEncodingDictionary:\n",
    "    def __init__(self, filtered_words):\n",
    "        self.word_to_code_dict = {}\n",
    "        self.code_to_word_dict = {}\n",
    "\n",
    "        for word in filtered_words:\n",
    "            self.insert_word(word)\n",
    "\n",
    "    # Only meant to be called when constructing the dictionary.\n",
    "    def insert_word(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            code = len(self.word_to_code_dict)\n",
    "            self.word_to_code_dict[word] = code\n",
    "            self.code_to_word_dict[code] = word\n",
    "\n",
    "    def word_to_code(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            return None\n",
    "\n",
    "        return self.word_to_code_dict[word]\n",
    "\n",
    "    def code_to_word(self, code):\n",
    "        if code not in self.code_to_word_dict:\n",
    "            raise f\"Code {code} not recorded!\"\n",
    "\n",
    "        return self.code_to_word_dict[code]\n",
    "\n",
    "    # This returns a vector of ones and zeros.\n",
    "    def encode_text(self, text):\n",
    "        codes = np.zeros(len(self.code_to_word_dict))\n",
    "\n",
    "        for word in text.split():\n",
    "            code = self.word_to_code(word)\n",
    "            if code is not None:\n",
    "                codes[code] = 1.0\n",
    "\n",
    "        return codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_to_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a simple subclass of Email that just encodes the words in an email.\n",
    "class EncodedEmail(Email):\n",
    "    def __init__(self, path, label, word_encoding_dictionary):\n",
    "        super().__init__(path, label)\n",
    "\n",
    "        self.codes = (\n",
    "            word_encoding_dictionary.encode_text(\n",
    "                self.text_content()\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "class Dataset:\n",
    "    DATA_FILE_PATH = os.path.join(DATA_DIR, 'lr_data.p')\n",
    "    WORD_REACH_LIMIT = 100\n",
    "\n",
    "    def __init__(\n",
    "            self, word_encoding_dictionary, encoded_ham_emails, encoded_spam_emails\n",
    "    ):\n",
    "        self.word_encoding_dictionary = word_encoding_dictionary\n",
    "        self.encoded_ham_emails = encoded_ham_emails\n",
    "        self.encoded_spam_emails = encoded_spam_emails\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_ham_emails) + len(self.encoded_spam_emails)\n",
    "    \n",
    "    @classmethod\n",
    "    def encode(cls, ham_emails, spam_emails):\n",
    "        # Count words, select which we will keep.\n",
    "        word_reaches = build_word_reaches(ham_emails, spam_emails)\n",
    "        filtered_words = filter_words(word_reaches, limit = cls.WORD_REACH_LIMIT)\n",
    "        \n",
    "        # Assign codes to all words.\n",
    "        word_encoding_dictionary = FilteredWordEncodingDictionary(filtered_words)\n",
    "        \n",
    "        # Encode each email as a vector of ones and zeros.\n",
    "        encoded_ham_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in ham_emails\n",
    "        ]\n",
    "        encoded_spam_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in spam_emails\n",
    "        ]\n",
    "        \n",
    "        # Construct the object!\n",
    "        return cls(\n",
    "            word_encoding_dictionary,\n",
    "            encoded_ham_emails,\n",
    "            encoded_spam_emails\n",
    "        )\n",
    "\n",
    "    INSTANCE = None\n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if not cls.INSTANCE:\n",
    "            with open(cls.DATA_FILE_PATH, 'rb') as f:\n",
    "                cls.INSTANCE = pickle.load(f)\n",
    "        return cls.INSTANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarfile already downloaded!\n",
      "Tarfile already extracted!\n",
      "Dataset already processed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "ENRON_SPAM_URL = (\n",
    "    \"http://csmining.org/index.php/\"\n",
    "    \"enron-spam-datasets.html\"\n",
    "    \"?file=tl_files/Project_Datasets/Enron-Spam%20datasets/Preprocessed\"\n",
    "    \"/enron1.tar.tar\"\n",
    ")\n",
    "\n",
    "TAR_FILE_NAME = \"enron1.tar.tar\"\n",
    "ENRON_DATA_DIR_NAME = \"enron1\"\n",
    "\n",
    "def download_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    if os.path.isfile(tarfile_path):\n",
    "        print(\"Tarfile already downloaded!\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading enron1.tar.tar\")\n",
    "    urlretrieve(ENRON_SPAM_URL, tarfile_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    enron_data_dir = os.path.join(DATA_DIR, ENRON_DATA_DIR_NAME)\n",
    "    if os.path.isdir(enron_data_dir):\n",
    "        print(\"Tarfile already extracted!\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting enron1.tar.tar\")\n",
    "    os.system(f\"tar -xf {tarfile_path} -C {DATA_DIR}\")\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def read_emails_dir(path, label):\n",
    "    emails = []\n",
    "    for email_fname in os.listdir(os.path.join(DATA_DIR, path)):\n",
    "        email_path = os.path.join(path, email_fname)\n",
    "        email = Email.read(\n",
    "            path = email_path,\n",
    "            label = label\n",
    "        )\n",
    "        emails.append(email)\n",
    "\n",
    "    return emails\n",
    "\n",
    "def build_dataset():\n",
    "    ham_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"ham\"),\n",
    "        label = 0\n",
    "    )\n",
    "    spam_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"spam\"),\n",
    "        label = 1\n",
    "    )\n",
    "\n",
    "    return Dataset.encode(\n",
    "        ham_emails = ham_emails,\n",
    "        spam_emails = spam_emails\n",
    "    )\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    with open(Dataset.DATA_FILE_PATH, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def build_and_save_dataset():\n",
    "    if os.path.isfile(Dataset.DATA_FILE_PATH):\n",
    "        print(\"Dataset already processed!\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading and processing emails!\")\n",
    "    dataset = build_dataset()\n",
    "    save_dataset(dataset)\n",
    "    print(\"Dataset created!\")\n",
    "\n",
    "download_tarfile()\n",
    "extract_tarfile()\n",
    "build_and_save_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, word_encoding_dictionary):\n",
    "        self.theta = np.zeros(\n",
    "            len(word_encoding_dictionary)\n",
    "        )\n",
    "        \n",
    "    def prob(self, codes):\n",
    "        return logistic(self.theta.dot(codes))\n",
    "    \n",
    "    def error(self, email):\n",
    "        if email.label == 0:\n",
    "            return -np.log(1 - self.prob(email.codes))\n",
    "        else:\n",
    "            return -np.log(self.prob(email.codes))\n",
    "\n",
    "    def partial_derivatives(self, email):\n",
    "        if email.label == 0:\n",
    "            # This is a vectorized version.\n",
    "            return (\n",
    "                email.codes * self.prob(email.codes)\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                -email.codes * (1 - self.prob(email.codes))\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "class DatasetSplitter:\n",
    "    @classmethod\n",
    "    def split(cls, dataset, ratio):\n",
    "        datasetA = cls._split(dataset, ratio, 0)\n",
    "        datasetB = cls._split(dataset, ratio, 1)\n",
    "        return (datasetA, datasetB)\n",
    "\n",
    "    @classmethod\n",
    "    def _split(cls, dataset, ratio, mode):\n",
    "        split_encoded_ham_emails, split_encoded_spam_emails = [], []\n",
    "        emails_pairs = [\n",
    "            (dataset.encoded_ham_emails, split_encoded_ham_emails),\n",
    "            (dataset.encoded_spam_emails, split_encoded_spam_emails)\n",
    "        ]\n",
    "\n",
    "        for (emails, split_emails) in emails_pairs:\n",
    "            for email in emails:\n",
    "                # This is a fancy way to pseudorandomly but\n",
    "                # deterministically select emails. That way we always\n",
    "                # pick the same set of emails for reproducability\n",
    "                # across program runs.\n",
    "                h = zlib.crc32(email.path.encode())\n",
    "                p = h / (2**32 - 1)\n",
    "                if (mode == 0 and p < ratio) or (mode == 1 and p >= ratio):\n",
    "                    split_emails.append(email)\n",
    "\n",
    "        return Dataset(\n",
    "            dataset.word_encoding_dictionary,\n",
    "            encoded_ham_emails = split_encoded_ham_emails,\n",
    "            encoded_spam_emails = split_encoded_spam_emails\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, training_dataset, test_dataset, learning_rate):\n",
    "        self.training_dataset, self.test_dataset = training_dataset, test_dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = LogisticRegressionModel(training_dataset.word_encoding_dictionary)\n",
    "        \n",
    "    def error(self, dataset):\n",
    "        error = 0.0\n",
    "        for emails in (dataset.encoded_ham_emails, dataset.encoded_spam_emails):\n",
    "            for email in emails:\n",
    "                error += self.model.error(email)\n",
    "\n",
    "        # Otherwise it isn't fair because longer sets have more error.\n",
    "        return error / len(dataset)\n",
    "    \n",
    "    def partial_derivatives(self):\n",
    "        partials = np.zeros(len(self.training_dataset.word_encoding_dictionary))\n",
    "        for emails in (self.training_dataset.encoded_ham_emails, self.training_dataset.encoded_spam_emails):\n",
    "            for email in emails:\n",
    "                partials += self.model.partial_derivatives(email)\n",
    "                \n",
    "        return partials\n",
    "\n",
    "    def train_step(self):\n",
    "        self.model.theta -= self.learning_rate * self.partial_derivatives()\n",
    "        print(f\"Train Error: {self.error(self.training_dataset):0.2f}\")\n",
    "        print(f\"Test Error: {self.error(self.test_dataset):0.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 3.02\n",
      "Test Error: 2.63\n",
      "Train Error: 0.66\n",
      "Test Error: 0.67\n",
      "Train Error: 2.37\n",
      "Test Error: 2.06\n",
      "Train Error: 0.71\n",
      "Test Error: 0.70\n",
      "Train Error: 1.40\n",
      "Test Error: 1.23\n",
      "Train Error: 0.95\n",
      "Test Error: 0.96\n",
      "Train Error: 1.15\n",
      "Test Error: 1.00\n",
      "Train Error: 0.64\n",
      "Test Error: 0.63\n",
      "Train Error: 0.30\n",
      "Test Error: 0.27\n",
      "Train Error: 0.21\n",
      "Test Error: 0.21\n",
      "Train Error: 0.15\n",
      "Test Error: 0.14\n",
      "Train Error: 0.13\n",
      "Test Error: 0.13\n",
      "Train Error: 0.12\n",
      "Test Error: 0.12\n",
      "Train Error: 0.12\n",
      "Test Error: 0.12\n",
      "Train Error: 0.11\n",
      "Test Error: 0.11\n",
      "Train Error: 0.11\n",
      "Test Error: 0.11\n",
      "Train Error: 0.10\n",
      "Test Error: 0.11\n",
      "Train Error: 0.10\n",
      "Test Error: 0.11\n",
      "Train Error: 0.10\n",
      "Test Error: 0.10\n",
      "Train Error: 0.10\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n"
     ]
    }
   ],
   "source": [
    "DATASET = Dataset.get()\n",
    "(training_set, test_set) = DatasetSplitter.split(DATASET, 0.80)\n",
    "\n",
    "trainer = Trainer(\n",
    "    training_set,\n",
    "    test_set,\n",
    "    learning_rate = 0.001\n",
    ")\n",
    "for _ in range(100):\n",
    "    trainer.train_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper class (see below)\n",
    "class RecallResult:\n",
    "    def __init__(self, score_cutoff, num_spams_identified, recall):\n",
    "        self.score_cutoff, self.num_spams_identified, self.recall = (\n",
    "            score_cutoff, num_spams_identified, recall\n",
    "        )\n",
    "\n",
    "# Determines what percentage of spam emails are detected if we can tolerate a given false positive rate.\n",
    "# Does this for multiple false positive rate limits.\n",
    "def recall_for_false_positive_rates(model, dataset, limits):\n",
    "    ham_scores = list(map(\n",
    "        lambda email: model.prob(email.codes),\n",
    "        dataset.encoded_ham_emails\n",
    "    ))\n",
    "    ham_scores.sort(key = lambda score: -score)\n",
    "    spam_scores = list(map(\n",
    "        lambda email: model.prob(email.codes),\n",
    "        dataset.encoded_spam_emails\n",
    "    ))\n",
    "\n",
    "    def calculate_result(limit):\n",
    "        score_cutoff = ham_scores[int(len(ham_scores) * limit)]\n",
    "        num_spams_identified = sum(\n",
    "            [1 if s > score_cutoff else 0 for s in spam_scores]\n",
    "        )\n",
    "        recall = (\n",
    "            num_spams_identified / len(dataset.encoded_spam_emails)\n",
    "        )\n",
    "\n",
    "        return RecallResult(\n",
    "            score_cutoff = score_cutoff,\n",
    "            num_spams_identified = num_spams_identified,\n",
    "            recall = recall,\n",
    "        )\n",
    "\n",
    "    return [\n",
    "        (limit, calculate_result(limit)) for limit in limits\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate 0.001 | Recall 0.65\n",
      "False Positive Rate 0.010 | Recall 0.90\n",
      "False Positive Rate 0.020 | Recall 0.98\n",
      "False Positive Rate 0.040 | Recall 0.99\n",
      "False Positive Rate 0.080 | Recall 1.00\n",
      "False Positive Rate 0.160 | Recall 1.00\n"
     ]
    }
   ],
   "source": [
    "FALSE_POSITIVE_RATES = [0.001, 0.01, 0.02, 0.04, 0.08, 0.16]\n",
    "results = recall_for_false_positive_rates(\n",
    "    trainer.model,\n",
    "    test_set,\n",
    "    FALSE_POSITIVE_RATES\n",
    ")\n",
    "\n",
    "for (false_positive_rate, result) in results:\n",
    "    print(f\"False Positive Rate {false_positive_rate:0.3f} | Recall {result.recall:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
