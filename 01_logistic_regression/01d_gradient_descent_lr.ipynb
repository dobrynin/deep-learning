{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    os.getcwd(),\n",
    "    \"data/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Simple email class. All it does is really allow you to read an email's text.\n",
    "class Email:\n",
    "    def __init__(self, path, label):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "\n",
    "    def text_content(self):\n",
    "        return type(self).read_text_content(self.path)\n",
    "    \n",
    "    def word_counts(self):\n",
    "        counts = {}\n",
    "        for word in self.text_content().split():\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "            \n",
    "        return counts\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, path, label):\n",
    "        return Email(\n",
    "            path = path,\n",
    "            label = label\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def read_text_content(cls, path):\n",
    "        full_path = os.path.join(DATA_DIR, path)\n",
    "        # Grr! Emails are encoded in Latin-1, not UTF-8. Python\n",
    "        # (rightly) freaks out.\n",
    "        with open(full_path, \"r\", encoding = \"iso-8859-1\") as f:\n",
    "            try:\n",
    "                return f.read()\n",
    "            except:\n",
    "                print(f\"Error with: {path}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "# This counts how many emails each word occurs in.\n",
    "def build_word_reaches(ham_emails, spam_emails):\n",
    "    word_reaches = {}\n",
    "    for emails in (ham_emails, spam_emails):\n",
    "        for email in emails:\n",
    "            for (word, _) in email.word_counts().items():\n",
    "                if word not in word_reaches:\n",
    "                    word_reaches[word] = 0\n",
    "                # No matter how frequent in the email, only counts once per email.\n",
    "                word_reaches[word] += 1\n",
    "                \n",
    "    return word_reaches\n",
    "\n",
    "# Throw away those emails that don't occur in at least 100 emails.\n",
    "# Throwing out low reach features means:\n",
    "# (1) Less chance for overfitting\n",
    "# (2) Smaller feature vectors, faster, less memory use.\n",
    "def filter_words(word_reaches, limit = 100):\n",
    "    filtered_words = SortedSet()\n",
    "    for (word, word_reach) in word_reaches.items():\n",
    "        if word_reach >= limit:\n",
    "            filtered_words.add(word)\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "# Bidirectional map. Limits to just filtered words, though.\n",
    "class FilteredWordEncodingDictionary:\n",
    "    def __init__(self, filtered_words):\n",
    "        self.word_to_code_dict = {}\n",
    "        self.code_to_word_dict = {}\n",
    "\n",
    "        for word in filtered_words:\n",
    "            self.insert_word(word)\n",
    "\n",
    "    # Only meant to be called when constructing the dictionary.\n",
    "    def insert_word(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            code = len(self.word_to_code_dict)\n",
    "            self.word_to_code_dict[word] = code\n",
    "            self.code_to_word_dict[code] = word\n",
    "\n",
    "    def word_to_code(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            return None\n",
    "\n",
    "        return self.word_to_code_dict[word]\n",
    "\n",
    "    def code_to_word(self, code):\n",
    "        if code not in self.code_to_word_dict:\n",
    "            raise f\"Code {code} not recorded!\"\n",
    "\n",
    "        return self.code_to_word_dict[code]\n",
    "\n",
    "    # This returns a vector of ones and zeros.\n",
    "    def encode_text(self, text):\n",
    "        codes = np.zeros(len(self.code_to_word_dict))\n",
    "\n",
    "        for word in text.split():\n",
    "            code = self.word_to_code(word)\n",
    "            if code is not None:\n",
    "                codes[code] = 1.0\n",
    "\n",
    "        return codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_to_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a simple subclass of Email that just encodes the words in an email.\n",
    "class EncodedEmail(Email):\n",
    "    def __init__(self, path, label, word_encoding_dictionary):\n",
    "        super().__init__(path, label)\n",
    "\n",
    "        self.codes = (\n",
    "            word_encoding_dictionary.encode_text(\n",
    "                self.text_content()\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "class Dataset:\n",
    "    DATA_FILE_PATH = os.path.join(DATA_DIR, 'lr_data.p')\n",
    "    WORD_REACH_LIMIT = 100\n",
    "\n",
    "    def __init__(\n",
    "            self, word_encoding_dictionary, encoded_ham_emails, encoded_spam_emails\n",
    "    ):\n",
    "        self.word_encoding_dictionary = word_encoding_dictionary\n",
    "        self.encoded_ham_emails = encoded_ham_emails\n",
    "        self.encoded_spam_emails = encoded_spam_emails\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def encode(cls, ham_emails, spam_emails):\n",
    "        # Count words, select which we will keep.\n",
    "        word_reaches = build_word_reaches(ham_emails, spam_emails)\n",
    "        filtered_words = filter_words(word_reaches, limit = cls.WORD_REACH_LIMIT)\n",
    "        \n",
    "        # Assign codes to all words.\n",
    "        word_encoding_dictionary = FilteredWordEncodingDictionary(filtered_words)\n",
    "        \n",
    "        # Encode each email as a vector of ones and zeros.\n",
    "        encoded_ham_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in ham_emails\n",
    "        ]\n",
    "        encoded_spam_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in spam_emails\n",
    "        ]\n",
    "        \n",
    "        # Construct the object!\n",
    "        return cls(\n",
    "            word_encoding_dictionary,\n",
    "            encoded_ham_emails,\n",
    "            encoded_spam_emails\n",
    "        )\n",
    "\n",
    "    INSTANCE = None\n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if not cls.INSTANCE:\n",
    "            with open(cls.DATA_FILE_PATH, 'rb') as f:\n",
    "                cls.INSTANCE = pickle.load(f)\n",
    "        return cls.INSTANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarfile already downloaded!\n",
      "Tarfile already extracted!\n",
      "Reading and processing emails!\n",
      "Dataset created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "ENRON_SPAM_URL = (\n",
    "    \"http://csmining.org/index.php/\"\n",
    "    \"enron-spam-datasets.html\"\n",
    "    \"?file=tl_files/Project_Datasets/Enron-Spam%20datasets/Preprocessed\"\n",
    "    \"/enron1.tar.tar\"\n",
    ")\n",
    "\n",
    "TAR_FILE_NAME = \"enron1.tar.tar\"\n",
    "ENRON_DATA_DIR_NAME = \"enron1\"\n",
    "\n",
    "def download_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    if os.path.isfile(tarfile_path):\n",
    "        print(\"Tarfile already downloaded!\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading enron1.tar.tar\")\n",
    "    urlretrieve(ENRON_SPAM_URL, tarfile_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    enron_data_dir = os.path.join(DATA_DIR, ENRON_DATA_DIR_NAME)\n",
    "    if os.path.isdir(enron_data_dir):\n",
    "        print(\"Tarfile already extracted!\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting enron1.tar.tar\")\n",
    "    os.system(f\"tar -xf {tarfile_path} -C {DATA_DIR}\")\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def read_emails_dir(path, label):\n",
    "    emails = []\n",
    "    for email_fname in os.listdir(os.path.join(DATA_DIR, path)):\n",
    "        email_path = os.path.join(path, email_fname)\n",
    "        email = Email.read(\n",
    "            path = email_path,\n",
    "            label = label\n",
    "        )\n",
    "        emails.append(email)\n",
    "\n",
    "    return emails\n",
    "\n",
    "def build_dataset():\n",
    "    ham_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"ham\"),\n",
    "        label = 0\n",
    "    )\n",
    "    spam_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"spam\"),\n",
    "        label = 1\n",
    "    )\n",
    "\n",
    "    return Dataset.encode(\n",
    "        ham_emails = ham_emails,\n",
    "        spam_emails = spam_emails\n",
    "    )\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    with open(Dataset.DATA_FILE_PATH, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def build_and_save_dataset():\n",
    "    if os.path.isfile(Dataset.DATA_FILE_PATH):\n",
    "        print(\"Dataset already processed!\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading and processing emails!\")\n",
    "    dataset = build_dataset()\n",
    "    save_dataset(dataset)\n",
    "    print(\"Dataset created!\")\n",
    "\n",
    "download_tarfile()\n",
    "extract_tarfile()\n",
    "build_and_save_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, word_encoding_dictionary):\n",
    "        self.theta = np.zeros(\n",
    "            len(word_encoding_dictionary)\n",
    "        )\n",
    "        \n",
    "    def prob(self, codes):\n",
    "        return logistic(self.theta.dot(codes))\n",
    "    \n",
    "    def error(self, email):\n",
    "        if email.label == 0:\n",
    "            return -np.log(1 - self.prob(email.codes))\n",
    "        else:\n",
    "            return -np.log(self.prob(email.codes))\n",
    "\n",
    "    def partial_derivatives(self, email):\n",
    "        if email.label == 0:\n",
    "            # This is a vectorized version.\n",
    "            return (\n",
    "                email.codes * self.prob(email.codes)\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                -email.codes * (1 - self.prob(email.codes))\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, dataset, learning_rate):\n",
    "        self.dataset = dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = LogisticRegressionModel(dataset.word_encoding_dictionary)\n",
    "        \n",
    "    def error(self):\n",
    "        error = 0.0\n",
    "        for emails in (self.dataset.encoded_ham_emails, self.dataset.encoded_spam_emails):\n",
    "            for email in emails:\n",
    "                error += self.model.error(email)\n",
    "                \n",
    "        return error\n",
    "    \n",
    "    def partial_derivatives(self):\n",
    "        partials = np.zeros(len(self.dataset.word_encoding_dictionary))\n",
    "        for emails in (self.dataset.encoded_ham_emails, self.dataset.encoded_spam_emails):\n",
    "            for email in emails:\n",
    "                partials += self.model.partial_derivatives(email)\n",
    "                \n",
    "        return partials\n",
    "\n",
    "    def train_step(self):\n",
    "        print(f\"Current Error: {self.error():0.2f}\")\n",
    "        self.model.theta -= self.learning_rate * self.partial_derivatives()\n",
    "        print(f\"Next Error: {self.error():0.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Error: 3584.96\n",
      "Next Error: 19360.43\n",
      "Current Error: 19360.43\n",
      "Next Error: 2925.03\n",
      "Current Error: 2925.03\n",
      "Next Error: 11382.55\n",
      "Current Error: 11382.55\n",
      "Next Error: 7686.49\n",
      "Current Error: 7686.49\n",
      "Next Error: 19145.61\n",
      "Current Error: 19145.61\n",
      "Next Error: 930.66\n",
      "Current Error: 930.66\n",
      "Next Error: 848.96\n",
      "Current Error: 848.96\n",
      "Next Error: 800.15\n",
      "Current Error: 800.15\n",
      "Next Error: 747.41\n",
      "Current Error: 747.41\n",
      "Next Error: 712.39\n",
      "Current Error: 712.39\n",
      "Next Error: 676.24\n",
      "Current Error: 676.24\n",
      "Next Error: 650.94\n",
      "Current Error: 650.94\n",
      "Next Error: 623.77\n",
      "Current Error: 623.77\n",
      "Next Error: 604.88\n",
      "Current Error: 604.88\n",
      "Next Error: 581.32\n",
      "Current Error: 581.32\n",
      "Next Error: 565.04\n",
      "Current Error: 565.04\n",
      "Next Error: 542.08\n",
      "Current Error: 542.08\n",
      "Next Error: 526.02\n",
      "Current Error: 526.02\n",
      "Next Error: 503.48\n",
      "Current Error: 503.48\n",
      "Next Error: 487.45\n",
      "Current Error: 487.45\n",
      "Next Error: 467.31\n",
      "Current Error: 467.31\n",
      "Next Error: 452.83\n",
      "Current Error: 452.83\n",
      "Next Error: 437.28\n",
      "Current Error: 437.28\n",
      "Next Error: 425.84\n",
      "Current Error: 425.84\n",
      "Next Error: 415.09\n",
      "Current Error: 415.09\n",
      "Next Error: 406.67\n",
      "Current Error: 406.67\n",
      "Next Error: 399.18\n",
      "Current Error: 399.18\n",
      "Next Error: 392.83\n",
      "Current Error: 392.83\n",
      "Next Error: 387.13\n",
      "Current Error: 387.13\n",
      "Next Error: 382.00\n",
      "Current Error: 382.00\n",
      "Next Error: 377.27\n",
      "Current Error: 377.27\n",
      "Next Error: 372.88\n",
      "Current Error: 372.88\n",
      "Next Error: 368.76\n",
      "Current Error: 368.76\n",
      "Next Error: 364.88\n",
      "Current Error: 364.88\n",
      "Next Error: 361.20\n",
      "Current Error: 361.20\n",
      "Next Error: 357.70\n",
      "Current Error: 357.70\n",
      "Next Error: 354.36\n",
      "Current Error: 354.36\n",
      "Next Error: 351.18\n",
      "Current Error: 351.18\n",
      "Next Error: 348.13\n",
      "Current Error: 348.13\n",
      "Next Error: 345.20\n",
      "Current Error: 345.20\n",
      "Next Error: 342.40\n",
      "Current Error: 342.40\n",
      "Next Error: 339.70\n",
      "Current Error: 339.70\n",
      "Next Error: 337.09\n",
      "Current Error: 337.09\n",
      "Next Error: 334.59\n",
      "Current Error: 334.59\n",
      "Next Error: 332.16\n",
      "Current Error: 332.16\n",
      "Next Error: 329.82\n",
      "Current Error: 329.82\n",
      "Next Error: 327.55\n",
      "Current Error: 327.55\n",
      "Next Error: 325.35\n",
      "Current Error: 325.35\n",
      "Next Error: 323.22\n",
      "Current Error: 323.22\n",
      "Next Error: 321.15\n",
      "Current Error: 321.15\n",
      "Next Error: 319.14\n",
      "Current Error: 319.14\n",
      "Next Error: 317.18\n",
      "Current Error: 317.18\n",
      "Next Error: 315.28\n",
      "Current Error: 315.28\n",
      "Next Error: 313.42\n",
      "Current Error: 313.42\n",
      "Next Error: 311.61\n",
      "Current Error: 311.61\n",
      "Next Error: 309.84\n",
      "Current Error: 309.84\n",
      "Next Error: 308.12\n",
      "Current Error: 308.12\n",
      "Next Error: 306.44\n",
      "Current Error: 306.44\n",
      "Next Error: 304.79\n",
      "Current Error: 304.79\n",
      "Next Error: 303.18\n",
      "Current Error: 303.18\n",
      "Next Error: 301.61\n",
      "Current Error: 301.61\n",
      "Next Error: 300.07\n",
      "Current Error: 300.07\n",
      "Next Error: 298.56\n",
      "Current Error: 298.56\n",
      "Next Error: 297.08\n",
      "Current Error: 297.08\n",
      "Next Error: 295.63\n",
      "Current Error: 295.63\n",
      "Next Error: 294.21\n",
      "Current Error: 294.21\n",
      "Next Error: 292.81\n",
      "Current Error: 292.81\n",
      "Next Error: 291.44\n",
      "Current Error: 291.44\n",
      "Next Error: 290.10\n",
      "Current Error: 290.10\n",
      "Next Error: 288.78\n",
      "Current Error: 288.78\n",
      "Next Error: 287.48\n",
      "Current Error: 287.48\n",
      "Next Error: 286.21\n",
      "Current Error: 286.21\n",
      "Next Error: 284.96\n",
      "Current Error: 284.96\n",
      "Next Error: 283.73\n",
      "Current Error: 283.73\n",
      "Next Error: 282.52\n",
      "Current Error: 282.52\n",
      "Next Error: 281.33\n",
      "Current Error: 281.33\n",
      "Next Error: 280.16\n",
      "Current Error: 280.16\n",
      "Next Error: 279.00\n",
      "Current Error: 279.00\n",
      "Next Error: 277.87\n",
      "Current Error: 277.87\n",
      "Next Error: 276.75\n",
      "Current Error: 276.75\n",
      "Next Error: 275.65\n",
      "Current Error: 275.65\n",
      "Next Error: 274.56\n",
      "Current Error: 274.56\n",
      "Next Error: 273.49\n",
      "Current Error: 273.49\n",
      "Next Error: 272.44\n",
      "Current Error: 272.44\n",
      "Next Error: 271.40\n",
      "Current Error: 271.40\n",
      "Next Error: 270.38\n",
      "Current Error: 270.38\n",
      "Next Error: 269.37\n",
      "Current Error: 269.37\n",
      "Next Error: 268.37\n",
      "Current Error: 268.37\n",
      "Next Error: 267.39\n",
      "Current Error: 267.39\n",
      "Next Error: 266.42\n",
      "Current Error: 266.42\n",
      "Next Error: 265.46\n",
      "Current Error: 265.46\n",
      "Next Error: 264.52\n",
      "Current Error: 264.52\n",
      "Next Error: 263.59\n",
      "Current Error: 263.59\n",
      "Next Error: 262.67\n",
      "Current Error: 262.67\n",
      "Next Error: 261.76\n",
      "Current Error: 261.76\n",
      "Next Error: 260.86\n",
      "Current Error: 260.86\n",
      "Next Error: 259.98\n",
      "Current Error: 259.98\n",
      "Next Error: 259.10\n",
      "Current Error: 259.10\n",
      "Next Error: 258.24\n",
      "Current Error: 258.24\n",
      "Next Error: 257.38\n"
     ]
    }
   ],
   "source": [
    "DATASET = Dataset.get()\n",
    "t = Trainer(DATASET, learning_rate = 0.001)\n",
    "for _ in range(100):\n",
    "    t.train_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
