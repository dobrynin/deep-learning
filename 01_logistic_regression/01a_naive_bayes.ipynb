{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Our task for today will be to classify emails as spam or not spam. We sill use the [Enron Email Corpus](https://en.wikipedia.org/wiki/Enron_Corpus). The dataset contains email text along with a label of whether that text was spam or not.\n",
    "\n",
    "First, let's load the dataset, and look at an example too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarfile already downloaded!\n",
      "Tarfile already extracted!\n",
      "Dataset already processed!\n",
      "\n",
      ">>> HAM EMAIL:\n",
      "========================================================================\n",
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n",
      "\n",
      ">>> SPAM EMAIL:\n",
      "========================================================================\n",
      "Subject: re : rdd , the auxiliary iturean\n",
      "free cable @ tv\n",
      "dabble bam servomechanism ferret canopy bookcase befog seductive elapse ballard daphne acrylate deride decadent desolate else sequestration condition ligament ornately yaqui giblet emphysematous woodland lie segovia almighty coffey shut china clubroom diagnostician\n",
      "cheer leadsman abominate cambric oligarchy mania woodyard quake tetrachloride contiguous welsh depressive synaptic trauma cloister banks canadian byroad alexander gnaw annette charlie\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import lib.download\n",
    "lib.download.run()\n",
    "\n",
    "import lib.dataset\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "\n",
    "print()\n",
    "print(\">>> HAM EMAIL:\")\n",
    "print(\"=\" * 72)\n",
    "print(RAW_DATASET.ham_emails[5].text_content())\n",
    "\n",
    "print()\n",
    "print(\">>> SPAM EMAIL:\")\n",
    "print(\"=\" * 72)\n",
    "print(RAW_DATASET.spam_emails[10].text_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the email text is all lower case, and each *token* (words, but also symbols like \"@\") is seperated by a space. The subject line is not technically part of the email body, but I will leave it in anyway.\n",
    "\n",
    "For the purposes of our algorithm, we will convert emails into a set of words, throwing away the order of the words, and also how frequently they occur in the email. Every token will be represented with an integer, rather than the word itself.\n",
    "\n",
    "We will represent whether an email is spam or not with a 1 for spam and a 0 for not spam. This is called the *label*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SortedSet([0, 7, 13, 27, 69, 209, 306, 679, 1076, 1466, 1492, 3200, 3498, 3880, 4518, 5332, 5673, 6040, 6075, 7845, 10796, 11378, 11981, 12835, 14532, 16407, 17924, 20486, 20487, 20488, 20489, 20490, 20491, 20492, 20493, 20494, 20495, 20496, 20497, 20498, 20499, 20500, 20501, 20502, 20503, 20504, 20505, 20506, 20507, 20508, 20509, 20510, 20511, 20512, 20513, 20514, 20515, 20516, 20517, 20518, 20519, 20520, 20521, 20522, 20523, 20524], key=None, load=1000)\n",
      "SortedSet([',', ':', '@', 'Subject:', 'abominate', 'acrylate', 'alexander', 'almighty', 'annette', 'auxiliary', 'ballard', 'bam', 'banks', 'befog', 'bookcase', 'byroad', 'cable', 'cambric', 'canadian', 'canopy', 'charlie', 'cheer', 'china', 'cloister', 'clubroom', 'coffey', 'condition', 'contiguous', 'dabble', 'daphne', 'decadent', 'depressive', 'deride', 'desolate', 'diagnostician', 'elapse', 'else', 'emphysematous', 'ferret', 'free', 'giblet', 'gnaw', 'iturean', 'leadsman', 'lie', 'ligament', 'mania', 'oligarchy', 'ornately', 'quake', 'rdd', 're', 'seductive', 'segovia', 'sequestration', 'servomechanism', 'shut', 'synaptic', 'tetrachloride', 'the', 'trauma', 'tv', 'welsh', 'woodland', 'woodyard', 'yaqui'], key=None, load=1000)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(RAW_DATASET.spam_emails[10].codes)\n",
    "print(RAW_DATASET.spam_emails[10].words())\n",
    "print(RAW_DATASET.spam_emails[10].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the preprocessing of the dataset *featurization*. The machine learning algorithm will interact with the *featurized* emails (the set of numbers and the 0/1 label), rather than the raw emails themselves.\n",
    "\n",
    "Featurization is an important part of data preparation. For instance, we saw previously in the linear regression notebooks that it was very helpful to normalize the mean and variance of continuous valued variables. Other featurization techniques are to lowercase words and to normalize whitespace (replace repeated whitespace characters with a single space). Sometimes *stemming* is done: this tries to normalize a word like \"robots\" to \"robot\", removing the \"s\". The \"stem\" is the base word. This is done because often the plethora of minor variants of words can be confusing to ML algorithms.\n",
    "\n",
    "It is not uncommon to throw away word order and word counts. This representation of text is called the *bag of words model*. Obviously a lot of information is lost with this representation. For some tasks like document retrieval based on keyword matching, the bag of words model can still be useful. For tasks like spam/not-spam bag of words performs well.\n",
    "\n",
    "For tasks which need deeper *semantic* understanding of a document (understanding what it means), we would want to use techniques which can exploit the information contained in the word ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Probabilities\n",
    "\n",
    "To detect which emails are spam and which aren't, we will use the observation that some words are more probable to appear in a spam email rather than in a non-spam email. For instance, I suspect that \"offer\" is more probable to appear in a spam email than a non-spam email. In notation:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{OFFER} = 1 \\condbar \\text{SPAM} = 1}\n",
    ">\n",
    "\\prob{\\text{OFFER} = 1 \\condbar \\text{SPAM} = 0}\n",
    "\\\\]\n",
    "\n",
    "Now, what I really want to know is what is the probability that an email is spam if it contains the word \"offer\". That is, I want to calculate:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 1 \\condbar \\text{OFFER} = 1}\n",
    "\\\\]\n",
    "\n",
    "To do so, I will use *Bayes' Rule*:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 1 \\condbar \\text{OFFER} = 1}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 1 \\wedge \\text{OFFER} = 1}\n",
    "}{\n",
    "    \\prob{\\text{OFFER} = 1}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 1}\n",
    "    \\cdot\n",
    "    \\prob{\\text{OFFER} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{OFFER} = 1}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "It is frequently convenient to consider the *odds* that something is true, rather than the probability. If the probability of $X$ is $p$, then the *odds* of $X$ are $\\frac{p}{1-p}$. For instance, a probability of $0.66$ corresponds to an odds of $2$, sometimes written $2:1$.\n",
    "\n",
    "To calculate the odds that an email is spam if it has the word offer in it, first apply Bayes' Rule again to the probability that the email is not spam:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 0 \\condbar \\text{OFFER} = 1}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 0}\n",
    "    \\cdot\n",
    "    \\prob{\\text{OFFER} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}{\n",
    "    \\prob{\\text{OFFER} = 1}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Last, let's compute the odds:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 1 \\condbar \\text{OFFER} = 1}\n",
    "}{\n",
    "    \\prob{\\text{SPAM} = 0 \\condbar \\text{OFFER} = 1}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{SPAM} = 0}\n",
    "}\n",
    "\\cdot\n",
    "\\frac{\n",
    "    \\prob{\\text{OFFER} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{OFFER} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "The second factor in the product I will call the *feature probability ratio*. It is the ratio of probabilities that the feature will occur conditional on $SPAM = 1$ versus when $SPAM = 0$. When this ratio is $>1$ then the feature is a positive indicator of spam, because multiplying the *base (unconditional) spam odds* by it increases the odds that it is spam. Likewise, if the ratio is $<1$, it is an indicator of ham, since it reduces the odds of spam.\n",
    "\n",
    "Alright! Let's do this with code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prob_feature_given_ham': 0.01661220043572985, 'prob_feature_given_spam': 0.094, 'feature_probability_ratio': 5.658491803278688}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lib.dataset\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "\n",
    "feature_probabilities = RAW_DATASET.feature_probabilities()\n",
    "feature_probabilities.code_prob_ratio(\n",
    "    code = RAW_DATASET.word_encoding_dictionary.word_to_code('offer')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the word \"offer\" is not very common in either ham or spam. However, it is $4.15$ times as likely to occur in a spam email versus a ham email. Therefore it is a strong indicator of spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Quantities Can Be Inaccurate\n",
    "\n",
    "To calculate the feature probability ratio, we are dividing two conditional probabilities: $\\prob{\\text{OFFER} = 1| \\text{SPAM} = 1}$ and $\\prob{\\text{OFFER} = 1| \\text{SPAM} = 0}$.\n",
    "\n",
    "We do not know what those probabilities are for all emails out there in the world. We call the universe of all emails the *population*. The probability that something happens in the population is called a *population probability*.\n",
    "\n",
    "We never observe the entire population. We observe a sample dataset. In this case, we see some Enron emails, but the whole point is we want to extrapolate what we learn about Enron emails to the wider population.\n",
    "\n",
    "The probability that something happens in our dataset is called an *empirical probability*. An *empirical* quantity is one that we are calculating using an observed sample. Empirical means \"known from experience or observation.\"\n",
    "\n",
    "If we gather more and more data, a empirical probability will converge to the true population probability. But if the sample size is too small, then the empirical probability may be a very poor *approximation* of the true population probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts | {'ham_count': 0, 'spam_count': 1}\n",
      "Feature Probability Ratio | {'prob_feature_given_ham': 0.0, 'prob_feature_given_spam': 0.0006666666666666666, 'feature_probability_ratio': inf}\n"
     ]
    }
   ],
   "source": [
    "import lib.dataset\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "\n",
    "code = RAW_DATASET.word_encoding_dictionary.word_to_code('bacterial')\n",
    "\n",
    "print(f\"Counts | {feature_probabilities.code_counts[code]}\")\n",
    "print(f\"Feature Probability Ratio | {feature_probabilities.code_prob_ratio(code)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, only one email had the word \"bacterial\" in it. It was a spam email. Therefore, the empirical probabilities are:\n",
    "\n",
    "\\\\[\n",
    "\\eprob{\\text{SPAM} = 1 \\condbar \\text{BACTERIAL} = 1} = 1.0\\\\\n",
    "\\eprob{\\text{SPAM} = 0 \\condbar \\text{BACTERIAL} = 1} = 0.0\n",
    "\\\\]\n",
    "\n",
    "I'm explicitly putting this hat on to make more obvious that these empirical probabilities are *estimates* of the population probabilities.\n",
    "\n",
    "Nobody thinks that the presence of the word \"bacterial\" guarantees an email is spam. In a case like this where the *reach* of a feature is only one example, the empirical probability can only be either $0.0$ or $1.0$. Both examples are completely extreme.\n",
    "\n",
    "The problem is that the word \"bacterial\" is uncommon enough that it will take more emails before we can start to reliably estimate its probability. In general: it takes more data to accurately estimate the probability of very rare events.\n",
    "\n",
    "As you can see, the feature probability ratio here is $\\infty$. That is stupid. If we used this feature to grade new emails, it would say any email with the word \"bacterial\" *must* be spam. We want to avoid stupidity like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discarding Low Reach Features\n",
    "\n",
    "How do we deal with this problem? There is more than one way. Let's adopt the simplest: let's throw away all features with a reach of less than twenty examples.\n",
    "\n",
    "To verify that the remaining features seem to be good, let's look at the top indicators that an email is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17077 | 2004 | reach: {'ham_count': 1, 'spam_count': 121} | feature_probability_ratio: 296.21:1\n",
      "5969 | microsoft | reach: {'ham_count': 11, 'spam_count': 98} | feature_probability_ratio: 21.81:1\n",
      "3104 | investment | reach: {'ham_count': 11, 'spam_count': 96} | feature_probability_ratio: 21.36:1\n",
      "3522 | results | reach: {'ham_count': 18, 'spam_count': 98} | feature_probability_ratio: 13.33:1\n",
      "370 | v | reach: {'ham_count': 26, 'spam_count': 134} | feature_probability_ratio: 12.62:1\n",
      "3951 | million | reach: {'ham_count': 20, 'spam_count': 97} | feature_probability_ratio: 11.87:1\n",
      "680 | stop | reach: {'ham_count': 31, 'spam_count': 147} | feature_probability_ratio: 11.61:1\n",
      "3900 | software | reach: {'ham_count': 22, 'spam_count': 101} | feature_probability_ratio: 11.24:1\n",
      "5621 | 80 | reach: {'ham_count': 23, 'spam_count': 104} | feature_probability_ratio: 11.07:1\n",
      "4002 | dollars | reach: {'ham_count': 26, 'spam_count': 113} | feature_probability_ratio: 10.64:1\n",
      "4611 | remove | reach: {'ham_count': 28, 'spam_count': 110} | feature_probability_ratio: 9.62:1\n",
      "3462 | stock | reach: {'ham_count': 22, 'spam_count': 84} | feature_probability_ratio: 9.35:1\n",
      "4795 | removed | reach: {'ham_count': 22, 'spam_count': 83} | feature_probability_ratio: 9.24:1\n",
      "2772 | money | reach: {'ham_count': 50, 'spam_count': 187} | feature_probability_ratio: 9.16:1\n",
      "2737 | world | reach: {'ham_count': 34, 'spam_count': 124} | feature_probability_ratio: 8.93:1\n",
      "2512 | save | reach: {'ham_count': 35, 'spam_count': 125} | feature_probability_ratio: 8.74:1\n",
      "2518 | http | reach: {'ham_count': 135, 'spam_count': 475} | feature_probability_ratio: 8.61:1\n",
      "570 | quality | reach: {'ham_count': 29, 'spam_count': 101} | feature_probability_ratio: 8.53:1\n",
      "4517 | canada | reach: {'ham_count': 23, 'spam_count': 79} | feature_probability_ratio: 8.41:1\n",
      "3347 | low | reach: {'ham_count': 31, 'spam_count': 106} | feature_probability_ratio: 8.37:1\n"
     ]
    }
   ],
   "source": [
    "import lib.dataset\n",
    "from lib.feature_probabilities_explorer import FeatureProbabilitiesExplorer\n",
    "\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "\n",
    "filtered_feature_probabilities = RAW_DATASET.feature_probabilities().filter(reach_limit = 100)\n",
    "best_spam_features = FeatureProbabilitiesExplorer.best_spam_features(\n",
    "    filtered_feature_probabilities\n",
    ")\n",
    "FeatureProbabilitiesExplorer.print_features_list(\n",
    "    best_spam_features, RAW_DATASET.word_encoding_dictionary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That list looks okay. It is a little bizarre to see 2004 up there. Let's move on for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Of Features Have Low Reach\n",
    "\n",
    "We want to use more than just a single word to characterize whether an email is spam. For instance, I want:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 1 \\condbar \\text{OFFER} = 1 \\wedge \\text{LIMITED} = 1}\n",
    "\\\\]\n",
    "\n",
    "Now, it is possible to count how many emails have *both* the words \"limited\" and \"offer.\" We could then count how many of those emails are spam. From these two counts we could calculate the empirical probability\n",
    "\n",
    "\\\\[\n",
    "\\eprob{\\text{SPAM} = 1 \\condbar \\text{OFFER} = 1 \\wedge \\text{LIMITED} = 1}\n",
    "\\\\]\n",
    "\n",
    "However, we're just going to go back to our old problem. Even if there are enough examples of \"offer\" and \"limited\" to accurately estimate\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 1 \\wedge \\text{LIMITED} = 1}\n",
    "\\\\\n",
    "\\prob{\\text{OFFER} = 1 \\wedge \\text{LIMITED} = 1}\n",
    "\\\\]\n",
    "\n",
    "that doesn't mean that there are enough examples of emails with *both* \"offer\" and \"limited\".\n",
    "\n",
    "The problem gets worse. Our ultimate goal is try to estimate:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 1 \\condbar \\text{all the words in the email}}\n",
    "\\\\]\n",
    "\n",
    "Since each new email likely represents a unique bag of words, there is almost no chance we've ever seen an email exactly like this. So we have *no* empirical estimate for this probability!\n",
    "\n",
    "We need to figure a way a conditional probability for a pair (or more) of words from using our relatively accurate estimates of the conditional probability for single words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence\n",
    "\n",
    "Say we want to calculate:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 1\\condbar \\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1}\n",
    "\\\\]\n",
    "\n",
    "The empirical probability may well be a poor estimate because too few emails might have both these words. But let's try to do some math and see where we get. Let's apply Bayes' rule as before:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{SPAM} = 1\\condbar \\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 1}\\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "We'll do the feature probability ratio thing again:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 1\\condbar \\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1}\n",
    "}{\n",
    "    \\prob{\\text{SPAM} = 0\\condbar \\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{SPAM} = 1}\\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{SPAM} = 0}\\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Now, wouldn't it beautiful if:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "If this were true, it would say we could calculate the combined effect of having both \"investment\" and \"quality\" in an email by multiplying the two effects of having (1) \"investment\" and (2) \"quality\" in an email. Since we have good estimates of those individual feature probability ratios, we'll have a good estimate of the combined effect.\n",
    "\n",
    "When *would* this be true? Well, consider:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "\\\\\n",
    "=\n",
    "\\\\\n",
    "\\prob{\\text{INVESTMENT} = 1 \\condbar \\text{SPAM} = 1}\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1 \\wedge \\text{INVESTMENT} = 1}\n",
    "\\\\]\n",
    "\n",
    "What did I just do here? Well: I just applied Bayes' rule. So this shows that we'll have our desired \"compounding\" of effects if:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1 \\wedge \\text{INVESTMENT} = 1}\n",
    "=\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "\\\\]\n",
    "\n",
    "This property is called *conditional independence*. We say that the presence of the word \"quality\" is conditionally independent from the presence of the word \"investment\", *given* that we know the email to be spam.\n",
    "\n",
    "Let's explore what conditional independence means by seeing some ways a pair of words can *fail* to be conditionally independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples When Unconditional Independence Is Violated\n",
    "\n",
    "Let me show that the presence of \"quality\" and \"investment\" are not *unconditionally* independent. That is:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{INVESTMENT} = 1}\n",
    "\\ne\n",
    "\\prob{\\text{QUALITY} = 1}\n",
    "\\\\]\n",
    "\n",
    "Here's why this isn't true in general. The presence of the word \"investment\" suggests that the email is spam. And, if the email is spam, then it is more likely to contain the word \"quality\" than the average email. That means that the presence of the word \"investment\" makes the presence of the word \"quality\" more likely. Which is to say: the presence of \"investment\" and the presence of \"quality\" are not independent of each other. For that reason, I say:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{INVESTMENT} = 1}\n",
    ">\n",
    "\\prob{\\text{QUALITY} = 1}\n",
    "\\\\]\n",
    "\n",
    "The reason is that \"investment\" indicates the email is spam, and spam emails are likely to have the word \"quality\".\n",
    "\n",
    "However, what if somehow I already know that an email is spam? In that case, the word \"investment\" doesn't really give me any new information about whether the email is spam, because I already know for 100% that the email is spam. In that case, the chain of reasoning from above doesn't apply.\n",
    "\n",
    "The same thing happens if I know that an email is not spam. In that case, even though \"investment\" normally indicates spam, in this case I know for sure the email *isn't* spam. Again, \"investment\" doesn't hint at the presence of \"quality\" via indicating spaminess when we know the email isn't spam.\n",
    "\n",
    "Thus, we are getting close to saying that:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{INVESTMENT} = 1}\n",
    "=\n",
    "\\prob{\\text{QUALITY} = 1}\n",
    "\\\\]\n",
    "\n",
    "unless somehome \"investment\" indicates the presence of \"quality\" in some *other* way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples When Conditional Independence Is Violated\n",
    "\n",
    "The question then becomes: does the presence of the word investment change the probability of \"quality\" appearing for *any other* reason? Now, it may be possible that the words \"quality\" and \"investment\" often appear in the same spam emails because \"investment\" frequently appears as part of the compound phrase \"a quality investment\". If that were true, it may be that:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1 \\wedge \\text{INVESTMENT} = 1}\n",
    ">\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "\\\\]\n",
    "\n",
    "On the other hand, considering another pair of words, it may be that \"baldness\" is less likely given the presence of the word \"investment\" in spam emails. That might be because a spam email either pitches an investment or a baldness cure, but not typically both. In that case, the presence of \"investment\" might *inhibit* \"baldness.\" If this is true:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{BALDNESS} = 1 \\condbar \\text{SPAM} = 1 \\wedge \\text{INVESTMENT} = 1}\n",
    "<\n",
    "\\prob{\\text{BALDNESS} = 1 \\condbar \\text{SPAM} = 1}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Just Pretend\n",
    "\n",
    "We've seen that, independent of their effect at hinting at whether an email is spam, a word like \"investment\" may (1) indicate the presence of \"quality\" or (2) indicate the absence of \"baldness.\"\n",
    "\n",
    "However, let's just pretend like that didn't happen. Let's just assume:\n",
    "\n",
    "\\\\[\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1 \\wedge \\text{INVESTMENT} = 1}\n",
    "=\n",
    "\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\\\\\n",
    "\\prob{\\text{BALDNESS} = 1 \\condbar \\text{SPAM} = 1 \\wedge \\text{INVESTMENT} = 1}\n",
    "=\n",
    "\\prob{\\text{BALDNESS} = 1 \\condbar \\text{SPAM} = 1}\n",
    "\\\\]\n",
    "\n",
    "We know this probably isn't true, but let's just pretend. The advantage is that we have pretty good estimates on quantities like $\\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}$. And doing so let's us combine them in a simple (if somewhat unjustified) way:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\wedge \\text{QUALITY} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{INVESTMENT} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\frac{\n",
    "    \\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{\\text{QUALITY} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "More generally, consider an email contains the words $W_{i_1}, \\ldots, W_{i_k}$. I have used $i_1,\\ldots,i_k$ to represent that these are a subset of $k$ words in the set of all words. If there are $\\card{W}$ words, then the indices range over $1,\\ldots,\\card{W}$, but unless the email contains all words $k < \\card{W}$.\n",
    "\n",
    "Anyway, if the email contains these words, then:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{W_{i_1} = 1, \\ldots, W_{i_k} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{W_{i_1} = 1, \\ldots, W_{i_k} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{W_{i_1} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{W_{i_1} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\cdots\n",
    "\\frac{\n",
    "    \\prob{W_{i_k} = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{W_{i_k} = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Okay! Let's try this out and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate 0.001 | Recall 0.08\n",
      "False Positive Rate 0.010 | Recall 0.62\n",
      "False Positive Rate 0.020 | Recall 0.88\n",
      "False Positive Rate 0.040 | Recall 0.98\n",
      "False Positive Rate 0.080 | Recall 1.00\n",
      "False Positive Rate 0.160 | Recall 1.00\n"
     ]
    }
   ],
   "source": [
    "import lib.dataset\n",
    "from lib.naive_bayes_evaluator import NaiveBayesEvaluator\n",
    "from lib.naive_bayes_model import NaiveBayesModel\n",
    "\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "\n",
    "model = NaiveBayesModel(\n",
    "    RAW_DATASET.feature_probabilities().filter(reach_limit = 100),\n",
    "    use_negative_features = False\n",
    ")\n",
    "FALSE_POSITIVE_RATES = [0.001, 0.01, 0.02, 0.04, 0.08, 0.16]\n",
    "results = NaiveBayesEvaluator.recall_for_false_positive_rates(\n",
    "    model,\n",
    "    RAW_DATASET,\n",
    "    FALSE_POSITIVE_RATES\n",
    ")\n",
    "for (false_positive_rate, result) in results:\n",
    "    print(f\"False Positive Rate {false_positive_rate:0.3f} | Recall {result.recall:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *false positive rate* is the percentage of ham emails that were marked as spam. *Recall* is the percentage of percentage of spam emails that were identified as spam. Recall is the same as the *true positive rate*. Obviously the ideal is to have a false positive rate of zero and a recall of one.\n",
    "\n",
    "These results are encouraging. It says that if we're okay with marking one in a hundred ham emails as spam, we'll catch 62% of the spam. And if we are okay with two in a hundred ham emails, we'll catch 88% of spam.\n",
    "\n",
    "This isn't quite good enough for real life systems, but it isn't that bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Absence (Sometimes) Matters Too!\n",
    "\n",
    "See that `use_negative_features = False`? Right now we've only been using words that were observed: words whose presence indicates spam or not spam.\n",
    "\n",
    "Another question is what about the *absence* of words? What if all spam emails contain a word? If it is absent, then we know this is a ham email. But our calculation isn't using that kind of information.\n",
    "\n",
    "The way we've written our equations, it's as if we know that some words are present, but *don't know* whether the other words are *absent*. So let's rewrite slightly. Assume that $w_i = 1$ when the word $W_i$ is present, and $w_i = 0$ otherwise. Then:\n",
    "\n",
    "\\\\[\n",
    "\\frac{\n",
    "    \\prob{W_1 = w_1, \\ldots, W_{\\card{W}} = w_{\\card{W}} \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = w_1, \\ldots, W_{\\card{W}} = w_{\\card{W}} \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "    \\prob{W_1 = 1 \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{W_1 = 1 \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\cdots\n",
    "\\frac{\n",
    "    \\prob{W_{\\card{W}} = w_{\\card{W}} \\condbar \\text{SPAM} = 1}\n",
    "}{\n",
    "    \\prob{W_{\\card{W}} = w_{\\card{W}} \\condbar \\text{SPAM} = 0}\n",
    "}\n",
    "\\\\]\n",
    "\n",
    "Let's see if the use of these features can help our model predict spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate 0.001 | Recall 0.09\n",
      "False Positive Rate 0.010 | Recall 0.71\n",
      "False Positive Rate 0.020 | Recall 0.93\n",
      "False Positive Rate 0.040 | Recall 0.98\n",
      "False Positive Rate 0.080 | Recall 1.00\n",
      "False Positive Rate 0.160 | Recall 1.00\n"
     ]
    }
   ],
   "source": [
    "import lib.dataset\n",
    "from lib.naive_bayes_evaluator import NaiveBayesEvaluator\n",
    "from lib.naive_bayes_model import NaiveBayesModel\n",
    "\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "\n",
    "model = NaiveBayesModel(\n",
    "    RAW_DATASET.feature_probabilities().filter(reach_limit = 100),\n",
    "    use_negative_features = True\n",
    ")\n",
    "FALSE_POSITIVE_RATES = [0.001, 0.01, 0.02, 0.04, 0.08, 0.16]\n",
    "results = NaiveBayesEvaluator.recall_for_false_positive_rates(\n",
    "    model,\n",
    "    RAW_DATASET,\n",
    "    FALSE_POSITIVE_RATES\n",
    ")\n",
    "for (false_positive_rate, result) in results:\n",
    "    print(f\"False Positive Rate {false_positive_rate:0.3f} | Recall {result.recall:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have made a fairly substantial improvement! Let's look at what the best word omission features were!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172\n",
      "===BEST SPAM OMISSION FEATURES===\n",
      "0 | Subject: | reach: {'ham_count': 0, 'spam_count': 0} | feature_probability_ratio: inf:1\n",
      "9 | . | reach: {'ham_count': 131, 'spam_count': 253} | feature_probability_ratio: 4.73:1\n",
      "7 | , | reach: {'ham_count': 447, 'spam_count': 423} | feature_probability_ratio: 2.32:1\n",
      "42 | for | reach: {'ham_count': 893, 'spam_count': 678} | feature_probability_ratio: 1.86:1\n",
      "100 | enron | reach: {'ham_count': 2210, 'spam_count': 1500} | feature_probability_ratio: 1.66:1\n",
      "610 | 2000 | reach: {'ham_count': 2162, 'spam_count': 1460} | feature_probability_ratio: 1.65:1\n",
      "55 | / | reach: {'ham_count': 1270, 'spam_count': 836} | feature_probability_ratio: 1.61:1\n",
      "24 | on | reach: {'ham_count': 1554, 'spam_count': 978} | feature_probability_ratio: 1.54:1\n",
      "19 | - | reach: {'ham_count': 893, 'spam_count': 558} | feature_probability_ratio: 1.53:1\n",
      "70 | cc | reach: {'ham_count': 2393, 'spam_count': 1490} | feature_probability_ratio: 1.52:1\n",
      "73 | subject | reach: {'ham_count': 2275, 'spam_count': 1393} | feature_probability_ratio: 1.50:1\n",
      "163 | thanks | reach: {'ham_count': 2323, 'spam_count': 1417} | feature_probability_ratio: 1.49:1\n",
      "13 | the | reach: {'ham_count': 992, 'spam_count': 596} | feature_probability_ratio: 1.47:1\n",
      "313 | hpl | reach: {'ham_count': 2574, 'spam_count': 1500} | feature_probability_ratio: 1.43:1\n",
      "128 | gas | reach: {'ham_count': 2558, 'spam_count': 1478} | feature_probability_ratio: 1.41:1\n",
      "81 | i | reach: {'ham_count': 2029, 'spam_count': 1153} | feature_probability_ratio: 1.39:1\n",
      "63 | daren | reach: {'ham_count': 2642, 'spam_count': 1500} | feature_probability_ratio: 1.39:1\n",
      "74 | please | reach: {'ham_count': 2091, 'spam_count': 1186} | feature_probability_ratio: 1.39:1\n",
      "62 | am | reach: {'ham_count': 2521, 'spam_count': 1429} | feature_probability_ratio: 1.39:1\n",
      "240 | pm | reach: {'ham_count': 2660, 'spam_count': 1486} | feature_probability_ratio: 1.37:1\n",
      "===BEST HAM OMISSION FEATURES===\n",
      "538 | ! | reach: {'ham_count': 3182, 'spam_count': 735} | feature_probability_ratio: 0.57:1\n",
      "571 | your | reach: {'ham_count': 2902, 'spam_count': 748} | feature_probability_ratio: 0.63:1\n",
      "2518 | http | reach: {'ham_count': 3537, 'spam_count': 1025} | feature_probability_ratio: 0.71:1\n",
      "717 | here | reach: {'ham_count': 3369, 'spam_count': 1056} | feature_probability_ratio: 0.77:1\n",
      "286 | no | reach: {'ham_count': 3248, 'spam_count': 1020} | feature_probability_ratio: 0.77:1\n",
      "965 | more | reach: {'ham_count': 3424, 'spam_count': 1079} | feature_probability_ratio: 0.77:1\n",
      "46 | % | reach: {'ham_count': 3493, 'spam_count': 1125} | feature_probability_ratio: 0.79:1\n",
      "547 | our | reach: {'ham_count': 3106, 'spam_count': 1048} | feature_probability_ratio: 0.83:1\n",
      "557 | all | reach: {'ham_count': 3041, 'spam_count': 1029} | feature_probability_ratio: 0.83:1\n",
      "963 | com | reach: {'ham_count': 3070, 'spam_count': 1055} | feature_probability_ratio: 0.84:1\n",
      "2519 | www | reach: {'ham_count': 3563, 'spam_count': 1237} | feature_probability_ratio: 0.85:1\n",
      "149 | you | reach: {'ham_count': 1818, 'spam_count': 632} | feature_probability_ratio: 0.85:1\n",
      "875 | $ | reach: {'ham_count': 3263, 'spam_count': 1147} | feature_probability_ratio: 0.86:1\n",
      "600 | email | reach: {'ham_count': 3452, 'spam_count': 1216} | feature_probability_ratio: 0.86:1\n",
      "77 | get | reach: {'ham_count': 3180, 'spam_count': 1125} | feature_probability_ratio: 0.87:1\n",
      "530 | best | reach: {'ham_count': 3572, 'spam_count': 1272} | feature_probability_ratio: 0.87:1\n",
      "162 | ? | reach: {'ham_count': 2779, 'spam_count': 990} | feature_probability_ratio: 0.87:1\n",
      "655 | only | reach: {'ham_count': 3338, 'spam_count': 1194} | feature_probability_ratio: 0.88:1\n",
      "4355 | click | reach: {'ham_count': 3573, 'spam_count': 1288} | feature_probability_ratio: 0.88:1\n",
      "136 | s | reach: {'ham_count': 2727, 'spam_count': 986} | feature_probability_ratio: 0.89:1\n"
     ]
    }
   ],
   "source": [
    "import lib.dataset\n",
    "from lib.feature_probabilities_explorer import FeatureProbabilitiesExplorer\n",
    "\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "print(len(RAW_DATASET.ham_emails) + len(RAW_DATASET.spam_emails))\n",
    "\n",
    "filtered_feature_probabilities = RAW_DATASET.feature_probabilities().filter(reach_limit = 100)\n",
    "\n",
    "print(\"===BEST SPAM OMISSION FEATURES===\")\n",
    "best_spam_features = FeatureProbabilitiesExplorer.best_spam_features(\n",
    "    filtered_feature_probabilities,\n",
    "    present_features = False\n",
    ")\n",
    "FeatureProbabilitiesExplorer.print_features_list(\n",
    "    best_spam_features, RAW_DATASET.word_encoding_dictionary\n",
    ")\n",
    "\n",
    "print(\"===BEST HAM OMISSION FEATURES===\")\n",
    "best_ham_features = FeatureProbabilitiesExplorer.best_ham_features(\n",
    "    filtered_feature_probabilities,\n",
    "    present_features = False\n",
    ")\n",
    "FeatureProbabilitiesExplorer.print_features_list(\n",
    "    best_ham_features, RAW_DATASET.word_encoding_dictionary\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam emails seem to be more likely to lack punctuation. And it looks like spam emails tend not to mention Enron: this is clearly specific to our training dataset!\n",
    "\n",
    "It looks clear that emails that don't contain a link are more likely to be ham. That's presumably because spammers want you to go to their website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training And Test Sets\n",
    "\n",
    "Wow, that's way better! However, the way we are measuring performance is a little too leniant. We are supposed to train a model that predicts whether future emails are ham/spam, but we're measuring performance on the *training set*. This is often problematic because sometimes models will just \"memorize\" the training dataset in a way that doesn't lead to any future good performance.\n",
    "\n",
    "For instance, say our model was able to record an exact map from a bag of words to a label of ham or spam. Then, since every email in the training set probably has a unique bag of words, the model would be able to just record an exact mapping of email to label. But when we go to evaluate new emails, new emails won't match any of those bags of words. Thus there would be no ability to predict labels for future emails.\n",
    "\n",
    "To make sure our model *generalizes* well, it is common to split our data into two parts: the *training set* and the *test set*. The training set is fed to the machine learning algorithm, and then we use the test set to measure performance of the learned model. Since the ML algorithm never has seen the test set before, this should be a fair test of its ability to detect spam.\n",
    "\n",
    "Let's train on 80% of the data, and leave 20% for testing. There is a conflict of interest when picking these proportions. The more data you train on, the better your model will be. But the more data in your testing set, the more accurate your estimate on how the model will generalize.\n",
    "\n",
    "When you have more data, you might use more less for testing, figuring this amount will still be sufficient. On the other hand, 80/20 is a pretty common ratio to use.\n",
    "\n",
    "There are fancy techniques like [cross-validation][0], but I won't talk about those here.\n",
    "\n",
    "[0]: https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_number of emails: 5172\n",
      "number of training emails: 4181\n",
      "number of test emails: 991\n",
      "False Positive Rate 0.00 | Recall 0.08\n",
      "False Positive Rate 0.01 | Recall 0.74\n",
      "False Positive Rate 0.02 | Recall 0.92\n",
      "False Positive Rate 0.04 | Recall 0.97\n",
      "False Positive Rate 0.08 | Recall 0.98\n",
      "False Positive Rate 0.16 | Recall 0.98\n"
     ]
    }
   ],
   "source": [
    "import lib.dataset\n",
    "from lib.naive_bayes_evaluator import NaiveBayesEvaluator\n",
    "from lib.naive_bayes_model import NaiveBayesModel\n",
    "\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "(training_set, test_set) = RAW_DATASET.split(ratio = 0.80)\n",
    "\n",
    "print(f\"total_number of emails: {len(RAW_DATASET.ham_emails) + len(RAW_DATASET.spam_emails)}\")\n",
    "print(f\"number of training emails: {len(training_set.ham_emails) + len(training_set.spam_emails)}\")\n",
    "print(f\"number of test emails: {len(test_set.ham_emails) + len(test_set.spam_emails)}\")\n",
    "\n",
    "model = NaiveBayesModel(\n",
    "    training_set.feature_probabilities().filter(reach_limit = 100),\n",
    "    use_negative_features = True\n",
    ")\n",
    "FALSE_POSITIVE_RATES = [0.001, 0.01, 0.02, 0.04, 0.08, 0.16]\n",
    "results = NaiveBayesEvaluator.recall_for_false_positive_rates(\n",
    "    model,\n",
    "    test_set,\n",
    "    FALSE_POSITIVE_RATES\n",
    ")\n",
    "for (false_positive_rate, result) in results:\n",
    "    print(f\"False Positive Rate {false_positive_rate:0.2f} | Recall {result.recall:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Generalizes Well\n",
    "\n",
    "As you can see, the naive bayes model has generalized very well to the test set. The recall numbers at each false positive rate are all mostly in line with the rates calculated over the entire dataset. That shows that we aren't just memorizing the training dataset.\n",
    "\n",
    "Overfitting is not normally a major problem for Naive Bayes models. This is because Naive Bayes is very simple. The way overfitting normally happens with other models is that parameters are carefully set so that if just the right set of features (those of the memorized example) come in, then the right answer goes out (the label of the memorized example).\n",
    "\n",
    "To do this, a model typically needs to coordinate many parameters so they are \"just right\" for the training example.\n",
    "\n",
    "However, the Naive Bayes model does no such coordination. It sets each parameter seperately. It has no way to tweak parameters so they have combined effects that are any different than their individual effects.\n",
    "\n",
    "Just because Naive Bayes is good at avoiding overfitting doesn't mean that it always gives high performance. It relies on this assumption that we *know* isn't true: that the presence of a word $w_i$ is independent of a word $w_j$ given the class of the example. In many contexts, understanding these interaction effects will be very important.\n",
    "\n",
    "For instance, if we want to classify emails based on *sentiment*, then words like \"good\", \"great\", \"awesome\" are positive signals, while \"terrible\", \"bad\", \"awful\" are negative signals. But what about \"This movie is not good?\" How is the Naive Bayes model supposed to know this is negative? It can't just make \"not\" a very negative word, because I could also say: \"This movie is not bad.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Versus Variance\n",
    "\n",
    "We will later explore a classic tradeoff of machine learning models: *bias* versus *variance*. We say a model class is very *biased* if it imposes a very simple, inflexible structure of model. For instance, a model class of linear models is much more biased than the model class of all polynomial functions.\n",
    "\n",
    "On the other hand, we say a model class has *high* variance if we learn very different models depending on what data we feed it. For instance: if I feed the first, third, et cetera emails into the Naive Bayes model, I will get mostly the same model as feeding in the second, fourth, et cetera emails. We say Naive Bayes has *low* variance. The idea is that the Naive Bayes model doesn't vary much in what sample you feed it.\n",
    "\n",
    "Models that exhibit high variance do so because they have more *capacity*. That means: they can capture more complex relationships. This additional flexibility often allows the model to capture relationships that \"aren't really there.\"\n",
    "\n",
    "We saw an example of this before with fitting the line to the Gaussian noise. A relationship like this is called *spurious*. Luckily, a linear model is very biased (has low capacity), so the spurious relationship the linear model thinks it found in the noise is very weak.\n",
    "\n",
    "The ability to capture spurious relationships often means that a model *thinks* it is doing a good job on the training dataset, but when we test it on the test dataset, the model fails badly. This happens because the things it thought it learned turn out to be false.\n",
    "\n",
    "All else equal, you want to minimize bias in your model class. Bias is basically simplifying assumptions about what kind of model explains the data. But those assumptions are typically not quite correct, and so your model may not perform well if your assumptions are too simplistic.\n",
    "\n",
    "On the other hand, when you have small amounts of data, you will prefer models with less capacity, because training a high capacity model with small amounts of data leads to high variance in outcome. Basically: there isn't enough data to convince the model not to use its additional capacity to model noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Probability Estimates Are Inaccurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes model is an example of a *probabilistic graphical model*. In particular, it is the simplest example of a *Bayesian Network*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_number of emails: 5172\n",
      "number of training emails: 4181\n",
      "number of test emails: 991\n",
      "1.0\n",
      "1.5\n",
      "2.33333333333\n",
      "4.0\n",
      "9.0\n",
      "[(0.5, {'limit': 0.5, 'expected_rate': 0.91064650168097672, 'true_rate': 0.9525862068965517}), (0.59999999999999998, {'limit': 0.59999999999999998, 'expected_rate': 0.9240938475636995, 'true_rate': 0.9555555555555556}), (0.69999999999999996, {'limit': 0.69999999999999996, 'expected_rate': 0.93292936845285546, 'true_rate': 0.9541284403669725}), (0.79999999999999993, {'limit': 0.79999999999999993, 'expected_rate': 0.93954518363462225, 'true_rate': 0.9523809523809523}), (0.89999999999999991, {'limit': 0.89999999999999991, 'expected_rate': 0.95228004210517281, 'true_rate': 0.96})]\n"
     ]
    }
   ],
   "source": [
    "import lib.dataset\n",
    "from lib.naive_bayes_evaluator import NaiveBayesEvaluator\n",
    "from lib.naive_bayes_model import NaiveBayesModel\n",
    "\n",
    "RAW_DATASET = lib.dataset.RawDataset.get()\n",
    "(training_set, test_set) = RAW_DATASET.split(ratio = 0.80)\n",
    "\n",
    "print(f\"total_number of emails: {len(RAW_DATASET.ham_emails) + len(RAW_DATASET.spam_emails)}\")\n",
    "print(f\"number of training emails: {len(training_set.ham_emails) + len(training_set.spam_emails)}\")\n",
    "print(f\"number of test emails: {len(test_set.ham_emails) + len(test_set.spam_emails)}\")\n",
    "\n",
    "model = NaiveBayesModel(\n",
    "    training_set.feature_probabilities().filter(reach_limit = 100),\n",
    "    use_negative_features = False\n",
    ")\n",
    "\n",
    "print(NaiveBayesEvaluator.spam_probability_at_different_scores(\n",
    "    model,\n",
    "    test_set\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAACGCAYAAACfQY77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1111a3048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import examples.pgm_diagram_example\n",
    "\n",
    "examples.pgm_diagram_example.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Explain PGM.\n",
    "\n",
    "**TODO**: Show that $\\prob{C \\condbar w_i}$ is inaccurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
