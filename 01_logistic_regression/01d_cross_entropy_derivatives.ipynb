{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to do Gradient Descent, we need to calculate the partial derivatives of the cross entropy error function.\n",
    "\n",
    "For the rest of this notebook, I will be using the notation $y^i$ as the label of the $i$th training example. This is the same as spam or not spam. It is encoded using a zero or a one.\n",
    "\n",
    "$x^i$ as the vector of features for example $i$. $x_j^i = 0$ if the $j$th feature is absent, $x_j^i = 1$ if the feature is present.\n",
    "\n",
    "Basically, I want to not use symbols that are specific to the spam classification problem, because Logistic Regression is a technique that can be applied to any similar problem.\n",
    "\n",
    "From our prior book, we have:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "E(\\theta)\n",
    "&=\n",
    "-\n",
    "\\sum_i^N\n",
    "y_i\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(\n",
    "        \\theta_0\n",
    "        +\n",
    "        \\sum_{j = 1}^M \\theta_j w_j^i\n",
    "    \\right)\n",
    "\\right)\n",
    "+\n",
    "(1 - y_i)\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(\n",
    "        -\\theta_0\n",
    "        -\n",
    "        \\sum_{j = 1}^M \\theta_j x_j^i\n",
    "    \\right)\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a notational convenience, I will often write\n",
    "\n",
    "\\\\[\n",
    "z_\\theta(x^i) = \\theta_0 + \\sum_j \\theta_j x_j^i\n",
    "\\\\]\n",
    "\n",
    "Thus:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "E(\\theta)\n",
    "&=\n",
    "-\n",
    "\\sum_i^N\n",
    "y_i\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "+\n",
    "(1 - y_i)\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(-z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do gradient descent, we need to find the partial derivative of the error function with respect to a proposed change in parameter $\\theta_k$:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{\n",
    "    E\n",
    "}{\n",
    "    \\theta_k\n",
    "}(\\theta)\n",
    "&=\n",
    "-\\fpartial{}{\\theta_k}\n",
    "    \\sum_{i = 1}^N\n",
    "        y^i\n",
    "        \\left(\n",
    "            \\log\n",
    "            \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "        \\right)\n",
    "        +\n",
    "        (1 - y^i)\n",
    "        \\left(\n",
    "            \\log\n",
    "            \\sigma\\left(-z_\\theta(x^i)\\right)\n",
    "        \\right)\n",
    "\\\\\n",
    "&=\n",
    "-\n",
    "\\sum_{i = 1}^N\n",
    "    y^i\n",
    "    \\fpartial{}{\\theta_k}\n",
    "    \\left(\n",
    "        \\log\n",
    "        \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "    \\right)\n",
    "    +\n",
    "    (1 - y^i)\n",
    "    \\fpartial{}{\\theta_k}\n",
    "    \\left(\n",
    "        \\log\n",
    "        \\sigma\\left(-z_\\theta(x^i)\\right)\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring on the calculus! Let's solve the first derivative first!\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{}{\\theta_k}\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "}\n",
    "\\fpartial{}{\\theta_k}\n",
    "\\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the next part, we'll need to use the chain rule again. Let's start by computing the partial derivative:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{z}\n",
    "\\sigma(z)\n",
    "&=\n",
    "\\fpartial{}{z}\n",
    "\\frac{\n",
    "    e^z\n",
    "}{\n",
    "    1 + e^z\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\fpartial{}{z}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1 + e^{-z}\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\fpartial{}{z}\n",
    "\\left(\n",
    "    1 + e^{-z}\n",
    "\\right)^-1\n",
    "\\\\\n",
    "&=\n",
    "(-1)\n",
    "\\left(\n",
    "    1 + e^{-z}\n",
    "\\right)^2\n",
    "e^{-z}\n",
    "(-1)\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1 + e^{-z}\n",
    "}\n",
    "\\frac{\n",
    "    e^{-z}\n",
    "}{\n",
    "    1 + e^{-z}\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    e^z\n",
    "}{\n",
    "    1 + e^z\n",
    "}\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    1 + e^z\n",
    "}\n",
    "\\\\\n",
    "&=\n",
    "\\sigma(z)\n",
    "(1 - \\sigma(z))\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed!\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{\\theta_k}\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "}\n",
    "\\fpartial{}{\\theta_k}\n",
    "\\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    1\n",
    "}{\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "}\n",
    "\\left(\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "    \\left(\n",
    "        1 - \\sigma\\left( z_\\theta(x^i) \\right)\n",
    "    \\right)\n",
    "\\right)\n",
    "\\fpartial{}{\\theta_k}\n",
    "z_\\theta(x^i)\n",
    "\\\\\n",
    "&=\n",
    "\\left(\n",
    "    1 - \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "\\fpartial{}{\\theta_k}\n",
    "z_\\theta(x^i)\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last question is: what is $\\fpartial{}{\\theta_k} z_\\theta(x^i)$? This is a little weird because I've written $z$ as if it is *parameterized* by $\\theta$, and as a *function of* $x^i$. But we can still consider the derivative:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{}{\\theta_k}\n",
    "    z_\\theta(x^i)\n",
    "=\n",
    "\\fpartial{}{\\theta_k}\n",
    "\\left(\n",
    "    \\theta_0\n",
    "    +\n",
    "    \\sum_{j = 1}^M\n",
    "        \\theta_j x_j^i\n",
    "\\right)\n",
    "=\n",
    "x_k^i\n",
    "\\\\]\n",
    "\n",
    "For binary valued features (what we're working with here), when $x_k^i = 0$, that means that feature isn't present for example $x^i$, so therefore changing $\\theta_k$ won't change $z_\\theta(x^i)$. Thus it makes sense that this partial derivative would be zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, finally we have:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{\\theta_k}\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "&=\n",
    "\\left(\n",
    "    1 - \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "x_k^i\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save you much boredom, it is also true that:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{\\theta_k}\n",
    "\\left(\n",
    "    \\log\n",
    "    \\sigma\\left( -z_\\theta(x^i) \\right)\n",
    "\\right)\n",
    "&=\n",
    "-\n",
    "\\sigma\\left(z_\\theta(x^i)\\right)\n",
    "x_k^i\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this, we have:\n",
    "    \n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{\\theta_k}\n",
    "E(\\theta)\n",
    "&=\n",
    "-\n",
    "\\sum_i^N\n",
    "    y_i\n",
    "    \\left(\n",
    "        1 - \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "    \\right)\n",
    "    x_k^i\n",
    "+\n",
    "    (1 - y_i)\n",
    "    \\left(\n",
    "        -\\sigma\\left( z_\\theta(x^i) \\right)\n",
    "    \\right)\n",
    "    x_k^i\n",
    "\\\\\n",
    "&=\n",
    "\\sum_i^N\n",
    "    -\n",
    "    y_i\n",
    "    x_k^i\n",
    "    \\left(\n",
    "        1 - \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "    \\right)\n",
    "+\n",
    "    (1 - y_i)\n",
    "    x_k^i\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, holy Moses! By the definition of $\\sigma$ and $z_\\theta$, we know that:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\pprob{\\theta}{Y = 0 \\condbar X = x^i}\n",
    "&=\n",
    "\\left(\n",
    "    1 - \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\right)\n",
    "\\\\\n",
    "\\pprob{\\theta}{Y = 1 \\condbar X = x^i}\n",
    "&=\n",
    "\\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can substitute back in:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{}{\\theta_k}\n",
    "E(\\theta)\n",
    "&=\n",
    "\\sum_i^N\n",
    "    -\n",
    "    y_i\n",
    "    x_k^i\n",
    "    \\left(\n",
    "        1 - \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "    \\right)\n",
    "+\n",
    "    (1 - y_i)\n",
    "    x_k^i\n",
    "    \\sigma\\left(z_\\theta(x^i)\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\sum_i^N\n",
    "    -\n",
    "    y_i\n",
    "    x_k^i\n",
    "    \\pprob{\\theta}{Y = 0 \\condbar X = x^i}\n",
    "+\n",
    "    (1 - y_i)\n",
    "    x_k^i\n",
    "    \\pprob{\\theta}{Y = 1 \\condbar X = x^i}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how I read the equation. Consider an example $(x^i, y^i)$. If $x_k^i = 0$, then changing the coefficient $\\theta_k$ has no effect. So forget it.\n",
    "\n",
    "Next, assume that $y^i = 1$. In that case, the more your model currently (wrongly) thinks this is a negative example, the greater the impact on increasing $\\theta_k$.\n",
    "\n",
    "On the other hand, if $y^i = 0$, the opposite applies. The more your model currently (wrongly) thinks this is a positive example, the greater the impact of increasing $\\theta_k$.\n",
    "\n",
    "Take note of the signs. When we are dealing with a positive example, increasing $\\theta_k$ will increase our belief this is a positive example. That should *lower* the error, which is why there is a leading negative sign outside the sum.\n",
    "\n",
    "On the other hand, when we have a *negative* example, then increasing $\\theta_k$ is increasing our belief in the wrong direction! That's why this effect has the *opposite* sign. The negative on the inside and outside the sum cancels. That indicates that increasing $\\theta_k$ will *increase* our error on this example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
