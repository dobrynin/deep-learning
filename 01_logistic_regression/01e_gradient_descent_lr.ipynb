{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    os.getcwd(),\n",
    "    \"data/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Simple email class. All it does is allow you to read an email's text.\n",
    "class Email:\n",
    "    def __init__(self, path, label):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "\n",
    "    def text_content(self):\n",
    "        return type(self).read_text_content(self.path)\n",
    "    \n",
    "    def word_counts(self):\n",
    "        counts = {}\n",
    "        for word in self.text_content().split():\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "            \n",
    "        return counts\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, path, label):\n",
    "        return Email(\n",
    "            path = path,\n",
    "            label = label\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def read_text_content(cls, path):\n",
    "        full_path = os.path.join(DATA_DIR, path)\n",
    "        # Grr! Emails are encoded in Latin-1, not UTF-8. Python\n",
    "        # (rightly) freaks out.\n",
    "        with open(full_path, \"r\", encoding = \"iso-8859-1\") as f:\n",
    "            try:\n",
    "                return f.read()\n",
    "            except:\n",
    "                print(f\"Error with: {path}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to only create codes for those words that occur frequently in emails. There are two main reasons.\n",
    "\n",
    "First, if I encode *all* the words, that means more features per email. To record the presence or absence of a feature takes space. It also means extra time spent learning, because Gradient Descent is going to need to decide how to update the parameter for that word.\n",
    "\n",
    "The vast majority of words in any given *corpus* (collection of documents) occur very few times. Therefore they have very little predictive value in the future. So it isn't useful to include these. You can save a *ton* of memory and time by throwing them out.\n",
    "\n",
    "Second, the more words included, the more parameters there, which means the more complicated the model. The extra parameters give the model additional *capacity* for overfitting. In particular, if a word occurs exactly once in the dataset, the model can use that feature to perfectly predict the one email that has it. Effectively this \"removes\" the email from the dataset: the other features on this email don't matter.\n",
    "\n",
    "If this happens for a lot of emails, it's as if you're training with much less data.\n",
    "\n",
    "Therefore, to filter words, I'll count their *reach*: the number of emails with that feature. I'll drop the low reach features, and build a bidirectional map of codes to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "# This counts how many emails each word occurs in.\n",
    "def build_word_reaches(ham_emails, spam_emails):\n",
    "    word_reaches = {}\n",
    "    for emails in (ham_emails, spam_emails):\n",
    "        for email in emails:\n",
    "            for (word, _) in email.word_counts().items():\n",
    "                if word not in word_reaches:\n",
    "                    word_reaches[word] = 0\n",
    "                # No matter how frequent in the email, only counts once per email.\n",
    "                word_reaches[word] += 1\n",
    "                \n",
    "    return word_reaches\n",
    "\n",
    "# Throw away those emails that don't occur in at least 100 emails.\n",
    "# Throwing out low reach features means:\n",
    "# (1) Less chance for overfitting\n",
    "# (2) Smaller feature vectors, faster, less memory use.\n",
    "def filter_words(word_reaches, limit = 100):\n",
    "    filtered_words = SortedSet()\n",
    "    for (word, word_reach) in word_reaches.items():\n",
    "        if word_reach >= limit:\n",
    "            filtered_words.add(word)\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "# Bidirectional map. Limits to just filtered words, though.\n",
    "class FilteredWordEncodingDictionary:\n",
    "    def __init__(self, filtered_words):\n",
    "        self.word_to_code_dict = {}\n",
    "        self.code_to_word_dict = {}\n",
    "\n",
    "        for word in filtered_words:\n",
    "            self.insert_word(word)\n",
    "\n",
    "    # Only meant to be called when constructing the dictionary.\n",
    "    def insert_word(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            code = len(self.word_to_code_dict)\n",
    "            self.word_to_code_dict[word] = code\n",
    "            self.code_to_word_dict[code] = word\n",
    "\n",
    "    def word_to_code(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            return None\n",
    "\n",
    "        return self.word_to_code_dict[word]\n",
    "\n",
    "    def code_to_word(self, code):\n",
    "        if code not in self.code_to_word_dict:\n",
    "            raise f\"Code {code} not recorded!\"\n",
    "\n",
    "        return self.code_to_word_dict[code]\n",
    "\n",
    "    # This returns a vector of ones and zeros.\n",
    "    def encode_text(self, text):\n",
    "        codes = np.zeros(len(self.code_to_word_dict))\n",
    "\n",
    "        for word in text.split():\n",
    "            code = self.word_to_code(word)\n",
    "            if code is not None:\n",
    "                codes[code] = 1.0\n",
    "\n",
    "        return codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_to_word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the encode method works. It returns a vector of ones and zeros. Each position $i$ in the vector means either $w_i = 1$ or $w_i = 0$. This is a very handy representation.\n",
    "\n",
    "Numpy can manipulate vectors much faster than Python can work with objects like Sets or Dicts. The primary reason is that all the numbers in the vector are packed one next to the other with no indirection. This is more space efficient, but it especially is helpful for *locality of reference*. CPUs work better when all the data they work on is near to each other, because then they can cache the data from RAM into the CPU cache better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a simple subclass of Email that just encodes the words in an email.\n",
    "class EncodedEmail(Email):\n",
    "    def __init__(self, path, label, word_encoding_dictionary):\n",
    "        super().__init__(path, label)\n",
    "\n",
    "        self.codes = (\n",
    "            word_encoding_dictionary.encode_text(\n",
    "                self.text_content()\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the `Dataset` class. It filters words, builds the dictionary, and encodes emails into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "class Dataset:\n",
    "    DATA_FILE_PATH = os.path.join(DATA_DIR, 'lr_data.p')\n",
    "    WORD_REACH_LIMIT = 100\n",
    "\n",
    "    def __init__(\n",
    "            self, word_encoding_dictionary, encoded_ham_emails, encoded_spam_emails\n",
    "    ):\n",
    "        self.word_encoding_dictionary = word_encoding_dictionary\n",
    "        self.encoded_ham_emails = encoded_ham_emails\n",
    "        self.encoded_spam_emails = encoded_spam_emails\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_ham_emails) + len(self.encoded_spam_emails)\n",
    "    \n",
    "    @classmethod\n",
    "    def encode(cls, ham_emails, spam_emails):\n",
    "        # Count words, select which we will keep.\n",
    "        word_reaches = build_word_reaches(ham_emails, spam_emails)\n",
    "        filtered_words = filter_words(word_reaches, limit = cls.WORD_REACH_LIMIT)\n",
    "        \n",
    "        # Assign codes to all words.\n",
    "        word_encoding_dictionary = FilteredWordEncodingDictionary(filtered_words)\n",
    "        \n",
    "        # Encode each email as a vector of ones and zeros.\n",
    "        encoded_ham_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in ham_emails\n",
    "        ]\n",
    "        encoded_spam_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in spam_emails\n",
    "        ]\n",
    "        \n",
    "        # Construct the object!\n",
    "        return cls(\n",
    "            word_encoding_dictionary,\n",
    "            encoded_ham_emails,\n",
    "            encoded_spam_emails\n",
    "        )\n",
    "\n",
    "    INSTANCE = None\n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if not cls.INSTANCE:\n",
    "            with open(cls.DATA_FILE_PATH, 'rb') as f:\n",
    "                cls.INSTANCE = pickle.load(f)\n",
    "        return cls.INSTANCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a bunch of code that downloads the raw dataset, builds the encoded dataset, and saves it. This code may not be terribly interesting to you :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarfile already downloaded!\n",
      "Tarfile already extracted!\n",
      "Dataset already processed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "ENRON_SPAM_URL = (\n",
    "    \"http://csmining.org/index.php/\"\n",
    "    \"enron-spam-datasets.html\"\n",
    "    \"?file=tl_files/Project_Datasets/Enron-Spam%20datasets/Preprocessed\"\n",
    "    \"/enron1.tar.tar\"\n",
    ")\n",
    "\n",
    "TAR_FILE_NAME = \"enron1.tar.tar\"\n",
    "ENRON_DATA_DIR_NAME = \"enron1\"\n",
    "\n",
    "def download_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    if os.path.isfile(tarfile_path):\n",
    "        print(\"Tarfile already downloaded!\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading enron1.tar.tar\")\n",
    "    urlretrieve(ENRON_SPAM_URL, tarfile_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    enron_data_dir = os.path.join(DATA_DIR, ENRON_DATA_DIR_NAME)\n",
    "    if os.path.isdir(enron_data_dir):\n",
    "        print(\"Tarfile already extracted!\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting enron1.tar.tar\")\n",
    "    os.system(f\"tar -xf {tarfile_path} -C {DATA_DIR}\")\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def read_emails_dir(path, label):\n",
    "    emails = []\n",
    "    for email_fname in os.listdir(os.path.join(DATA_DIR, path)):\n",
    "        email_path = os.path.join(path, email_fname)\n",
    "        email = Email.read(\n",
    "            path = email_path,\n",
    "            label = label\n",
    "        )\n",
    "        emails.append(email)\n",
    "\n",
    "    return emails\n",
    "\n",
    "def build_dataset():\n",
    "    ham_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"ham\"),\n",
    "        label = 0\n",
    "    )\n",
    "    spam_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"spam\"),\n",
    "        label = 1\n",
    "    )\n",
    "\n",
    "    return Dataset.encode(\n",
    "        ham_emails = ham_emails,\n",
    "        spam_emails = spam_emails\n",
    "    )\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    with open(Dataset.DATA_FILE_PATH, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def build_and_save_dataset():\n",
    "    if os.path.isfile(Dataset.DATA_FILE_PATH):\n",
    "        print(\"Dataset already processed!\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading and processing emails!\")\n",
    "    dataset = build_dataset()\n",
    "    save_dataset(dataset)\n",
    "    print(\"Dataset created!\")\n",
    "\n",
    "download_tarfile()\n",
    "extract_tarfile()\n",
    "build_and_save_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our model! `LogisticRegressionModel` has a single instance variable `theta`. This is a vector of theta values.\n",
    "\n",
    "Notice how I use `self.theta.dot(emails.codes)`. Numpy loves taking dot product of numpy arrays; it is very fast at this!\n",
    "\n",
    "Otherwise, this code simply instantiates the math discussed in the previous notebook. You should verify it does what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, word_encoding_dictionary):\n",
    "        self.theta = np.zeros(\n",
    "            len(word_encoding_dictionary)\n",
    "        )\n",
    "        \n",
    "    def prob(self, codes):\n",
    "        return logistic(self.theta.dot(codes))\n",
    "    \n",
    "    def error(self, email):\n",
    "        if email.label == 0:\n",
    "            return -np.log(1 - self.prob(email.codes))\n",
    "        else:\n",
    "            return -np.log(self.prob(email.codes))\n",
    "\n",
    "    def partial_derivatives(self, email):\n",
    "        if email.label == 0:\n",
    "            # This is a vectorized version.\n",
    "            return (\n",
    "                email.codes * self.prob(email.codes)\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                -email.codes * (1 - self.prob(email.codes))\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically a copy from the prior notebook. It just splits the dataset into training and test sets so that we can evaluate the model performance accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "class DatasetSplitter:\n",
    "    @classmethod\n",
    "    def split(cls, dataset, ratio):\n",
    "        datasetA = cls._split(dataset, ratio, 0)\n",
    "        datasetB = cls._split(dataset, ratio, 1)\n",
    "        return (datasetA, datasetB)\n",
    "\n",
    "    @classmethod\n",
    "    def _split(cls, dataset, ratio, mode):\n",
    "        split_encoded_ham_emails, split_encoded_spam_emails = [], []\n",
    "        emails_pairs = [\n",
    "            (dataset.encoded_ham_emails, split_encoded_ham_emails),\n",
    "            (dataset.encoded_spam_emails, split_encoded_spam_emails)\n",
    "        ]\n",
    "\n",
    "        for (emails, split_emails) in emails_pairs:\n",
    "            for email in emails:\n",
    "                # This is a fancy way to pseudorandomly but\n",
    "                # deterministically select emails. That way we always\n",
    "                # pick the same set of emails for reproducability\n",
    "                # across program runs.\n",
    "                h = zlib.crc32(email.path.encode())\n",
    "                p = h / (2**32 - 1)\n",
    "                if (mode == 0 and p < ratio) or (mode == 1 and p >= ratio):\n",
    "                    split_emails.append(email)\n",
    "\n",
    "        return Dataset(\n",
    "            dataset.word_encoding_dictionary,\n",
    "            encoded_ham_emails = split_encoded_ham_emails,\n",
    "            encoded_spam_emails = split_encoded_spam_emails\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens!\n",
    "\n",
    "The trainer iterates through the positive and negative examples. It calculates the partial derivative of the error function using each email individually. It sums up these terms.\n",
    "\n",
    "To train, we do the typical gradient descent thing. We set $\\theta_{k + 1} = \\theta_k - \\lambda \\nabla E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, training_dataset, test_dataset, learning_rate):\n",
    "        self.training_dataset, self.test_dataset = training_dataset, test_dataset\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = LogisticRegressionModel(training_dataset.word_encoding_dictionary)\n",
    "        \n",
    "    def error(self, dataset):\n",
    "        error = 0.0\n",
    "        for emails in (dataset.encoded_ham_emails, dataset.encoded_spam_emails):\n",
    "            for email in emails:\n",
    "                error += self.model.error(email)\n",
    "\n",
    "        # Otherwise it isn't fair because longer sets have more error.\n",
    "        return error / len(dataset)\n",
    "    \n",
    "    def partial_derivatives(self):\n",
    "        partials = np.zeros(len(self.training_dataset.word_encoding_dictionary))\n",
    "        for emails in (self.training_dataset.encoded_ham_emails, self.training_dataset.encoded_spam_emails):\n",
    "            for email in emails:\n",
    "                partials += self.model.partial_derivatives(email)\n",
    "                \n",
    "        return partials\n",
    "\n",
    "    def train_step(self):\n",
    "        self.model.theta -= self.learning_rate * self.partial_derivatives()\n",
    "        print(f\"Train Error: {self.error(self.training_dataset):0.2f}\")\n",
    "        print(f\"Test Error: {self.error(self.test_dataset):0.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model! Why don't you try out other learning rates? You should see the error rate steadily go down. It is typical for the test error to be a little bit higher than the training error: that's because the model can't help but overtrain a little!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 3.02\n",
      "Test Error: 2.63\n",
      "Train Error: 0.66\n",
      "Test Error: 0.67\n",
      "Train Error: 2.37\n",
      "Test Error: 2.06\n",
      "Train Error: 0.71\n",
      "Test Error: 0.70\n",
      "Train Error: 1.40\n",
      "Test Error: 1.23\n",
      "Train Error: 0.95\n",
      "Test Error: 0.96\n",
      "Train Error: 1.15\n",
      "Test Error: 1.00\n",
      "Train Error: 0.64\n",
      "Test Error: 0.63\n",
      "Train Error: 0.30\n",
      "Test Error: 0.27\n",
      "Train Error: 0.21\n",
      "Test Error: 0.21\n",
      "Train Error: 0.15\n",
      "Test Error: 0.14\n",
      "Train Error: 0.13\n",
      "Test Error: 0.13\n",
      "Train Error: 0.12\n",
      "Test Error: 0.12\n",
      "Train Error: 0.12\n",
      "Test Error: 0.12\n",
      "Train Error: 0.11\n",
      "Test Error: 0.11\n",
      "Train Error: 0.11\n",
      "Test Error: 0.11\n",
      "Train Error: 0.10\n",
      "Test Error: 0.11\n",
      "Train Error: 0.10\n",
      "Test Error: 0.11\n",
      "Train Error: 0.10\n",
      "Test Error: 0.10\n",
      "Train Error: 0.10\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.10\n",
      "Train Error: 0.09\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.08\n",
      "Test Error: 0.09\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.07\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.08\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.06\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n",
      "Train Error: 0.05\n",
      "Test Error: 0.07\n"
     ]
    }
   ],
   "source": [
    "DATASET = Dataset.get()\n",
    "(training_set, test_set) = DatasetSplitter.split(DATASET, 0.80)\n",
    "\n",
    "trainer = Trainer(\n",
    "    training_set,\n",
    "    test_set,\n",
    "    learning_rate = 0.001\n",
    ")\n",
    "for _ in range(100):\n",
    "    trainer.train_step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I copy and paste some code from a prior notebook, so that we can evaluate performance at different levels of false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper class (see below)\n",
    "class RecallResult:\n",
    "    def __init__(self, score_cutoff, num_true_positives, recall, accuracy):\n",
    "        self.score_cutoff, self.num_true_positives, self.recall, self.accuracy = (\n",
    "            score_cutoff, num_true_positives, recall, accuracy\n",
    "        )\n",
    "\n",
    "# Determines what percentage of spam emails are detected if we can tolerate a given false positive rate.\n",
    "# Does this for multiple false positive rate limits.\n",
    "def recall_for_false_positive_rates(model, dataset, limits):\n",
    "    ham_scores = list(map(\n",
    "        lambda email: model.prob(email.codes),\n",
    "        dataset.encoded_ham_emails\n",
    "    ))\n",
    "    ham_scores.sort(key = lambda score: -score)\n",
    "    spam_scores = list(map(\n",
    "        lambda email: model.prob(email.codes),\n",
    "        dataset.encoded_spam_emails\n",
    "    ))\n",
    "\n",
    "    def calculate_result(limit):\n",
    "        score_cutoff = ham_scores[int(len(ham_scores) * limit)]\n",
    "        num_true_positives = sum(\n",
    "            [1 if s > score_cutoff else 0 for s in spam_scores]\n",
    "        )\n",
    "        recall = (\n",
    "            num_true_positives / len(dataset.encoded_spam_emails)\n",
    "        )\n",
    "        \n",
    "        num_true_negatives = sum(\n",
    "            [1 if s <= score_cutoff else 0 for s in ham_scores]\n",
    "        )\n",
    "        accuracy = (\n",
    "            (num_true_positives + num_true_negatives)\n",
    "            /\n",
    "            (len(ham_scores) + len(spam_scores))\n",
    "        )\n",
    "\n",
    "        return RecallResult(\n",
    "            score_cutoff = score_cutoff,\n",
    "            num_true_positives = num_true_positives,\n",
    "            recall = recall,\n",
    "            accuracy = accuracy\n",
    "        )\n",
    "\n",
    "    return [\n",
    "        (limit, calculate_result(limit)) for limit in limits\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate 0.001 | Recall 0.65 | Accuracy 0.91\n",
      "False Positive Rate 0.010 | Recall 0.90 | Accuracy 0.97\n",
      "False Positive Rate 0.020 | Recall 0.98 | Accuracy 0.98\n",
      "False Positive Rate 0.040 | Recall 0.99 | Accuracy 0.97\n",
      "False Positive Rate 0.080 | Recall 1.00 | Accuracy 0.94\n",
      "False Positive Rate 0.160 | Recall 1.00 | Accuracy 0.88\n"
     ]
    }
   ],
   "source": [
    "FALSE_POSITIVE_RATES = [0.001, 0.01, 0.02, 0.04, 0.08, 0.16]\n",
    "results = recall_for_false_positive_rates(\n",
    "    trainer.model,\n",
    "    test_set,\n",
    "    FALSE_POSITIVE_RATES\n",
    ")\n",
    "\n",
    "for (false_positive_rate, result) in results:\n",
    "    print(\n",
    "        f\"False Positive Rate {false_positive_rate:0.3f} | \"\n",
    "        f\"Recall {result.recall:0.2f} | \"\n",
    "        f\"Accuracy {result.accuracy:0.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way, way, better than the Naive Bayes model! The ability to freely set parameters allows much more accurate estimation of whether an email is spam, because the model doesn't have to assume that words are conditionally independent given email class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
