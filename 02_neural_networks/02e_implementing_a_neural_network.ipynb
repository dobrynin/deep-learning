{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Neural Network\n",
    "\n",
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\bigoh}[1]{\\mathcal{O}\\left(#1\\right)}\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\trans}[0]{^\\intercal}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is just our standard code to load the Enron email dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    os.getcwd(),\n",
    "    \"data/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Simple email class. All it does is allow you to read an email's text.\n",
    "class Email:\n",
    "    def __init__(self, path, label):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "\n",
    "    def text_content(self):\n",
    "        return type(self).read_text_content(self.path)\n",
    "    \n",
    "    def word_counts(self):\n",
    "        counts = {}\n",
    "        for word in self.text_content().split():\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "            \n",
    "        return counts\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, path, label):\n",
    "        return Email(\n",
    "            path = path,\n",
    "            label = label\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def read_text_content(cls, path):\n",
    "        full_path = os.path.join(DATA_DIR, path)\n",
    "        # Grr! Emails are encoded in Latin-1, not UTF-8. Python\n",
    "        # (rightly) freaks out.\n",
    "        with open(full_path, \"r\", encoding = \"iso-8859-1\") as f:\n",
    "            try:\n",
    "                return f.read()\n",
    "            except:\n",
    "                print(f\"Error with: {path}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "# This counts how many emails each word occurs in.\n",
    "def build_word_reaches(ham_emails, spam_emails):\n",
    "    word_reaches = {}\n",
    "    for emails in (ham_emails, spam_emails):\n",
    "        for email in emails:\n",
    "            for (word, _) in email.word_counts().items():\n",
    "                if word not in word_reaches:\n",
    "                    word_reaches[word] = 0\n",
    "                # No matter how frequent in the email, only counts once per email.\n",
    "                word_reaches[word] += 1\n",
    "                \n",
    "    return word_reaches\n",
    "\n",
    "# Throw away those emails that don't occur in at least 100 emails.\n",
    "# Throwing out low reach features means:\n",
    "# (1) Less chance for overfitting\n",
    "# (2) Smaller feature vectors, faster, less memory use.\n",
    "def filter_words(word_reaches, limit = 100):\n",
    "    filtered_words = SortedSet()\n",
    "    for (word, word_reach) in word_reaches.items():\n",
    "        if word_reach >= limit:\n",
    "            filtered_words.add(word)\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "# Bidirectional map. Limits to just filtered words, though.\n",
    "class FilteredWordEncodingDictionary:\n",
    "    def __init__(self, filtered_words):\n",
    "        self.word_to_code_dict = {}\n",
    "        self.code_to_word_dict = {}\n",
    "\n",
    "        for word in filtered_words:\n",
    "            self.insert_word(word)\n",
    "\n",
    "    # Only meant to be called when constructing the dictionary.\n",
    "    def insert_word(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            code = len(self.word_to_code_dict)\n",
    "            self.word_to_code_dict[word] = code\n",
    "            self.code_to_word_dict[code] = word\n",
    "\n",
    "    def word_to_code(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            return None\n",
    "\n",
    "        return self.word_to_code_dict[word]\n",
    "\n",
    "    def code_to_word(self, code):\n",
    "        if code not in self.code_to_word_dict:\n",
    "            raise f\"Code {code} not recorded!\"\n",
    "\n",
    "        return self.code_to_word_dict[code]\n",
    "\n",
    "    # This returns a vector of ones and zeros.\n",
    "    def encode_text(self, text):\n",
    "        codes = np.zeros(len(self.code_to_word_dict))\n",
    "\n",
    "        for word in text.split():\n",
    "            code = self.word_to_code(word)\n",
    "            if code is not None:\n",
    "                codes[code] = 1.0\n",
    "\n",
    "        return codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_to_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a simple subclass of Email that just encodes the words in an email.\n",
    "class EncodedEmail(Email):\n",
    "    def __init__(self, path, label, word_encoding_dictionary):\n",
    "        super().__init__(path, label)\n",
    "\n",
    "        self.codes = (\n",
    "            word_encoding_dictionary.encode_text(\n",
    "                self.text_content()\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "class Dataset:\n",
    "    DATA_FILE_PATH = os.path.join(DATA_DIR, 'lr_data.p')\n",
    "    WORD_REACH_LIMIT = 100\n",
    "\n",
    "    def __init__(\n",
    "            self, word_encoding_dictionary, encoded_ham_emails, encoded_spam_emails\n",
    "    ):\n",
    "        self.word_encoding_dictionary = word_encoding_dictionary\n",
    "        self.encoded_ham_emails = encoded_ham_emails\n",
    "        self.encoded_spam_emails = encoded_spam_emails\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_ham_emails) + len(self.encoded_spam_emails)\n",
    "    \n",
    "    @classmethod\n",
    "    def encode(cls, ham_emails, spam_emails):\n",
    "        # Count words, select which we will keep.\n",
    "        word_reaches = build_word_reaches(ham_emails, spam_emails)\n",
    "        filtered_words = filter_words(word_reaches, limit = cls.WORD_REACH_LIMIT)\n",
    "        \n",
    "        # Assign codes to all words.\n",
    "        word_encoding_dictionary = FilteredWordEncodingDictionary(filtered_words)\n",
    "        \n",
    "        # Encode each email as a vector of ones and zeros.\n",
    "        encoded_ham_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in ham_emails\n",
    "        ]\n",
    "        encoded_spam_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in spam_emails\n",
    "        ]\n",
    "        \n",
    "        # Construct the object!\n",
    "        return cls(\n",
    "            word_encoding_dictionary,\n",
    "            encoded_ham_emails,\n",
    "            encoded_spam_emails\n",
    "        )\n",
    "\n",
    "    INSTANCE = None\n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if not cls.INSTANCE:\n",
    "            with open(cls.DATA_FILE_PATH, 'rb') as f:\n",
    "                cls.INSTANCE = pickle.load(f)\n",
    "        return cls.INSTANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarfile already downloaded!\n",
      "Tarfile already extracted!\n",
      "Dataset already processed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "ENRON_SPAM_URL = (\n",
    "    \"http://csmining.org/index.php/\"\n",
    "    \"enron-spam-datasets.html\"\n",
    "    \"?file=tl_files/Project_Datasets/Enron-Spam%20datasets/Preprocessed\"\n",
    "    \"/enron1.tar.tar\"\n",
    ")\n",
    "\n",
    "TAR_FILE_NAME = \"enron1.tar.tar\"\n",
    "ENRON_DATA_DIR_NAME = \"enron1\"\n",
    "\n",
    "def download_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    if os.path.isfile(tarfile_path):\n",
    "        print(\"Tarfile already downloaded!\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading enron1.tar.tar\")\n",
    "    urlretrieve(ENRON_SPAM_URL, tarfile_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    enron_data_dir = os.path.join(DATA_DIR, ENRON_DATA_DIR_NAME)\n",
    "    if os.path.isdir(enron_data_dir):\n",
    "        print(\"Tarfile already extracted!\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting enron1.tar.tar\")\n",
    "    os.system(f\"tar -xf {tarfile_path} -C {DATA_DIR}\")\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def read_emails_dir(path, label):\n",
    "    emails = []\n",
    "    for email_fname in os.listdir(os.path.join(DATA_DIR, path)):\n",
    "        email_path = os.path.join(path, email_fname)\n",
    "        email = Email.read(\n",
    "            path = email_path,\n",
    "            label = label\n",
    "        )\n",
    "        emails.append(email)\n",
    "\n",
    "    return emails\n",
    "\n",
    "def build_dataset():\n",
    "    ham_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"ham\"),\n",
    "        label = 0\n",
    "    )\n",
    "    spam_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"spam\"),\n",
    "        label = 1\n",
    "    )\n",
    "\n",
    "    return Dataset.encode(\n",
    "        ham_emails = ham_emails,\n",
    "        spam_emails = spam_emails\n",
    "    )\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    with open(Dataset.DATA_FILE_PATH, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def build_and_save_dataset():\n",
    "    if os.path.isfile(Dataset.DATA_FILE_PATH):\n",
    "        print(\"Dataset already processed!\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading and processing emails!\")\n",
    "    dataset = build_dataset()\n",
    "    save_dataset(dataset)\n",
    "    print(\"Dataset created!\")\n",
    "\n",
    "download_tarfile()\n",
    "extract_tarfile()\n",
    "build_and_save_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "class DatasetSplitter:\n",
    "    @classmethod\n",
    "    def split(cls, dataset, ratio):\n",
    "        datasetA = cls._split(dataset, ratio, 0)\n",
    "        datasetB = cls._split(dataset, ratio, 1)\n",
    "        return (datasetA, datasetB)\n",
    "\n",
    "    @classmethod\n",
    "    def _split(cls, dataset, ratio, mode):\n",
    "        split_encoded_ham_emails, split_encoded_spam_emails = [], []\n",
    "        emails_pairs = [\n",
    "            (dataset.encoded_ham_emails, split_encoded_ham_emails),\n",
    "            (dataset.encoded_spam_emails, split_encoded_spam_emails)\n",
    "        ]\n",
    "\n",
    "        for (emails, split_emails) in emails_pairs:\n",
    "            for email in emails:\n",
    "                # This is a fancy way to pseudorandomly but\n",
    "                # deterministically select emails. That way we always\n",
    "                # pick the same set of emails for reproducability\n",
    "                # across program runs.\n",
    "                h = zlib.crc32(email.path.encode())\n",
    "                p = h / (2**32 - 1)\n",
    "                if (mode == 0 and p < ratio) or (mode == 1 and p >= ratio):\n",
    "                    split_emails.append(email)\n",
    "\n",
    "        return Dataset(\n",
    "            dataset.word_encoding_dictionary,\n",
    "            encoded_ham_emails = split_encoded_ham_emails,\n",
    "            encoded_spam_emails = split_encoded_spam_emails\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DatasetBatcher:\n",
    "    @classmethod\n",
    "    def batch(cls, d, batch_size):\n",
    "        emails = d.encoded_ham_emails + d.encoded_spam_emails\n",
    "        random.shuffle(emails)\n",
    "        idxs = range(0, len(emails), batch_size)\n",
    "        return [emails[idx:(idx + batch_size)] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.25 0.13@fpr=0.0000\n",
      "2: 0.17 0.24@fpr=0.0000\n",
      "3: 0.14 0.41@fpr=0.0000\n",
      "4: 0.12 0.54@fpr=0.0000\n",
      "5: 0.10 0.59@fpr=0.0000\n",
      "6: 0.09 0.65@fpr=0.0000\n",
      "7: 0.09 0.69@fpr=0.0000\n",
      "8: 0.08 0.69@fpr=0.0000\n",
      "9: 0.08 0.71@fpr=0.0000\n",
      "10: 0.07 0.74@fpr=0.0000\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import itertools\n",
    "\n",
    "d = Dataset.get()\n",
    "training_dataset, test_dataset = DatasetSplitter.split(d, 0.80)\n",
    "\n",
    "VOCAB_SIZE = len(d.word_encoding_dictionary)\n",
    "NUM_HIDDEN_UNITS = 128\n",
    "\n",
    "THETA1 = np.random.normal(\n",
    "    scale = 2 / np.sqrt(NUM_HIDDEN_UNITS + VOCAB_SIZE),\n",
    "    size = (NUM_HIDDEN_UNITS, VOCAB_SIZE)\n",
    ")\n",
    "B1 = np.zeros(NUM_HIDDEN_UNITS)\n",
    "THETA2 = np.random.normal(\n",
    "    scale = 2 / np.sqrt(1 + NUM_HIDDEN_UNITS),\n",
    "    size = (1, NUM_HIDDEN_UNITS)\n",
    ")\n",
    "B2 = np.zeros(1)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "ForwardResult = namedtuple('ForwardResult', 'x z2 a2 z3 a3')\n",
    "\n",
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward(email):\n",
    "    z2 = THETA1.dot(email.codes)\n",
    "    z2 += B1\n",
    "    a2 = logistic(z2)\n",
    "    \n",
    "    z3 = THETA2.dot(a2)\n",
    "    z3 += B2\n",
    "    a3 = logistic(z3)\n",
    "    \n",
    "    return ForwardResult(\n",
    "        x = email.codes,\n",
    "        z2 = z2,\n",
    "        a2 = a2,\n",
    "        z3 = z3,\n",
    "        a3 = a3\n",
    "    )\n",
    "\n",
    "BackwardResult = namedtuple('BackwardResult', 'da3, dz3, d_theta2, db2, da2, dz2, d_theta1, db1')\n",
    "\n",
    "def backward(forward_result, label):\n",
    "    da3 = dxe_error(forward_result.a3, label)\n",
    "    dz3 = dlogistic(forward_result.z3) * da3\n",
    "    d_theta2 = np.outer(dz3, forward_result.a2)\n",
    "    db2 = dz3\n",
    "    da2 = np.dot(THETA2.T, dz3)\n",
    "    dz2 = dlogistic(forward_result.z2) * da2\n",
    "    d_theta1 = np.outer(dz2, forward_result.x)\n",
    "    db1 = dz2\n",
    "    \n",
    "    return BackwardResult(\n",
    "        da3 = da3,\n",
    "        dz3 = dz3,\n",
    "        d_theta2 = d_theta2,\n",
    "        db2 = db2,\n",
    "        da2 = da2,\n",
    "        dz2 = dz2,\n",
    "        d_theta1 = d_theta1,\n",
    "        db1 = db1\n",
    "    )\n",
    "\n",
    "def xe_error(prob, label):\n",
    "    if label == 1:\n",
    "        return -np.log(prob)\n",
    "    else:\n",
    "        return -np.log(1 - prob)\n",
    "\n",
    "def dxe_error(prob, label):\n",
    "    if label == 1:\n",
    "        return -(1 / prob)\n",
    "    else:\n",
    "        return (1 / (1 - prob))\n",
    "\n",
    "def dlogistic(z):\n",
    "    return logistic(z) * logistic(1 - z)\n",
    "\n",
    "PartialsResult = namedtuple('PartialsResult', 'd_theta1, db1, d_theta2, db2')\n",
    "\n",
    "def partials(emails):\n",
    "    d_theta1 = np.zeros_like(THETA1)\n",
    "    db1 = np.zeros_like(B1)\n",
    "    d_theta2 = np.zeros_like(THETA2)\n",
    "    db2 = np.zeros_like(B2)\n",
    "\n",
    "    for email in emails:\n",
    "        forward_result = forward(email)\n",
    "        backward_result = backward(forward_result, email.label)\n",
    "        d_theta1 += backward_result.d_theta1\n",
    "        db1 += backward_result.db1\n",
    "        d_theta2 += backward_result.d_theta2\n",
    "        db2 += backward_result.db2\n",
    "\n",
    "    return PartialsResult(\n",
    "        d_theta1 = d_theta1,\n",
    "        db1 = db1,\n",
    "        d_theta2 = d_theta2,\n",
    "        db2 = db2\n",
    "    )\n",
    "\n",
    "def train(training_dataset, test_dataset, epochs, learning_rate):\n",
    "    global THETA1, B1, THETA2, B2\n",
    "    \n",
    "    batches = DatasetBatcher.batch(training_dataset, BATCH_SIZE)\n",
    "\n",
    "    for epoch_idx in range(1, 1 + epochs):\n",
    "        for batch in batches:\n",
    "            partials_result = partials(batch)\n",
    "            THETA1 -= (learning_rate / len(training_dataset)) * partials_result.d_theta1\n",
    "            B1 -= (learning_rate / len(training_dataset)) * partials_result.db1\n",
    "            THETA2 -= (learning_rate / len(training_dataset)) * partials_result.d_theta2\n",
    "            B2 -= (learning_rate / len(training_dataset)) * partials_result.db2\n",
    "\n",
    "        error = dataset_avg_error(test_dataset)\n",
    "        fpr, recall = dataset_recall(test_dataset)\n",
    "\n",
    "        print(\n",
    "            f\"{epoch_idx}: {error:0.2f} {recall:0.2f}@fpr={fpr:0.4f}\"\n",
    "        )\n",
    "\n",
    "def dataset_avg_error(d):\n",
    "    total_error = 0.0\n",
    "    for email in itertools.chain(d.encoded_ham_emails, d.encoded_spam_emails):\n",
    "        forward_result = forward(email)\n",
    "        total_error += xe_error(forward_result.a3[0], email.label)\n",
    "\n",
    "    return total_error / len(d)\n",
    "\n",
    "FPR_RATE = 0.0001\n",
    "def dataset_recall(d):\n",
    "    ham_scores = [forward(email).a3 for email in d.encoded_ham_emails]\n",
    "    ham_scores.sort()\n",
    "    ham_scores.reverse()\n",
    "    spam_scores = [forward(email).a3 for email in d.encoded_spam_emails]\n",
    "    \n",
    "    cutoff = ham_scores[int(FPR_RATE * len(ham_scores))]\n",
    "    num_false_positives = np.sum([\n",
    "        1 if score > cutoff else 0 for score in ham_scores\n",
    "    ])\n",
    "    num_true_positives = np.sum([\n",
    "        1 if score > cutoff else 0 for score in spam_scores\n",
    "    ])\n",
    "    \n",
    "    return (\n",
    "        num_false_positives / len(d.encoded_ham_emails),\n",
    "        num_true_positives / len(d.encoded_spam_emails)\n",
    "    )\n",
    "\n",
    "train(training_dataset, test_dataset, 10, 10.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4181 samples, validate on 991 samples\n",
      "Epoch 1/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.08\n",
      " - 0s - loss: 1.0720 - acc: 0.7238 - val_loss: 0.6194 - val_acc: 0.6297\n",
      "Epoch 2/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.20\n",
      " - 0s - loss: 0.2045 - acc: 0.9271 - val_loss: 0.1369 - val_acc: 0.9596\n",
      "Epoch 3/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.42\n",
      " - 0s - loss: 0.1272 - acc: 0.9572 - val_loss: 0.1087 - val_acc: 0.9697\n",
      "Epoch 4/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.65\n",
      " - 0s - loss: 0.0995 - acc: 0.9677 - val_loss: 0.0947 - val_acc: 0.9687\n",
      "Epoch 5/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.71\n",
      " - 0s - loss: 0.0868 - acc: 0.9725 - val_loss: 0.0805 - val_acc: 0.9758\n",
      "Epoch 6/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.68\n",
      " - 0s - loss: 0.0748 - acc: 0.9751 - val_loss: 0.0825 - val_acc: 0.9657\n",
      "Epoch 7/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.67\n",
      " - 0s - loss: 0.0660 - acc: 0.9780 - val_loss: 0.0691 - val_acc: 0.9798\n",
      "Epoch 8/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.65\n",
      " - 0s - loss: 0.0586 - acc: 0.9828 - val_loss: 0.0662 - val_acc: 0.9818\n",
      "Epoch 9/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.65\n",
      " - 0s - loss: 0.0539 - acc: 0.9837 - val_loss: 0.0638 - val_acc: 0.9818\n",
      "Epoch 10/10\n",
      ">> FPR: 0.0000\n",
      ">> Recall: 0.69\n",
      " - 0s - loss: 0.0499 - acc: 0.9847 - val_loss: 0.0610 - val_acc: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12876bac8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import keras.callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "\n",
    "training_dataset, validation_dataset = DatasetSplitter.split(d, 0.80)\n",
    "training_emails = training_dataset.encoded_ham_emails + training_dataset.encoded_spam_emails\n",
    "random.shuffle(training_emails)\n",
    "\n",
    "training_X = np.zeros(shape = (len(training_emails), VOCAB_SIZE))\n",
    "training_y = np.zeros(shape = len(training_emails))\n",
    "for idx, email in enumerate(training_emails):\n",
    "    training_X[idx, :] = email.codes\n",
    "    training_y[idx] = email.label\n",
    "\n",
    "validation_emails = validation_dataset.encoded_ham_emails + validation_dataset.encoded_spam_emails\n",
    "random.shuffle(validation_emails)\n",
    "validation_X = np.zeros(shape = (len(validation_emails), VOCAB_SIZE))\n",
    "validation_y = np.zeros(shape = len(validation_emails))\n",
    "for idx, email in enumerate(validation_emails):\n",
    "    validation_X[idx, :] = email.codes\n",
    "    validation_y[idx] = email.label\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(\n",
    "    128,\n",
    "    activation = 'sigmoid',\n",
    "    input_shape = (VOCAB_SIZE,)\n",
    "))\n",
    "model.add(Dense(\n",
    "    1,\n",
    "    activation = 'sigmoid'\n",
    "))\n",
    "\n",
    "FPR_RATE = 0.0001\n",
    "class RecallRate(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        scores = self.model.predict(self.validation_data[0])\n",
    "        targets = self.validation_data[1]\n",
    "\n",
    "        ham_scores = scores[np.logical_not(targets)]\n",
    "        ham_scores = np.sort(ham_scores)[::-1]\n",
    "        cutoff = ham_scores[int(FPR_RATE * len(ham_scores))]\n",
    "        \n",
    "        num_false_positives = np.sum(\n",
    "            scores[np.logical_not(targets)] > cutoff\n",
    "        )\n",
    "        num_true_positives = np.sum(\n",
    "            scores[targets.astype(np.bool)] > cutoff\n",
    "        )\n",
    "        fpr = num_false_positives / np.sum(np.logical_not(targets))\n",
    "        recall = num_true_positives / np.sum(targets)\n",
    "        \n",
    "        print(f\">> FPR: {fpr:0.4f}\")\n",
    "        print(f\">> Recall: {recall:0.2f}\")\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = SGD(lr = 1.0),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    training_X,\n",
    "    training_y,\n",
    "    epochs = 10,\n",
    "    batch_size = 128,\n",
    "    validation_data = (validation_X, validation_y),\n",
    "    callbacks = [RecallRate()],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
