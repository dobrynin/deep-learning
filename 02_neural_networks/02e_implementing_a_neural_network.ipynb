{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Neural Network\n",
    "\n",
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\bigoh}[1]{\\mathcal{O}\\left(#1\\right)}\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\trans}[0]{^\\intercal}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is just our standard code to load the Enron email dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    os.getcwd(),\n",
    "    \"data/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Simple email class. All it does is allow you to read an email's text.\n",
    "class Email:\n",
    "    def __init__(self, path, label):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "\n",
    "    def text_content(self):\n",
    "        return type(self).read_text_content(self.path)\n",
    "    \n",
    "    def word_counts(self):\n",
    "        counts = {}\n",
    "        for word in self.text_content().split():\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "            \n",
    "        return counts\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, path, label):\n",
    "        return Email(\n",
    "            path = path,\n",
    "            label = label\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def read_text_content(cls, path):\n",
    "        full_path = os.path.join(DATA_DIR, path)\n",
    "        # Grr! Emails are encoded in Latin-1, not UTF-8. Python\n",
    "        # (rightly) freaks out.\n",
    "        with open(full_path, \"r\", encoding = \"iso-8859-1\") as f:\n",
    "            try:\n",
    "                return f.read()\n",
    "            except:\n",
    "                print(f\"Error with: {path}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "# This counts how many emails each word occurs in.\n",
    "def build_word_reaches(ham_emails, spam_emails):\n",
    "    word_reaches = {}\n",
    "    for emails in (ham_emails, spam_emails):\n",
    "        for email in emails:\n",
    "            for (word, _) in email.word_counts().items():\n",
    "                if word not in word_reaches:\n",
    "                    word_reaches[word] = 0\n",
    "                # No matter how frequent in the email, only counts once per email.\n",
    "                word_reaches[word] += 1\n",
    "                \n",
    "    return word_reaches\n",
    "\n",
    "# Throw away those emails that don't occur in at least 100 emails.\n",
    "# Throwing out low reach features means:\n",
    "# (1) Less chance for overfitting\n",
    "# (2) Smaller feature vectors, faster, less memory use.\n",
    "def filter_words(word_reaches, limit = 100):\n",
    "    filtered_words = SortedSet()\n",
    "    for (word, word_reach) in word_reaches.items():\n",
    "        if word_reach >= limit:\n",
    "            filtered_words.add(word)\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "# Bidirectional map. Limits to just filtered words, though.\n",
    "class FilteredWordEncodingDictionary:\n",
    "    def __init__(self, filtered_words):\n",
    "        self.word_to_code_dict = {}\n",
    "        self.code_to_word_dict = {}\n",
    "\n",
    "        for word in filtered_words:\n",
    "            self.insert_word(word)\n",
    "\n",
    "    # Only meant to be called when constructing the dictionary.\n",
    "    def insert_word(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            code = len(self.word_to_code_dict)\n",
    "            self.word_to_code_dict[word] = code\n",
    "            self.code_to_word_dict[code] = word\n",
    "\n",
    "    def word_to_code(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            return None\n",
    "\n",
    "        return self.word_to_code_dict[word]\n",
    "\n",
    "    def code_to_word(self, code):\n",
    "        if code not in self.code_to_word_dict:\n",
    "            raise f\"Code {code} not recorded!\"\n",
    "\n",
    "        return self.code_to_word_dict[code]\n",
    "\n",
    "    # This returns a vector of ones and zeros.\n",
    "    def encode_text(self, text):\n",
    "        codes = np.zeros(len(self.code_to_word_dict))\n",
    "\n",
    "        for word in text.split():\n",
    "            code = self.word_to_code(word)\n",
    "            if code is not None:\n",
    "                codes[code] = 1.0\n",
    "\n",
    "        return codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_to_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a simple subclass of Email that just encodes the words in an email.\n",
    "class EncodedEmail(Email):\n",
    "    def __init__(self, path, label, word_encoding_dictionary):\n",
    "        super().__init__(path, label)\n",
    "\n",
    "        self.codes = (\n",
    "            word_encoding_dictionary.encode_text(\n",
    "                self.text_content()\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "class Dataset:\n",
    "    DATA_FILE_PATH = os.path.join(DATA_DIR, 'lr_data.p')\n",
    "    WORD_REACH_LIMIT = 100\n",
    "\n",
    "    def __init__(\n",
    "            self, word_encoding_dictionary, encoded_ham_emails, encoded_spam_emails\n",
    "    ):\n",
    "        self.word_encoding_dictionary = word_encoding_dictionary\n",
    "        self.encoded_ham_emails = encoded_ham_emails\n",
    "        self.encoded_spam_emails = encoded_spam_emails\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_ham_emails) + len(self.encoded_spam_emails)\n",
    "    \n",
    "    @classmethod\n",
    "    def encode(cls, ham_emails, spam_emails):\n",
    "        # Count words, select which we will keep.\n",
    "        word_reaches = build_word_reaches(ham_emails, spam_emails)\n",
    "        filtered_words = filter_words(word_reaches, limit = cls.WORD_REACH_LIMIT)\n",
    "        \n",
    "        # Assign codes to all words.\n",
    "        word_encoding_dictionary = FilteredWordEncodingDictionary(filtered_words)\n",
    "        \n",
    "        # Encode each email as a vector of ones and zeros.\n",
    "        encoded_ham_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in ham_emails\n",
    "        ]\n",
    "        encoded_spam_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in spam_emails\n",
    "        ]\n",
    "        \n",
    "        # Construct the object!\n",
    "        return cls(\n",
    "            word_encoding_dictionary,\n",
    "            encoded_ham_emails,\n",
    "            encoded_spam_emails\n",
    "        )\n",
    "\n",
    "    INSTANCE = None\n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if not cls.INSTANCE:\n",
    "            with open(cls.DATA_FILE_PATH, 'rb') as f:\n",
    "                cls.INSTANCE = pickle.load(f)\n",
    "        return cls.INSTANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading enron1.tar.tar\n",
      "Download complete!\n",
      "Extracting enron1.tar.tar\n",
      "Extraction complete!\n",
      "Reading and processing emails!\n",
      "Dataset created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "ENRON_SPAM_URL = (\n",
    "    \"http://csmining.org/index.php/\"\n",
    "    \"enron-spam-datasets.html\"\n",
    "    \"?file=tl_files/Project_Datasets/Enron-Spam%20datasets/Preprocessed\"\n",
    "    \"/enron1.tar.tar\"\n",
    ")\n",
    "\n",
    "TAR_FILE_NAME = \"enron1.tar.tar\"\n",
    "ENRON_DATA_DIR_NAME = \"enron1\"\n",
    "\n",
    "def download_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    if os.path.isfile(tarfile_path):\n",
    "        print(\"Tarfile already downloaded!\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading enron1.tar.tar\")\n",
    "    urlretrieve(ENRON_SPAM_URL, tarfile_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    enron_data_dir = os.path.join(DATA_DIR, ENRON_DATA_DIR_NAME)\n",
    "    if os.path.isdir(enron_data_dir):\n",
    "        print(\"Tarfile already extracted!\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting enron1.tar.tar\")\n",
    "    os.system(f\"tar -xf {tarfile_path} -C {DATA_DIR}\")\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def read_emails_dir(path, label):\n",
    "    emails = []\n",
    "    for email_fname in os.listdir(os.path.join(DATA_DIR, path)):\n",
    "        email_path = os.path.join(path, email_fname)\n",
    "        email = Email.read(\n",
    "            path = email_path,\n",
    "            label = label\n",
    "        )\n",
    "        emails.append(email)\n",
    "\n",
    "    return emails\n",
    "\n",
    "def build_dataset():\n",
    "    ham_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"ham\"),\n",
    "        label = 0\n",
    "    )\n",
    "    spam_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"spam\"),\n",
    "        label = 1\n",
    "    )\n",
    "\n",
    "    return Dataset.encode(\n",
    "        ham_emails = ham_emails,\n",
    "        spam_emails = spam_emails\n",
    "    )\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    with open(Dataset.DATA_FILE_PATH, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def build_and_save_dataset():\n",
    "    if os.path.isfile(Dataset.DATA_FILE_PATH):\n",
    "        print(\"Dataset already processed!\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading and processing emails!\")\n",
    "    dataset = build_dataset()\n",
    "    save_dataset(dataset)\n",
    "    print(\"Dataset created!\")\n",
    "\n",
    "download_tarfile()\n",
    "extract_tarfile()\n",
    "build_and_save_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "class DatasetSplitter:\n",
    "    @classmethod\n",
    "    def split(cls, dataset, ratio):\n",
    "        datasetA = cls._split(dataset, ratio, 0)\n",
    "        datasetB = cls._split(dataset, ratio, 1)\n",
    "        return (datasetA, datasetB)\n",
    "\n",
    "    @classmethod\n",
    "    def _split(cls, dataset, ratio, mode):\n",
    "        split_encoded_ham_emails, split_encoded_spam_emails = [], []\n",
    "        emails_pairs = [\n",
    "            (dataset.encoded_ham_emails, split_encoded_ham_emails),\n",
    "            (dataset.encoded_spam_emails, split_encoded_spam_emails)\n",
    "        ]\n",
    "\n",
    "        for (emails, split_emails) in emails_pairs:\n",
    "            for email in emails:\n",
    "                # This is a fancy way to pseudorandomly but\n",
    "                # deterministically select emails. That way we always\n",
    "                # pick the same set of emails for reproducability\n",
    "                # across program runs.\n",
    "                h = zlib.crc32(email.path.encode())\n",
    "                p = h / (2**32 - 1)\n",
    "                if (mode == 0 and p < ratio) or (mode == 1 and p >= ratio):\n",
    "                    split_emails.append(email)\n",
    "\n",
    "        return Dataset(\n",
    "            dataset.word_encoding_dictionary,\n",
    "            encoded_ham_emails = split_encoded_ham_emails,\n",
    "            encoded_spam_emails = split_encoded_spam_emails\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DatasetBatcher:\n",
    "    @classmethod\n",
    "    def batch(cls, d, batch_size):\n",
    "        emails = d.encoded_ham_emails + d.encoded_spam_emails\n",
    "        random.shuffle(emails)\n",
    "        idxs = range(0, len(emails), batch_size)\n",
    "        return [emails[idx:(idx + batch_size)] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.29 {'false_positive_rate': 0.012448132780082987, 'recall': 0.49626865671641796}\n",
      "2: 0.18 {'false_positive_rate': 0.029045643153526972, 'recall': 0.9216417910447761}\n",
      "3: 0.14 {'false_positive_rate': 0.029045643153526972, 'recall': 0.9328358208955224}\n",
      "4: 0.12 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9477611940298507}\n",
      "5: 0.10 {'false_positive_rate': 0.0318118948824343, 'recall': 0.9664179104477612}\n",
      "6: 0.09 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9738805970149254}\n",
      "7: 0.09 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9776119402985075}\n",
      "8: 0.08 {'false_positive_rate': 0.0318118948824343, 'recall': 0.9850746268656716}\n",
      "9: 0.08 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9850746268656716}\n",
      "10: 0.07 {'false_positive_rate': 0.0318118948824343, 'recall': 0.9850746268656716}\n",
      "11: 0.07 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9850746268656716}\n",
      "12: 0.07 {'false_positive_rate': 0.029045643153526972, 'recall': 0.9850746268656716}\n",
      "13: 0.07 {'false_positive_rate': 0.029045643153526972, 'recall': 0.9850746268656716}\n",
      "14: 0.07 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9850746268656716}\n",
      "15: 0.06 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9850746268656716}\n",
      "16: 0.06 {'false_positive_rate': 0.030428769017980636, 'recall': 0.9850746268656716}\n",
      "17: 0.06 {'false_positive_rate': 0.029045643153526972, 'recall': 0.9850746268656716}\n",
      "18: 0.06 {'false_positive_rate': 0.029045643153526972, 'recall': 0.9850746268656716}\n",
      "19: 0.06 {'false_positive_rate': 0.029045643153526972, 'recall': 0.9850746268656716}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-684494388f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m     }\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.00\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-684494388f2a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(training_dataset, test_dataset, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mpartials_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mTHETA1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpartials_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_theta1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mB1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpartials_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-684494388f2a>\u001b[0m in \u001b[0;36mpartials\u001b[0;34m(emails)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0memail\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memails\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mforward_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mbackward_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0md_theta1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbackward_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_theta1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdb1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbackward_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-684494388f2a>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(forward_result, label)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mda3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdxe_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdz3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mda3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0md_theta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdz3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-684494388f2a>\u001b[0m in \u001b[0;36mdlogistic\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mPartialsResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PartialsResult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd_theta1, db1, d_theta2, db2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-684494388f2a>\u001b[0m in \u001b[0;36mlogistic\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import itertools\n",
    "\n",
    "d = Dataset.get()\n",
    "training_dataset, test_dataset = DatasetSplitter.split(d, 0.80)\n",
    "\n",
    "VOCAB_SIZE = len(d.word_encoding_dictionary)\n",
    "NUM_HIDDEN_UNITS = 128\n",
    "\n",
    "THETA1 = np.random.normal(\n",
    "    scale = 2 / np.sqrt(NUM_HIDDEN_UNITS + VOCAB_SIZE),\n",
    "    size = (NUM_HIDDEN_UNITS, VOCAB_SIZE)\n",
    ")\n",
    "B1 = np.zeros(NUM_HIDDEN_UNITS)\n",
    "THETA2 = np.random.normal(\n",
    "    scale = 2 / np.sqrt(1 + NUM_HIDDEN_UNITS),\n",
    "    size = (1, NUM_HIDDEN_UNITS)\n",
    ")\n",
    "B2 = np.zeros(1)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "ForwardResult = namedtuple('ForwardResult', 'x z2 a2 z3 a3')\n",
    "\n",
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward(email):\n",
    "    z2 = THETA1.dot(email.codes)\n",
    "    z2 += B1\n",
    "    a2 = logistic(z2)\n",
    "    \n",
    "    z3 = THETA2.dot(a2)\n",
    "    z3 += B2\n",
    "    a3 = logistic(z3)\n",
    "    \n",
    "    return ForwardResult(\n",
    "        x = email.codes,\n",
    "        z2 = z2,\n",
    "        a2 = a2,\n",
    "        z3 = z3,\n",
    "        a3 = a3\n",
    "    )\n",
    "\n",
    "BackwardResult = namedtuple('BackwardResult', 'da3, dz3, d_theta2, db2, da2, dz2, d_theta1, db1')\n",
    "\n",
    "def backward(forward_result, label):\n",
    "    da3 = dxe_error(forward_result.a3, label)\n",
    "    dz3 = dlogistic(forward_result.z3) * da3\n",
    "    d_theta2 = np.outer(dz3, forward_result.a2)\n",
    "    db2 = dz3\n",
    "    da2 = np.dot(THETA2.T, dz3)\n",
    "    dz2 = dlogistic(forward_result.z2) * da2\n",
    "    d_theta1 = np.outer(dz2, forward_result.x)\n",
    "    db1 = dz2\n",
    "    \n",
    "    return BackwardResult(\n",
    "        da3 = da3,\n",
    "        dz3 = dz3,\n",
    "        d_theta2 = d_theta2,\n",
    "        db2 = db2,\n",
    "        da2 = da2,\n",
    "        dz2 = dz2,\n",
    "        d_theta1 = d_theta1,\n",
    "        db1 = db1\n",
    "    )\n",
    "\n",
    "def xe_error(prob, label):\n",
    "    if label == 1:\n",
    "        return -np.log(prob)\n",
    "    else:\n",
    "        return -np.log(1 - prob)\n",
    "\n",
    "def dxe_error(prob, label):\n",
    "    if label == 1:\n",
    "        return -(1 / prob)\n",
    "    else:\n",
    "        return (1 / (1 - prob))\n",
    "\n",
    "def dlogistic(z):\n",
    "    return logistic(z) * logistic(1 - z)\n",
    "\n",
    "PartialsResult = namedtuple('PartialsResult', 'd_theta1, db1, d_theta2, db2')\n",
    "\n",
    "def partials(emails):\n",
    "    d_theta1 = np.zeros_like(THETA1)\n",
    "    db1 = np.zeros_like(B1)\n",
    "    d_theta2 = np.zeros_like(THETA2)\n",
    "    db2 = np.zeros_like(B2)\n",
    "\n",
    "    for email in emails:\n",
    "        forward_result = forward(email)\n",
    "        backward_result = backward(forward_result, email.label)\n",
    "        d_theta1 += backward_result.d_theta1\n",
    "        db1 += backward_result.db1\n",
    "        d_theta2 += backward_result.d_theta2\n",
    "        db2 += backward_result.db2\n",
    "\n",
    "    return PartialsResult(\n",
    "        d_theta1 = d_theta1,\n",
    "        db1 = db1,\n",
    "        d_theta2 = d_theta2,\n",
    "        db2 = db2\n",
    "    )\n",
    "\n",
    "def train(training_dataset, test_dataset, epochs, learning_rate):\n",
    "    global THETA1, B1, THETA2, B2\n",
    "    \n",
    "    batches = DatasetBatcher.batch(training_dataset, BATCH_SIZE)\n",
    "\n",
    "    for epoch_idx in range(1, 1 + epochs):\n",
    "        for batch in batches:\n",
    "            partials_result = partials(batch)\n",
    "            THETA1 -= (learning_rate / len(training_dataset)) * partials_result.d_theta1\n",
    "            B1 -= (learning_rate / len(training_dataset)) * partials_result.db1\n",
    "            THETA2 -= (learning_rate / len(training_dataset)) * partials_result.d_theta2\n",
    "            B2 -= (learning_rate / len(training_dataset)) * partials_result.db2\n",
    "\n",
    "        error = dataset_avg_error(test_dataset)\n",
    "        accuracy = dataset_accuracy(test_dataset)\n",
    "\n",
    "        print(\n",
    "            f\"{epoch_idx}: {error:0.2f} {accuracy}\"\n",
    "        )\n",
    "\n",
    "def dataset_avg_error(d):\n",
    "    total_error = 0.0\n",
    "    for email in itertools.chain(d.encoded_ham_emails, d.encoded_spam_emails):\n",
    "        forward_result = forward(email)\n",
    "        total_error += xe_error(forward_result.a3[0], email.label)\n",
    "\n",
    "    return total_error / len(d)\n",
    "\n",
    "def dataset_accuracy(d):\n",
    "    false_positive_errors = 0\n",
    "    false_negative_errors = 0\n",
    "    for email in itertools.chain(d.encoded_ham_emails, d.encoded_spam_emails):\n",
    "        forward_result = forward(email)\n",
    "        if (forward_result.a3 > 0.50) and (email.label == 0):\n",
    "            false_positive_errors += 1\n",
    "        elif (forward_result.a3 <= 0.50) and (email.label == 1):\n",
    "            false_negative_errors += 1\n",
    "            \n",
    "    return {\n",
    "        'false_positive_rate': false_positive_errors / len(d.encoded_ham_emails),\n",
    "        'recall': 1 - (false_negative_errors / len(d.encoded_spam_emails))\n",
    "    }\n",
    "\n",
    "train(training_dataset, test_dataset, 100, 10.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4181 samples, validate on 991 samples\n",
      "Epoch 1/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.71\n",
      " - 2s - loss: 27.3485 - acc: 0.7663 - val_loss: 9.7137 - val_acc: 0.9122\n",
      "Epoch 2/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.86\n",
      " - 0s - loss: 11.8271 - acc: 0.8850 - val_loss: 7.2740 - val_acc: 0.9506\n",
      "Epoch 3/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.88\n",
      " - 0s - loss: 8.7129 - acc: 0.9189 - val_loss: 5.4888 - val_acc: 0.9586\n",
      "Epoch 4/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.89\n",
      " - 0s - loss: 7.9898 - acc: 0.9316 - val_loss: 5.2245 - val_acc: 0.9596\n",
      "Epoch 5/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.76\n",
      " - 0s - loss: 6.9071 - acc: 0.9381 - val_loss: 4.9401 - val_acc: 0.9334\n",
      "Epoch 6/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.83\n",
      " - 0s - loss: 5.9917 - acc: 0.9491 - val_loss: 4.4580 - val_acc: 0.9485\n",
      "Epoch 7/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.88\n",
      " - 0s - loss: 5.4779 - acc: 0.9522 - val_loss: 3.9556 - val_acc: 0.9637\n",
      "Epoch 8/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.88\n",
      " - 0s - loss: 5.7318 - acc: 0.9491 - val_loss: 3.8674 - val_acc: 0.9657\n",
      "Epoch 9/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.91\n",
      " - 0s - loss: 5.0150 - acc: 0.9605 - val_loss: 3.6878 - val_acc: 0.9707\n",
      "Epoch 10/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.88\n",
      " - 0s - loss: 4.6284 - acc: 0.9610 - val_loss: 3.6591 - val_acc: 0.9637\n",
      "Epoch 11/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.90\n",
      " - 0s - loss: 4.0597 - acc: 0.9663 - val_loss: 3.5541 - val_acc: 0.9667\n",
      "Epoch 12/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.91\n",
      " - 0s - loss: 4.5055 - acc: 0.9620 - val_loss: 3.4580 - val_acc: 0.9707\n",
      "Epoch 13/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.93\n",
      " - 0s - loss: 4.0167 - acc: 0.9684 - val_loss: 3.5107 - val_acc: 0.9768\n",
      "Epoch 14/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 3.6823 - acc: 0.9668 - val_loss: 3.6659 - val_acc: 0.9788\n",
      "Epoch 15/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.93\n",
      " - 0s - loss: 3.6115 - acc: 0.9694 - val_loss: 3.3025 - val_acc: 0.9758\n",
      "Epoch 16/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.92\n",
      " - 0s - loss: 3.6549 - acc: 0.9713 - val_loss: 3.4088 - val_acc: 0.9717\n",
      "Epoch 17/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.94\n",
      " - 0s - loss: 3.1386 - acc: 0.9735 - val_loss: 3.4834 - val_acc: 0.9758\n",
      "Epoch 18/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.95\n",
      " - 0s - loss: 3.3226 - acc: 0.9754 - val_loss: 3.3446 - val_acc: 0.9808\n",
      "Epoch 19/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.95\n",
      " - 0s - loss: 3.0622 - acc: 0.9754 - val_loss: 3.6846 - val_acc: 0.9758\n",
      "Epoch 20/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.95\n",
      " - 0s - loss: 2.9850 - acc: 0.9749 - val_loss: 3.4493 - val_acc: 0.9798\n",
      "Epoch 21/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 3.0092 - acc: 0.9756 - val_loss: 3.5552 - val_acc: 0.9788\n",
      "Epoch 22/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 2.8475 - acc: 0.9756 - val_loss: 3.5930 - val_acc: 0.9798\n",
      "Epoch 23/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.94\n",
      " - 0s - loss: 2.9107 - acc: 0.9766 - val_loss: 3.4599 - val_acc: 0.9778\n",
      "Epoch 24/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 2.8805 - acc: 0.9744 - val_loss: 3.8157 - val_acc: 0.9788\n",
      "Epoch 25/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 2.5012 - acc: 0.9801 - val_loss: 3.3278 - val_acc: 0.9798\n",
      "Epoch 26/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 2.8476 - acc: 0.9787 - val_loss: 3.1496 - val_acc: 0.9808\n",
      "Epoch 27/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.95\n",
      " - 0s - loss: 2.7175 - acc: 0.9785 - val_loss: 3.4544 - val_acc: 0.9778\n",
      "Epoch 28/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 2.5492 - acc: 0.9782 - val_loss: 3.4686 - val_acc: 0.9798\n",
      "Epoch 29/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 2.0915 - acc: 0.9835 - val_loss: 3.5189 - val_acc: 0.9808\n",
      "Epoch 30/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.93\n",
      " - 0s - loss: 2.7313 - acc: 0.9811 - val_loss: 3.5066 - val_acc: 0.9748\n",
      "Epoch 31/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.96\n",
      " - 0s - loss: 2.6398 - acc: 0.9792 - val_loss: 3.6118 - val_acc: 0.9798\n",
      "Epoch 32/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.97\n",
      " - 0s - loss: 2.1131 - acc: 0.9806 - val_loss: 4.1395 - val_acc: 0.9798\n",
      "Epoch 33/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.94\n",
      " - 0s - loss: 1.9765 - acc: 0.9840 - val_loss: 3.1908 - val_acc: 0.9778\n",
      "Epoch 34/100\n",
      ">> FPR: 0.001\n",
      ">> Recall: 0.92\n",
      " - 0s - loss: 2.0847 - acc: 0.9845 - val_loss: 3.0415 - val_acc: 0.9738\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-2c6dc07d44f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalidation_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRecallRate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m )\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ruggeri/.anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import keras.callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "\n",
    "training_dataset, validation_dataset = DatasetSplitter.split(d, 0.80)\n",
    "training_emails = training_dataset.encoded_ham_emails + training_dataset.encoded_spam_emails\n",
    "random.shuffle(training_emails)\n",
    "\n",
    "training_X = np.zeros(shape = (len(training_emails), VOCAB_SIZE))\n",
    "training_y = np.zeros(shape = len(training_emails))\n",
    "for idx, email in enumerate(training_emails):\n",
    "    training_X[idx, :] = email.codes\n",
    "    training_y[idx] = email.label\n",
    "\n",
    "validation_emails = validation_dataset.encoded_ham_emails + validation_dataset.encoded_spam_emails\n",
    "random.shuffle(validation_emails)\n",
    "validation_X = np.zeros(shape = (len(validation_emails), VOCAB_SIZE))\n",
    "validation_y = np.zeros(shape = len(validation_emails))\n",
    "for idx, email in enumerate(validation_emails):\n",
    "    validation_X[idx, :] = email.codes\n",
    "    validation_y[idx] = email.label\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(\n",
    "    128,\n",
    "    activation = 'sigmoid',\n",
    "    input_shape = (VOCAB_SIZE,)\n",
    "))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(\n",
    "    1,\n",
    "    activation = 'sigmoid'\n",
    "))\n",
    "\n",
    "class RecallRate(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        scores = self.model.predict(self.validation_data[0])\n",
    "        predictions = np.round(scores)\n",
    "        targets = self.validation_data[1]\n",
    "\n",
    "        ham_scores = scores[np.logical_not(targets)]\n",
    "        cutoff = ham_scores[int(0.001 * len(ham_scores))]\n",
    "        \n",
    "        num_true_positives = np.sum(\n",
    "            predictions[targets.astype(np.bool)] > cutoff\n",
    "        )\n",
    "        recall = num_true_positives / np.sum(targets)\n",
    "        \n",
    "        print(f\">> FPR: 0.001\")\n",
    "        print(f\">> Recall: {recall:0.2f}\")\n",
    "\n",
    "def enhanced_crossentropy(true_y, pred_y):\n",
    "    true_bits = K.sum(\n",
    "        -K.log(\n",
    "            pred_y * true_y\n",
    "            + (1 - true_y)\n",
    "        )\n",
    "    )\n",
    "    false_bits = K.sum(\n",
    "        -K.log(\n",
    "            (1 - pred_y) * (1 - true_y)\n",
    "            + true_y\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Weight true misaccuracy much more heavily\n",
    "    return true_bits + 4 * false_bits\n",
    "\n",
    "model.compile(\n",
    "    loss = enhanced_crossentropy,\n",
    "    optimizer = SGD(),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    training_X,\n",
    "    training_y,\n",
    "    epochs = 100,\n",
    "    batch_size = 32,\n",
    "    validation_data = (validation_X, validation_y),\n",
    "    callbacks = [RecallRate()],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
