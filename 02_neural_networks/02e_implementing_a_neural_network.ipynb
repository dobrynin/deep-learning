{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Neural Network\n",
    "\n",
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\bigoh}[1]{\\mathcal{O}\\left(#1\\right)}\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\trans}[0]{^\\intercal}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is just our standard code to load the Enron email dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    os.getcwd(),\n",
    "    \"data/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Simple email class. All it does is allow you to read an email's text.\n",
    "class Email:\n",
    "    def __init__(self, path, label):\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "\n",
    "    def text_content(self):\n",
    "        return type(self).read_text_content(self.path)\n",
    "    \n",
    "    def word_counts(self):\n",
    "        counts = {}\n",
    "        for word in self.text_content().split():\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "            \n",
    "        return counts\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, path, label):\n",
    "        return Email(\n",
    "            path = path,\n",
    "            label = label\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def read_text_content(cls, path):\n",
    "        full_path = os.path.join(DATA_DIR, path)\n",
    "        # Grr! Emails are encoded in Latin-1, not UTF-8. Python\n",
    "        # (rightly) freaks out.\n",
    "        with open(full_path, \"r\", encoding = \"iso-8859-1\") as f:\n",
    "            try:\n",
    "                return f.read()\n",
    "            except:\n",
    "                print(f\"Error with: {path}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "# This counts how many emails each word occurs in.\n",
    "def build_word_reaches(ham_emails, spam_emails):\n",
    "    word_reaches = {}\n",
    "    for emails in (ham_emails, spam_emails):\n",
    "        for email in emails:\n",
    "            for (word, _) in email.word_counts().items():\n",
    "                if word not in word_reaches:\n",
    "                    word_reaches[word] = 0\n",
    "                # No matter how frequent in the email, only counts once per email.\n",
    "                word_reaches[word] += 1\n",
    "                \n",
    "    return word_reaches\n",
    "\n",
    "# Throw away those emails that don't occur in at least 100 emails.\n",
    "# Throwing out low reach features means:\n",
    "# (1) Less chance for overfitting\n",
    "# (2) Smaller feature vectors, faster, less memory use.\n",
    "def filter_words(word_reaches, limit = 100):\n",
    "    filtered_words = SortedSet()\n",
    "    for (word, word_reach) in word_reaches.items():\n",
    "        if word_reach >= limit:\n",
    "            filtered_words.add(word)\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "# Bidirectional map. Limits to just filtered words, though.\n",
    "class FilteredWordEncodingDictionary:\n",
    "    def __init__(self, filtered_words):\n",
    "        self.word_to_code_dict = {}\n",
    "        self.code_to_word_dict = {}\n",
    "\n",
    "        for word in filtered_words:\n",
    "            self.insert_word(word)\n",
    "\n",
    "    # Only meant to be called when constructing the dictionary.\n",
    "    def insert_word(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            code = len(self.word_to_code_dict)\n",
    "            self.word_to_code_dict[word] = code\n",
    "            self.code_to_word_dict[code] = word\n",
    "\n",
    "    def word_to_code(self, word):\n",
    "        if word not in self.word_to_code_dict:\n",
    "            return None\n",
    "\n",
    "        return self.word_to_code_dict[word]\n",
    "\n",
    "    def code_to_word(self, code):\n",
    "        if code not in self.code_to_word_dict:\n",
    "            raise f\"Code {code} not recorded!\"\n",
    "\n",
    "        return self.code_to_word_dict[code]\n",
    "\n",
    "    # This returns a vector of ones and zeros.\n",
    "    def encode_text(self, text):\n",
    "        codes = np.zeros(len(self.code_to_word_dict))\n",
    "\n",
    "        for word in text.split():\n",
    "            code = self.word_to_code(word)\n",
    "            if code is not None:\n",
    "                codes[code] = 1.0\n",
    "\n",
    "        return codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.code_to_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a simple subclass of Email that just encodes the words in an email.\n",
    "class EncodedEmail(Email):\n",
    "    def __init__(self, path, label, word_encoding_dictionary):\n",
    "        super().__init__(path, label)\n",
    "\n",
    "        self.codes = (\n",
    "            word_encoding_dictionary.encode_text(\n",
    "                self.text_content()\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "class Dataset:\n",
    "    DATA_FILE_PATH = os.path.join(DATA_DIR, 'lr_data.p')\n",
    "    WORD_REACH_LIMIT = 100\n",
    "\n",
    "    def __init__(\n",
    "            self, word_encoding_dictionary, encoded_ham_emails, encoded_spam_emails\n",
    "    ):\n",
    "        self.word_encoding_dictionary = word_encoding_dictionary\n",
    "        self.encoded_ham_emails = encoded_ham_emails\n",
    "        self.encoded_spam_emails = encoded_spam_emails\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_ham_emails) + len(self.encoded_spam_emails)\n",
    "    \n",
    "    @classmethod\n",
    "    def encode(cls, ham_emails, spam_emails):\n",
    "        # Count words, select which we will keep.\n",
    "        word_reaches = build_word_reaches(ham_emails, spam_emails)\n",
    "        filtered_words = filter_words(word_reaches, limit = cls.WORD_REACH_LIMIT)\n",
    "        \n",
    "        # Assign codes to all words.\n",
    "        word_encoding_dictionary = FilteredWordEncodingDictionary(filtered_words)\n",
    "        \n",
    "        # Encode each email as a vector of ones and zeros.\n",
    "        encoded_ham_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in ham_emails\n",
    "        ]\n",
    "        encoded_spam_emails = [\n",
    "            EncodedEmail(e.path, e.label, word_encoding_dictionary)\n",
    "            for\n",
    "            e in spam_emails\n",
    "        ]\n",
    "        \n",
    "        # Construct the object!\n",
    "        return cls(\n",
    "            word_encoding_dictionary,\n",
    "            encoded_ham_emails,\n",
    "            encoded_spam_emails\n",
    "        )\n",
    "\n",
    "    INSTANCE = None\n",
    "    @classmethod\n",
    "    def get(cls):\n",
    "        if not cls.INSTANCE:\n",
    "            with open(cls.DATA_FILE_PATH, 'rb') as f:\n",
    "                cls.INSTANCE = pickle.load(f)\n",
    "        return cls.INSTANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarfile already downloaded!\n",
      "Tarfile already extracted!\n",
      "Dataset already processed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "ENRON_SPAM_URL = (\n",
    "    \"http://csmining.org/index.php/\"\n",
    "    \"enron-spam-datasets.html\"\n",
    "    \"?file=tl_files/Project_Datasets/Enron-Spam%20datasets/Preprocessed\"\n",
    "    \"/enron1.tar.tar\"\n",
    ")\n",
    "\n",
    "TAR_FILE_NAME = \"enron1.tar.tar\"\n",
    "ENRON_DATA_DIR_NAME = \"enron1\"\n",
    "\n",
    "def download_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    if os.path.isfile(tarfile_path):\n",
    "        print(\"Tarfile already downloaded!\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading enron1.tar.tar\")\n",
    "    urlretrieve(ENRON_SPAM_URL, tarfile_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_tarfile():\n",
    "    tarfile_path = os.path.join(DATA_DIR, TAR_FILE_NAME)\n",
    "    enron_data_dir = os.path.join(DATA_DIR, ENRON_DATA_DIR_NAME)\n",
    "    if os.path.isdir(enron_data_dir):\n",
    "        print(\"Tarfile already extracted!\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting enron1.tar.tar\")\n",
    "    os.system(f\"tar -xf {tarfile_path} -C {DATA_DIR}\")\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "def read_emails_dir(path, label):\n",
    "    emails = []\n",
    "    for email_fname in os.listdir(os.path.join(DATA_DIR, path)):\n",
    "        email_path = os.path.join(path, email_fname)\n",
    "        email = Email.read(\n",
    "            path = email_path,\n",
    "            label = label\n",
    "        )\n",
    "        emails.append(email)\n",
    "\n",
    "    return emails\n",
    "\n",
    "def build_dataset():\n",
    "    ham_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"ham\"),\n",
    "        label = 0\n",
    "    )\n",
    "    spam_emails = read_emails_dir(\n",
    "        path = os.path.join(ENRON_DATA_DIR_NAME, \"spam\"),\n",
    "        label = 1\n",
    "    )\n",
    "\n",
    "    return Dataset.encode(\n",
    "        ham_emails = ham_emails,\n",
    "        spam_emails = spam_emails\n",
    "    )\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    with open(Dataset.DATA_FILE_PATH, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def build_and_save_dataset():\n",
    "    if os.path.isfile(Dataset.DATA_FILE_PATH):\n",
    "        print(\"Dataset already processed!\")\n",
    "        return\n",
    "\n",
    "    print(\"Reading and processing emails!\")\n",
    "    dataset = build_dataset()\n",
    "    save_dataset(dataset)\n",
    "    print(\"Dataset created!\")\n",
    "\n",
    "download_tarfile()\n",
    "extract_tarfile()\n",
    "build_and_save_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "class DatasetSplitter:\n",
    "    @classmethod\n",
    "    def split(cls, dataset, ratio):\n",
    "        datasetA = cls._split(dataset, ratio, 0)\n",
    "        datasetB = cls._split(dataset, ratio, 1)\n",
    "        return (datasetA, datasetB)\n",
    "\n",
    "    @classmethod\n",
    "    def _split(cls, dataset, ratio, mode):\n",
    "        split_encoded_ham_emails, split_encoded_spam_emails = [], []\n",
    "        emails_pairs = [\n",
    "            (dataset.encoded_ham_emails, split_encoded_ham_emails),\n",
    "            (dataset.encoded_spam_emails, split_encoded_spam_emails)\n",
    "        ]\n",
    "\n",
    "        for (emails, split_emails) in emails_pairs:\n",
    "            for email in emails:\n",
    "                # This is a fancy way to pseudorandomly but\n",
    "                # deterministically select emails. That way we always\n",
    "                # pick the same set of emails for reproducability\n",
    "                # across program runs.\n",
    "                h = zlib.crc32(email.path.encode())\n",
    "                p = h / (2**32 - 1)\n",
    "                if (mode == 0 and p < ratio) or (mode == 1 and p >= ratio):\n",
    "                    split_emails.append(email)\n",
    "\n",
    "        return Dataset(\n",
    "            dataset.word_encoding_dictionary,\n",
    "            encoded_ham_emails = split_encoded_ham_emails,\n",
    "            encoded_spam_emails = split_encoded_spam_emails\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DatasetBatcher:\n",
    "    @classmethod\n",
    "    def batch(cls, d, batch_size):\n",
    "        emails = d.encoded_ham_emails + d.encoded_spam_emails\n",
    "        random.shuffle(emails)\n",
    "        idxs = range(0, len(emails), batch_size)\n",
    "        return [emails[idx:(idx + batch_size)] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin The Neural Network!\n",
    "\n",
    "At this point we can load the dataset. Every email is represented as a word vector and a label of whether it is spam. A position in the word vector is set to 1.0 if the corresponding word appears in the email; 0.0 if not.\n",
    "\n",
    "Only words with a reach of >100 have been kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = Dataset.get()\n",
    "training_dataset, test_dataset = DatasetSplitter.split(d, 0.80)\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 128 # Explained later\n",
    "NUM_HIDDEN_UNITS = 128\n",
    "VOCAB_SIZE = len(d.word_encoding_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "Here I have set the weight matrices to have random values. I sampled the values from the normal distribution, using a standard deviation that has been found to work well. This is called the \"Glorot initialization\" after the researcher who proposed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight matrices and biases\n",
    "THETA1 = np.random.normal(\n",
    "    scale = np.sqrt(2/ (NUM_HIDDEN_UNITS + VOCAB_SIZE)),\n",
    "    size = (NUM_HIDDEN_UNITS, VOCAB_SIZE)\n",
    ")\n",
    "B1 = np.zeros(NUM_HIDDEN_UNITS)\n",
    "THETA2 = np.random.normal(\n",
    "    scale = np.sqrt(2 / (1 + NUM_HIDDEN_UNITS)),\n",
    "    size = (1, NUM_HIDDEN_UNITS)\n",
    ")\n",
    "B2 = np.zeros(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**\n",
    "\n",
    "Here are a number of helpful functions that I will use. I will also need their derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions and their derivatives.\n",
    "\n",
    "def xe_error(prob, label):\n",
    "    if label == 1:\n",
    "        return -np.log(prob)\n",
    "    else:\n",
    "        return -np.log(1 - prob)\n",
    "\n",
    "def dxe_error(prob, label):\n",
    "    if label == 1:\n",
    "        return -(1 / prob)\n",
    "    else:\n",
    "        return (1 / (1 - prob))\n",
    "\n",
    "def logistic(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def dlogistic(z):\n",
    "    return logistic(z) * (1 - logistic(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**\n",
    "\n",
    "These functions report how well we are doing. I keep track of the average cross entropy, as well as the recall rate for a 1% false positive rate.\n",
    "\n",
    "We will train the model by minimizing the cross entropy, but ultimately we want to block emails, so it makes sense to report both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metrics to report on performance\n",
    "import itertools\n",
    "\n",
    "def dataset_avg_xe_error(d):\n",
    "    total_xe_error = 0.0\n",
    "    for email in itertools.chain(d.encoded_ham_emails, d.encoded_spam_emails):\n",
    "        forward_result = forward(email)\n",
    "        total_xe_error += xe_error(forward_result.a3[0], email.label)\n",
    "\n",
    "    return total_xe_error / len(d)\n",
    "\n",
    "FPR_RATE = 0.01\n",
    "def dataset_recall(d):\n",
    "    ham_scores = [forward(email).a3 for email in d.encoded_ham_emails]\n",
    "    ham_scores.sort()\n",
    "    ham_scores.reverse()\n",
    "    spam_scores = [forward(email).a3 for email in d.encoded_spam_emails]\n",
    "    \n",
    "    cutoff = ham_scores[int(FPR_RATE * len(ham_scores))]\n",
    "    num_false_positives = np.sum([\n",
    "        1 if score > cutoff else 0 for score in ham_scores\n",
    "    ])\n",
    "    num_true_positives = np.sum([\n",
    "        1 if score > cutoff else 0 for score in spam_scores\n",
    "    ])\n",
    "    \n",
    "    return (\n",
    "        num_false_positives / len(d.encoded_ham_emails),\n",
    "        num_true_positives / len(d.encoded_spam_emails)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass Calculation**\n",
    "\n",
    "This performs the forward pass by:\n",
    "\n",
    "1. Multiplying the input by the first weight matrix. The bias is added. This is $z^2$.\n",
    "2. Calculating $a^2 = \\sigma\\left(z^2\\right)$.\n",
    "3. Next, calculating $z^3$ by multiplying by the next weight matrix and addingn the bias.\n",
    "4. Calculating $a^3$ by applying the logistic function to $z^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to perform a forward pass\n",
    "from collections import namedtuple\n",
    "\n",
    "ForwardResult = namedtuple('ForwardResult', 'x z2 a2 z3 a3')\n",
    "\n",
    "def forward(email):\n",
    "    z2 = THETA1.dot(email.codes)\n",
    "    z2 += B1\n",
    "    a2 = logistic(z2)\n",
    "    \n",
    "    z3 = THETA2.dot(a2)\n",
    "    z3 += B2\n",
    "    a3 = logistic(z3)\n",
    "    \n",
    "    return ForwardResult(\n",
    "        x = email.codes,\n",
    "        z2 = z2,\n",
    "        a2 = a2,\n",
    "        z3 = z3,\n",
    "        a3 = a3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Pass Calculation**\n",
    "\n",
    "This pass calculates all the partial derivatives we will need. I must first perform a forward pass to calculate needed quantities.\n",
    "\n",
    "1. First I calculate $\\fpartial{E}{a^3}$, which just involves the derivative of the cross-entropy function, since $a^3$ is a direct input into the error function.\n",
    "2. To calculate $\\fpartial{E}{z^3}$, I use $\\fpartial{E}{z^3} = \\fpartial{E}{a^3}\\fpartial{a^3}{z^3}$. Since I know $a^3 = \\sigma\\left(z^3\\right)$, I know $\\fpartial{a^3}{z^3} = \\sigma\\left(z^3\\right)\\left(1 - \\sigma\\left(z^3\\right)\\right)$.\n",
    "3. To calculate $\\fpartial{E}{\\Theta^2_{1, j}}$, I know that this equals $\\fpartial{E}{z^3}\\fpartial{z^3}{\\Theta^2_{1, j}}$. I know $z_1^3 = b^2 + \\sum_j \\Theta^2_{1, j} a_j^2$. Therefore $\\fpartial{z^3}{\\Theta^2_{1, j}} = a_j^2$.\n",
    "    * Since for every $j$ we have $\\fpartial{E}{\\Theta^2_{1, j}} = \\fpartial{E}{z^3}a_j^2$, we may vectorize this operation.\n",
    "    * Thus, I have written $\\fpartial{E}{\\Theta^2} = \\fpartial{E}{z^3} a^2$.\n",
    "    * Actually, I wrote this in terms of the outer product, but since `dz3.shape = (1,)`, this is the same.\n",
    "    * The outer product will be useful in the next level of backpropagation.\n",
    "4. To calculate $\\fpartial{E}{b^2}$, this is $\\fpartial{E}{z^3}\\fpartial{z^3}{b^2}$. Since $z_1^3 = b^2 + \\sum_j \\Theta^2_{1, j} a_j^2$, we know $\\fpartial{z^3}{b^2} = 1$.\n",
    "5. To calculate $\\fpartial{E}{a_j^2}$, we break it into $\\fpartial{E}{z^3}\\fpartial{z^3}{a_j^2}$. We note that $z^3 = b^2 + \\sum_j \\Theta_{1, j}a_j^2$. Thus this partial is $\\Theta_{1, j}$.\n",
    "    * Since $\\fpartial{E}{a_j^2} = \\fpartial{E}{z^3} \\Theta_{1, j}$, we vectorize this operation.\n",
    "    * I wrote this as $\\Theta^{2\\intercal} \\fpartial{E}{z^3}$. This would have been useful if there were more than one unit in $z^3$.\n",
    "6. To calculate $\\fpartial{E}{z^2}$, I just multiply $\\fpartial{E}{a^2}$ by the derivative of the logistic function at $z^2$. This is the same as above.\n",
    "7. **TODO**: theta1...\n",
    "8. To calculate $\\fpartial{E}{b^1} = \\fpartial{E}{z^2} \\circ \\fpartial{z^2}{b^1}$. The first part is a vector. The second part is also a vector: of all ones. Why is $\\fpartial{z_i^2}{b_i^1}$ always equal to 1.0? I explained why for $b^2$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to perform a backward pass\n",
    "from collections import namedtuple\n",
    "\n",
    "BackwardResult = namedtuple('BackwardResult', 'da3, dz3, d_theta2, db2, da2, dz2, d_theta1, db1')\n",
    "\n",
    "def backward(forward_result, label):\n",
    "    da3 = dxe_error(forward_result.a3, label)\n",
    "    dz3 = dlogistic(forward_result.z3) * da3\n",
    "    d_theta2 = np.outer(dz3, forward_result.a2)\n",
    "    db2 = dz3\n",
    "    da2 = np.dot(THETA2.T, dz3)\n",
    "    dz2 = dlogistic(forward_result.z2) * da2\n",
    "    d_theta1 = np.outer(dz2, forward_result.x)\n",
    "    db1 = dz2\n",
    "    \n",
    "    return BackwardResult(\n",
    "        da3 = da3,\n",
    "        dz3 = dz3,\n",
    "        d_theta2 = d_theta2,\n",
    "        db2 = db2,\n",
    "        da2 = da2,\n",
    "        dz2 = dz2,\n",
    "        d_theta1 = d_theta1,\n",
    "        db1 = db1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulate Partial Derivatives**\n",
    "\n",
    "The `backward` function just works on a single example. To do gradient descent, we need to calculate the total partial derivative over a batch of examples. Luckily, that just involves summing up the partial derivatives calculated for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to collect the sum of partial values across a batch of examples\n",
    "from collections import namedtuple\n",
    "\n",
    "PartialsResult = namedtuple('PartialsResult', 'd_theta1, db1, d_theta2, db2')\n",
    "\n",
    "def partials(emails):\n",
    "    d_theta1 = np.zeros_like(THETA1)\n",
    "    db1 = np.zeros_like(B1)\n",
    "    d_theta2 = np.zeros_like(THETA2)\n",
    "    db2 = np.zeros_like(B2)\n",
    "\n",
    "    for email in emails:\n",
    "        forward_result = forward(email)\n",
    "        backward_result = backward(forward_result, email.label)\n",
    "        d_theta1 += backward_result.d_theta1\n",
    "        db1 += backward_result.db1\n",
    "        d_theta2 += backward_result.d_theta2\n",
    "        db2 += backward_result.db2\n",
    "\n",
    "    return PartialsResult(\n",
    "        d_theta1 = d_theta1,\n",
    "        db1 = db1,\n",
    "        d_theta2 = d_theta2,\n",
    "        db2 = db2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training!**\n",
    "\n",
    "Finally we can train the model! We'll proceed in batches of 128 emails. We'll run for 10 *epochs*. An *epoch* is a pass through the entire dataset.\n",
    "\n",
    "Each pass is made up of many batches. We make an update after each batch. This lets us make more small updates per epoch, which leads to quicker convergence.\n",
    "\n",
    "The downside is that the partial derivative calculated over a batch isn't exactly the same as the partial derivative calculated over the entire dataset. The batch partial derivative is a *noisy estimate* of the true partial derivative over the entire dataset.\n",
    "\n",
    "Still, making many slightly inaccurate steps is in practice better than making very few accurate updates. Since it takes a long time to run forward and backward passes on every email, if you only update once per epoch, it will take a lot more time to make as many updates as you would if you ran many batches per epoch.\n",
    "\n",
    "In this case, perfection is the enemy of the good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.41 0.40@fpr=0.0097\n",
      "2: 0.27 0.59@fpr=0.0097\n",
      "3: 0.21 0.69@fpr=0.0097\n",
      "4: 0.17 0.74@fpr=0.0097\n",
      "5: 0.15 0.75@fpr=0.0097\n",
      "6: 0.14 0.81@fpr=0.0097\n",
      "7: 0.12 0.82@fpr=0.0097\n",
      "8: 0.12 0.82@fpr=0.0097\n",
      "9: 0.11 0.82@fpr=0.0097\n",
      "10: 0.10 0.85@fpr=0.0097\n"
     ]
    }
   ],
   "source": [
    "def train(training_dataset, test_dataset, epochs, learning_rate):\n",
    "    global THETA1, B1, THETA2, B2\n",
    "    \n",
    "    batches = DatasetBatcher.batch(training_dataset, BATCH_SIZE)\n",
    "\n",
    "    for epoch_idx in range(1, 1 + epochs):\n",
    "        for batch in batches:\n",
    "            partials_result = partials(batch)\n",
    "            THETA1 -= (learning_rate / len(training_dataset)) * partials_result.d_theta1\n",
    "            B1 -= (learning_rate / len(training_dataset)) * partials_result.db1\n",
    "            THETA2 -= (learning_rate / len(training_dataset)) * partials_result.d_theta2\n",
    "            B2 -= (learning_rate / len(training_dataset)) * partials_result.db2\n",
    "\n",
    "        xe_error = dataset_avg_xe_error(test_dataset)\n",
    "        fpr, recall = dataset_recall(test_dataset)\n",
    "\n",
    "        print(\n",
    "            f\"{epoch_idx}: {xe_error:0.2f} {recall:0.2f}@fpr={fpr:0.4f}\"\n",
    "        )\n",
    "\n",
    "# NB: 10.0 is a weirdly high learning rate...\n",
    "train(training_dataset, test_dataset, epochs = 10, learning_rate = 10.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4181 samples, validate on 991 samples\n",
      "Epoch 1/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.37\n",
      " - 0s - loss: 1.1924 - acc: 0.6924 - val_loss: 0.6294 - val_acc: 0.7366\n",
      "Epoch 2/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.74\n",
      " - 0s - loss: 0.1897 - acc: 0.9294 - val_loss: 0.1347 - val_acc: 0.9586\n",
      "Epoch 3/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.85\n",
      " - 0s - loss: 0.1270 - acc: 0.9586 - val_loss: 0.1069 - val_acc: 0.9687\n",
      "Epoch 4/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.87\n",
      " - 0s - loss: 0.0976 - acc: 0.9687 - val_loss: 0.0926 - val_acc: 0.9657\n",
      "Epoch 5/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.90\n",
      " - 0s - loss: 0.0837 - acc: 0.9725 - val_loss: 0.0808 - val_acc: 0.9697\n",
      "Epoch 6/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.93\n",
      " - 0s - loss: 0.0721 - acc: 0.9766 - val_loss: 0.0736 - val_acc: 0.9707\n",
      "Epoch 7/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.91\n",
      " - 0s - loss: 0.0661 - acc: 0.9792 - val_loss: 0.0729 - val_acc: 0.9748\n",
      "Epoch 8/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.94\n",
      " - 0s - loss: 0.0593 - acc: 0.9809 - val_loss: 0.0637 - val_acc: 0.9788\n",
      "Epoch 9/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.94\n",
      " - 0s - loss: 0.0546 - acc: 0.9823 - val_loss: 0.0627 - val_acc: 0.9768\n",
      "Epoch 10/10\n",
      ">> FPR: 0.0097\n",
      ">> Recall: 0.94\n",
      " - 0s - loss: 0.0491 - acc: 0.9840 - val_loss: 0.0606 - val_acc: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d718198>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import keras.callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "\n",
    "training_dataset, validation_dataset = DatasetSplitter.split(d, 0.80)\n",
    "training_emails = training_dataset.encoded_ham_emails + training_dataset.encoded_spam_emails\n",
    "random.shuffle(training_emails)\n",
    "\n",
    "training_X = np.zeros(shape = (len(training_emails), VOCAB_SIZE))\n",
    "training_y = np.zeros(shape = len(training_emails))\n",
    "for idx, email in enumerate(training_emails):\n",
    "    training_X[idx, :] = email.codes\n",
    "    training_y[idx] = email.label\n",
    "\n",
    "validation_emails = validation_dataset.encoded_ham_emails + validation_dataset.encoded_spam_emails\n",
    "random.shuffle(validation_emails)\n",
    "validation_X = np.zeros(shape = (len(validation_emails), VOCAB_SIZE))\n",
    "validation_y = np.zeros(shape = len(validation_emails))\n",
    "for idx, email in enumerate(validation_emails):\n",
    "    validation_X[idx, :] = email.codes\n",
    "    validation_y[idx] = email.label\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(\n",
    "    128,\n",
    "    activation = 'sigmoid',\n",
    "    input_shape = (VOCAB_SIZE,)\n",
    "))\n",
    "model.add(Dense(\n",
    "    1,\n",
    "    activation = 'sigmoid'\n",
    "))\n",
    "\n",
    "FPR_RATE = 0.01\n",
    "class RecallRate(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        scores = self.model.predict(self.validation_data[0])\n",
    "        targets = self.validation_data[1]\n",
    "\n",
    "        ham_scores = scores[np.logical_not(targets)]\n",
    "        ham_scores = np.sort(ham_scores)[::-1]\n",
    "        cutoff = ham_scores[int(FPR_RATE * len(ham_scores))]\n",
    "        \n",
    "        num_false_positives = np.sum(\n",
    "            scores[np.logical_not(targets)] > cutoff\n",
    "        )\n",
    "        num_true_positives = np.sum(\n",
    "            scores[targets.astype(np.bool)] > cutoff\n",
    "        )\n",
    "        fpr = num_false_positives / np.sum(np.logical_not(targets))\n",
    "        recall = num_true_positives / np.sum(targets)\n",
    "        \n",
    "        print(f\">> FPR: {fpr:0.4f}\")\n",
    "        print(f\">> Recall: {recall:0.2f}\")\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = SGD(lr = 1.0),\n",
    "    metrics = ['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    training_X,\n",
    "    training_y,\n",
    "    epochs = 10,\n",
    "    batch_size = 128,\n",
    "    validation_data = (validation_X, validation_y),\n",
    "    callbacks = [RecallRate()],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
