{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing A Neural Network\n",
    "\n",
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\bigoh}[1]{\\mathcal{O}\\left(#1\\right)}\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\trans}[0]{^\\intercal}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/neural_network_diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculating Activations In A Neural Network\n",
    "\n",
    "We will denote the inputs to a neural network as $x$. These are the activations of the first layer in the network. In my example, $x$ is an $m$-dimensional vector.\n",
    "\n",
    "The second layer is the first *hidden layer*. In my example there are $n$ units in the second layer.\n",
    "\n",
    "Each unit of the second layer is effectively a linear regression model. For each unit, we need to do a weighted sum of the previous layer's activations to calculate a $z$ value, then we run this value through the logistic function to calculate $a = \\sigma(z)$.\n",
    "\n",
    "Consider the $i$th unit in the second layer. Then:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "z_i^2 &:= \\theta_i^1 x\n",
    "\\\\\n",
    "a_i^2 &:= \\sigma\\left(z_i^2\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "$\\theta_i^1$ is the list of $m$ weights to use to calculate the preactivation of the $i$th unit from a weighted sum of the $m$ values of $x$. Every unit in the layer will use a different $\\theta_i^1$ weight vector.\n",
    "\n",
    "To make things compact, we write all each $\\theta_i^1$ as the $i$th row of $n$-by-$m$ matrix denoted $\\Theta^1$. Using this matrix we can calculate $\\Theta^1 x$, which transforms the $m$-dimensional input into an $n$-dimensional output. This output is exactly $z^2$, the vector of individual $z_i^2$ values.\n",
    "\n",
    "It is common to denote the $i$-th column of $\\Theta^1$ by $\\Theta_{i, :}^1$. Here the $:$ symbol means \"all the columns.\"\n",
    "\n",
    "To calculate the $a^2$ values, the activations of the second layer, we just calculate $\\sigma\\left(z^2\\right)$, which applies the sigmoid to each individual coordinate of $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These activations are then used for the next step of the process, which proceeds the same way.\n",
    "\n",
    "In this case, the third layer happens to be the *output layer*. In my example it looks like there is only one output value. In principle there could be many outputs, or there could be more hidden layers.\n",
    "\n",
    "Everything works the same. There is a $1$-by-$n$ matrix $\\Theta^2$ that calculates 1-dimensional $z^3$ preactivation for the third layer by $\\Theta^2 a^2$. We again calculate $a^3 = \\sigma\\left(z^3\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "To train Neural Networks using Gradient Descent, we need to be able to calculate the partial derivatives of the error function with respect to the parameters.\n",
    "\n",
    "Calculating these partial derivatives is an exercise in repeated application of the chain rule.\n",
    "\n",
    "First, we calculate $\\fpartial{E}{a_1^3}$. For my purposes it is not important what the error function actually is. To calculate this derivative, we need to know the current value of $a_1^3$. This means that we first must do a *forward pass* to calculate this value on each example.\n",
    "\n",
    "Once we know $\\fpartial{E}{a_1^3}$, we can calculate $\\fpartial{E}{z_1^3}$ via the chain rule:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{E}{z_1^3}\n",
    "=\n",
    "\\fpartial{E}{a_1^3}\n",
    "\\fpartial{a_1^3}{z_1^3}\n",
    "=\n",
    "\\fpartial{E}{a_1^3}\n",
    "\\sigma'\\left(z_1^3\\right)\n",
    "=\n",
    "\\fpartial{E}{a_1^3}\n",
    "\\sigma\\left(z_1^3\\right)\n",
    "\\left( 1 - \\sigma\\left(z_1^3\\right) \\right)\n",
    "\\\\]\n",
    "\n",
    "We can do this because by definition $a_1^3 = \\sigma(z_1^3)$.\n",
    "\n",
    "Once we know $\\fpartial{E}{z_1^3}$, we can move on to calculate $\\fpartial{E}{\\Theta_{i, 1}^2}$. Here is how:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "z_1^3\n",
    "&= \\Theta^2 a^2\n",
    "\\\\\n",
    "&= \\sum_i \\Theta_{i, 1}^2 a_i^2\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Therefore:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{z_1^3}{\\Theta_{i, 1}^2} = a_i^2\n",
    "\\\\]\n",
    "\n",
    "Using this together with what we know about $\\fpartial{E}{z_1^3}$, we have:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{E}{\\Theta_{i, 1}^2}\n",
    "&=\n",
    "\\fpartial{E}{z_1^3}\n",
    "\\fpartial{z_1^3}{\\Theta_{i, 1}^2}\n",
    "\\\\\n",
    "&=\n",
    "\\fpartial{E}{a_1^3}\n",
    "\\sigma\\left(z_1^3\\right)\n",
    "\\left( 1 - \\sigma\\left(z_1^3\\right) \\right)\n",
    "a_i^2\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This is exactly the partial derivative we will need to use to update $\\Theta_{i, 1}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Keep Pushing Backward\n",
    "\n",
    "We also need to find $\\fpartial{E}{\\Theta_{i, j}^1}$. To do this, we need to first find $\\fpartial{E}{a_i^2}$.\n",
    "\n",
    "Now, via the chain rule we know we can write:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{E}{a_i^2}\n",
    "&=\n",
    "\\fpartial{E}{z_1^3}\n",
    "\\fpartial{z_1^3}{a_i^2}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "We have previous calculated $\\fpartial{E}{z_1^3}$. The new part is $\\fpartial{z_1^3}{a_i^2}$. But we know:\n",
    "\n",
    "\\\\[\n",
    "z_1^3\n",
    "=\n",
    "\\sum_i\n",
    "\\Theta_{i, 1}^2\n",
    "a_i^2\n",
    "\\\\]\n",
    "\n",
    "Therefore we have $\\fpartial{z_1^3}{a_i^2} = \\Theta_{i, 1}^2$. Thus we have:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{E}{a_i^2}\n",
    "=\n",
    "\\fpartial{E}{z_1^3}\n",
    "\\Theta_{i, 1}^2\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagating Further: Vectors!\n",
    "\n",
    "In the second layer there are multiple $a_i^2$ units. Let me write $\\fpartial{E}{a^2}$ to mean the vector of partial derivatives with respect to each $a_i^2$.\n",
    "\n",
    "Then, to calculate $\\fpartial{E}{z^2}$ (again, a vector of partial derivatives), I note:\n",
    "\n",
    "\\\\[\n",
    "a^2 = \\sigma\\left(z^2\\right)\n",
    "\\\\]\n",
    "\n",
    "Thus, I have:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{E}{z^2}\n",
    "&=\n",
    "\\fpartial{E}{a^2}\n",
    "\\circ\n",
    "\\fpartial{a^2}{z^2}\n",
    "\\\\\n",
    "&=\n",
    "\\fpartial{E}{a^2}\n",
    "\\circ\n",
    "\\fpartial{}{z^2} \\sigma\\left(z^2 \\right)\n",
    "\\\\\n",
    "&=\n",
    "\\fpartial{E}{a^2}\n",
    "\\circ\n",
    "\\sigma\\left(z^2\\right)\n",
    "\\circ\n",
    "\\left(1 - \\sigma\\left(z^2\\right)\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Note the $\\circ$ operation means coordinatewise multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partials With Respect To $\\Theta^1$\n",
    "\n",
    "Okay, unlike when we were calculating the partials for $\\Theta^2$, where multiplied $n$ activations $a^2$ from the previous layer to produce a single value $z^3$, now we have $m$ activations $a^1$ (aka, $x$) multiplied by an $m$-by-$n$ matrix $\\Theta^1$ to produce an $n$-dimensional vector $z^2$.\n",
    "\n",
    "So let's think about this! The entry $\\Theta_{i, j}^1$ multiplies $a_j^1$ to contribute a part of $z_i^2$. Therefore,\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{E}{\\Theta_{i, j}^1}\n",
    "&=\n",
    "\\fpartial{E}{z_i^2}\n",
    "\\fpartial{z_i^2}{\\Theta_{i, j}^1}\n",
    "\\\\\n",
    "&=\n",
    "\\fpartial{E}{z_i^2}\n",
    "a_j^1\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This formula shows how to calculate all of the matrix $\\fpartial{E}{\\Theta^1}$:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{E}{\\Theta_{i, :}^1}\n",
    "=\n",
    "\\fpartial{E}{z_i^2}\n",
    "\\circ\n",
    "a^1\n",
    "\\\\]\n",
    "\n",
    "Sometimes this is written using the *outer product* operation:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{E}{\\Theta^1}\n",
    "=\n",
    "\\fpartial{E}{z^2}\n",
    "\\otimes\n",
    "a^1\n",
    "\\\\]\n",
    "\n",
    "Honestly, I frequently forget the definition of the outer product, so I just have to re-derive the formula for this matrix of partial derivatives.\n",
    "\n",
    "You don't have to memorize any of these as formulas. You need to be able to apply the chain rule to figure them out when you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partials With Respect To $a^1$\n",
    "\n",
    "So $x = a^1$, so you don't need to calculate these partial derivatives, because you aren't allowed to change $x$, that's just your input.\n",
    "\n",
    "Let's do it anyway, because we haven't backpropagated across an $m$-by-$n$ matrix yet.\n",
    "\n",
    "So let's think. The value $a_j^1$ effects all the $z^2$ values, by virtue of the column $\\Theta_{:, j}^1$. Therefore:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "\\fpartial{E}{a_j^1}\n",
    "&=\n",
    "\\sum_{i = 1}^n\n",
    "\\fpartial{E}{z_i^2}\n",
    "\\Theta{i, j}\n",
    "\\\\\n",
    "&=\n",
    "\\fpartial{E}{z^2}\n",
    "\\cdot\n",
    "\\Theta{:, j}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Basically, to calculate $\\fpartial{E}{a_j^1}$, we take the dot product of $\\fpartial{E}{z^2}$ with the column $\\Theta{:, j}$. Normal matrix multiplication is dot products with rows, but this is with columns.\n",
    "\n",
    "That suggests maybe we can use a transpose of $\\Theta^1$ to write this in terms of regular matrix multiplication. We have:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{E}{a_j^1}\n",
    "=\n",
    "\\Theta^{1\\intercal}_{j, :}\n",
    "\\fpartial{E}{z^2}\n",
    "\\\\]\n",
    "\n",
    "Indeed, we can calculate all the $\\fpartial{E}{a^1}$ vector in one go:\n",
    "\n",
    "\\\\[\n",
    "\\fpartial{E}{a^1}\n",
    "=\n",
    "\\Theta\\trans\n",
    "\\fpartial{E}{z^2}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End!\n",
    "\n",
    "This is it! This is how neural network partial derivatives are calculated for the parameters $\\Theta^i$. Using these partial derivatives we can use normal gradient descent to train the model.\n",
    "\n",
    "We did it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
