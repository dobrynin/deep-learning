{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying To Capturing Nonlinear Effects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\bigoh}[1]{\\mathcal{O}\\left(#1\\right)}\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We've seen two kinds of *linear* classifier: (1) Naive Bayes and (2) Logistic Regression. Both kinds of classifier assign a weight to each feature, and classify an example by summing up the weights for the features that appear in the example.\n",
    "\n",
    "We've seen that Naive Bayes calculates a specific choice of weights, whereas when we train Logistic Regression, it is free to choose the weights. In that sense, Naive Bayes is a special case of Logistic Regression.\n",
    "\n",
    "Both kinds of model are linear. That means if feature $F_i$ is present, then you add in the weight $\\theta_i$ regardless of what other features are present. You do not add in any different weight for $F_i$ based on whether or not some other feature is present. $F_i$ always has the same incremental contribution no matter the other features.\n",
    "\n",
    "Note: the final probability calculated by a Logistic Regression model is not linear, because the probability is calculated by applying the nonlinear logistic function to the calculated log odds: it is the log odds which is linear in $x$. All the same, we call this a linear model because the effects of the $x_i$ are all linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These linear classifiers yield a *linear decision boundary*. The decision boundary is those values of $x$ where you believe $\\prob{Y = 1 \\condbar X = x} = \\prob{Y = 0 \\condbar X = x}$. On one side of the boundary you think it is more likely that $Y = 1$, and on the other that it is more likely that $Y = 0$.\n",
    "\n",
    "In that case:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "&&\n",
    "\\prob{Y = 1 \\condbar X = x}\n",
    "&=\n",
    "\\prob{Y = 0 \\condbar X = x}\n",
    "\\\\\n",
    "\\Rightarrow&&\n",
    "\\frac{\n",
    "    \\prob{Y = 1 \\condbar X = x}\n",
    "}{\n",
    "    \\prob{Y = 0 \\condbar X = x}\n",
    "}\n",
    "&=\n",
    "1\n",
    "\\\\\n",
    "\\Rightarrow&&\n",
    "e^{\n",
    "    \\theta_0\n",
    "    +\n",
    "    \\sum_i\n",
    "    \\theta_i x_i\n",
    "}\n",
    "&=\n",
    "1\n",
    "\\\\\n",
    "\\Rightarrow&&\n",
    "    \\theta_0\n",
    "    +\n",
    "    \\sum_i\n",
    "    \\theta_i x_i\n",
    "&=\n",
    "    0\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Since this last equation is linear in the $x_i$, it shows that the decision boundary is a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Of Nonlinear Effect\n",
    "\n",
    "Let me give you an example of something that can *not* be modeled with just linear effects. I will give you a silly example, then I will give you a good one :-)\n",
    "\n",
    "I want to make sure my roommate feeds my cats while I am away on vacation. The variable I want to predict is $Y$, where $Y = 1$ means my cats get fed, and $Y = 0$ means my cats don't get fed.\n",
    "\n",
    "I ask my roommate whether they will agree to feed the cats while I'm gone. Their reply is encoded as a variable $X_1$, where $X_1 = 1$ means they say yes they will feed the cats, and $X_1 = 0$ means they say no they will not.\n",
    "\n",
    "Because my roommate is not always entirely truthful, I will ask them \"Was your answer to my question about feeding the cats a lie?\" Luckily, even though my roommate is not always entirely truthful, they would never lie to me about lying. Their response is encoded as $X_2 = 1$ if they admit they were originally lying, and $X_2 = 0$ if not.\n",
    "\n",
    "What is happening here? The presence of the second feature is *reversing* the power of the first. If $X_1 = 1$ indicates my roommate will probably feed my cats when $X_2 = 0$, then it means the exact opposite when $X_2 = 1$.\n",
    "\n",
    "In fact, if I only know the value of $X_1$, that tells me *nothing* about whether my cats will get fed (assuming my roommate lies 50% of the time). So the \"individual\" weight of $X_1$ should be no evidence. Likewise $X_2$ alone conveys no information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I define the desired output $Y$ value in terms of $X_1$ and $X_2$? The kind of function we are trying to define is called *exclusive or*, abbreviated *xor*. An xor works like this:\n",
    "\n",
    "\\\\[\n",
    "0 ⊕ 0 := 0\n",
    "\\\\\n",
    "1 ⊕ 0 := 1\n",
    "\\\\\n",
    "0 ⊕ 1 := 1\n",
    "\\\\\n",
    "1 ⊕ 1 := 0\n",
    "\\\\]\n",
    "\n",
    "So the model I am trying to learn is like this $Y = X_1 \\oplus X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Linear Function Captures This Relationship\n",
    "\n",
    "In isolation, $X_1$ (and $X_2$) are meaningless. You cannot decompose the overall phenomenon into a combination of phenomena involving just $X_1$ and $X_2$.\n",
    "\n",
    "For instance, consider any linear function:\n",
    "\n",
    "\\\\[\n",
    "w_1 x_1 + w_2 x_2\n",
    "\\\\]\n",
    "\n",
    "Is there a setting of the $w_1, w_2$ values such that this equation is positive when $(x_1, x_2) = (0, 1)$ or $(1, 0)$, but negative when $(x_1, x_2) = (0, 0)$ or $(1, 1)$? The answer is there is not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE2tJREFUeJzt3X+MVeWdx/H3V8E/RI3blSoVUUlIu1aLdUeKbrOKP9Ah\nTWg3xegfqzEaAq2JmjURt1Gi/iOaNNVtV0L9sZo07UL6i2xBIjrGNtHKYIYfYm0pshGWVmpXQW1a\ncL/7x3kwk+kMM3PvYe69w/uV3Nzz4znP8xyPMx/OOc85E5mJJEnHtLoDkqT2YCBIkgADQZJUGAiS\nJMBAkCQVBoIkCaghECLijIjoiYhtEfFaRNw6SJmIiEciYntEbI6IC5ptV5JUrwk11HEQ+JfMfDUi\nTgQ2RsSzmbmtX5luYEb5fAF4tHxLktpE02cImbknM18t0/uB14HTBxSbDzydlZeBkyNiSrNtS5Lq\nU8cZwsci4izg88AvB6w6HXir3/yusmzPIHUsBBYCTJo06e8/85nP1NlFSRrXNm7c+IfMnNzItrUF\nQkScAPwQuC0z9zVaT2auAFYAdHV1ZW9vb009lKTxLyL+u9FtaxllFBETqcLge5n5o0GK7AbO6Dc/\ntSyTJLWJOkYZBfA48HpmfnOIYquB68too9nAe5n5V5eLJEmtU8clo38A/hnYEhF9Zdm/AtMAMnM5\nsAaYB2wHPgRurKFdSVKNmg6EzPwFEMOUSeDrzbYlSTpyfFJZkgQYCJKkwkCQJAEGgiSpMBAkSYCB\nIEkqDARJEmAgSJIKA0GSBBgIkqTCQJAkAQaCJKkwECRJgIEgSSoMBEkSYCBIkgoDQZIEGAiSpMJA\nkCQBBoIkqTAQJEmAgSBJKgwESRJgIEiSCgNBkgQYCJKkwkCQJAE1BUJEPBERb0fE1iHWXxoR70VE\nX/ncU0e7kqT6TKipnv8Avg08fZgyP8/ML9XUniSpZrWcIWTmi8Af66hLktQaY3kP4eKI2BwRayPi\ns2PYriRpBOq6ZDScV4Fpmfl+RMwDfgLMGKxgRCwEFgJMmzZtjLonSRqTM4TM3JeZ75fpNcDEiDhl\niLIrMrMrM7smT548Ft2TJDFGgRARp0VElOlZpd13xqJtSdLI1HLJKCK+D1wKnBIRu4ClwESAzFwO\nfBVYHBEHgT8B12Zm1tG2JKketQRCZl43zPpvUw1LlSS1KZ9UliQBBoIkqTAQJEmAgSBJKgwESRJg\nIEiSCgNBkgQYCJKkwkCQJAEGgiSpMBAkSYCBIEkqDARJEmAgSJIKA0GSBBgIkqTCQJAkAQaCJKkw\nECRJgIEgSSoMBEkSYCBotB58EHp66qmrp6eqTy23ZQvcfjvMng0nnAAR1ffs2dXyLVta3UONBQNB\no3PhhXDNNc2HQk9PVc+FF9bTLzVkxw6YOxe6u+Gkk2DZMti1Cz76qPpetqxa3t0NV11Vldf4ZSBo\ndObMgZUrmwuFQ2GwcmVVn1pi1SqYNasKhJ074d574ZJL4OST4Zhjqu9LLqmWv/kmXHllVX7Vqlb3\nXEfKhFZ3QB2ofyiM9pe6YdAWVq2CW2+F556DmTOHLz9xItxxRxUK3d3VsgULjmwfNfY8Q1BjGjlT\nMAzawo4dsHgxrF07sjDob+bMarvFi6uzBo0vBoIaN5pQMAzaxqJFsGTJ6MPgkJkz4c47q3o0vhgI\nas5IQsEwaBubN8O2bXDbbc3Vc/vtsHWro4/Gm1oCISKeiIi3I2LrEOsjIh6JiO0RsTkiLqijXbWJ\nw4WCYdBWnnwSbroJJjR593DCBLj55qo+jR91nSH8B3D1YdZ3AzPKZyHwaE3tql0MFgqGQdt56SW4\n7LJ66pozp6pP40ctgZCZLwJ/PEyR+cDTWXkZODkiptTRttpI/1C45x7DoA1t3dr4vYOBzj/fS0bj\nzVjdQzgdeKvf/K6y7K9ExMKI6I2I3r17945J51SjOXOqISj33199GwZt5YMPqgfN6nDiifDhh/XU\npfbQdjeVM3NFZnZlZtfkyZNb3R2NVk8PPPoo3H139V3Xay5Ui0mTYN++euravx+OP76eutQexioQ\ndgNn9JufWpZpPOl/z+C++5p/olm1O/dc2LSpnrr6+uC88+qpS+1hrAJhNXB9GW00G3gvM/eMUdsa\nC4PdQK7jNReq1UUXwfPP11NXT09Vn8aPuoadfh94Cfh0ROyKiJsiYlFEHHp0ZQ2wA9gOfBf4Wh3t\nqk0cbjSRodBWbrwRHn8cDhxorp4DB+Cxx6r6NH7U8i6jzLxumPUJfL2OttRmRjK0tJl3H6lWn/sc\nnHMOPPxw9W6iRn3rW9XlJy8ZjS9td1NZHWQ0zxl4ptA2li+HBx5o/F5CX1/1Wuzly+vtl1rPQFBj\nGnnozFBoC9OnVwPAurtHHwp9fTBvXrX92Wcfmf6pdQwEjV4zTyAbCm1hwYLqstHll8NDD8HBg4cv\nf+BAVe6KK6rtfPX1+GQgaHTqeB2FodAWFiyAV16B9evhzDNh6VJ44QV4993qL6a9+241v3QpnHVW\nVW7DBsNgPDMQNDobNtRzY/hQKGzYUE+/1JDp02HdOnjmmepBs7vugqlTqz+IM3VqNb9/f7V+3Tov\nE413UQ0Aak9dXV3Z29vb6m5IUseIiI2Z2dXItp4hSJIAA0GSVBgIkiTAQJAkFQaCJAkwECRJhYEg\nSQIMBElSYSBIkgADQZJUGAiSJMBAkCQVBoIkCTAQJEmFgSBJAgwESVJhIEiSAANBklQYCJIkwECQ\nJBUGgiQJqCkQIuLqiHgjIrZHxJJB1l8aEe9FRF/53FNHu5Kk+kxotoKIOBb4DnAlsAvYEBGrM3Pb\ngKI/z8wvNdueJOnIqOMMYRawPTN3ZOZfgB8A82uoV5I0huoIhNOBt/rN7yrLBro4IjZHxNqI+OxQ\nlUXEwojojYjevXv31tA9SdJIjNVN5VeBaZn5OeDfgJ8MVTAzV2RmV2Z2TZ48eYy6J0mqIxB2A2f0\nm59aln0sM/dl5vtleg0wMSJOqaFtSVJN6giEDcCMiDg7Io4DrgVW9y8QEadFRJTpWaXdd2poW5JU\nk6ZHGWXmwYi4BVgHHAs8kZmvRcSisn458FVgcUQcBP4EXJuZ2WzbkqT6RDv/Xu7q6sre3t5Wd0OS\nOkZEbMzMrka29UllSRJgIEiSCgNBkgQYCJKkwkCQJAEGgiSpMBAkSYCBIEkqDARJEmAgSJIKA0GS\nBBgIkqTCQJAkAQaCJKkwECRJgIEgSSoMBEkSYCBIkgoDQZIEGAiSpMJAUC22bIHbb4fZs+GEEyCi\n+p49u1q+ZUureygAHnwQenrqqaunp6pP44aBoKbs2AFz50J3N5x0EixbBrt2wUcfVd/LllXLu7vh\nqquq8mqhCy+Ea65pPhR6eqp6Lrywnn6pLRgIatiqVTBrVhUIO3fCvffCJZfAySfDMcdU35dcUi1/\n80248sqq/KpVre75UWzOHFi5srlQOBQGK1dW9WncmNDqDqgzrVoFt94Kzz0HM2cOX37iRLjjjioU\nururZQsWHNk+agj9Q2G0v9QNg3HNMwSN2o4dsHgxrF07sjDob+bMarvFi6uzBrVII2cKhsG4ZyBo\n1BYtgiVLRh8Gh8ycCXfeWdWjFhpNKBgGRwUDQaOyeTNs2wa33dZcPbffDlu3Ovqo5UYSCobBUaOW\nQIiIqyPijYjYHhFLBlkfEfFIWb85Ii6oo12NvSefhJtugglN3n2aMAFuvrmqTy12uFAwDI4qTQdC\nRBwLfAfoBs4BrouIcwYU6wZmlM9C4NFm21VrvPQSXHZZPXXNmVPVpzYwWCgYBkedOs4QZgHbM3NH\nZv4F+AEwf0CZ+cDTWXkZODkiptTQtsbY1q2N3zsY6PzzvWTUVvqHwj33GAZHoToC4XTgrX7zu8qy\n0ZYBICIWRkRvRPTu3bu3hu6pTh98UD1oVocTT4QPP6ynLtVkzpxqCNj991ffhsFRpe1uKmfmiszs\nysyuyZMnt7o7GmDSJNi3r5669u+H44+vpy7VpKcHHn0U7r67+q7rNRfqCHUEwm7gjH7zU8uy0ZZR\nBzj3XNi0qZ66+vrgvPPqqUs16H/P4L77mn+iWR2njkDYAMyIiLMj4jjgWmD1gDKrgevLaKPZwHuZ\nuaeGtjXGLroInn++nrp6eqr61AYGu4Fcx2su1FGaDoTMPAjcAqwDXgdWZuZrEbEoIg49erQG2AFs\nB74LfK3ZdtUaN94Ijz8OBw40V8+BA/DYY1V9arHDjSYyFI4qkZmt7sOQurq6sre3t9Xd0ABz51af\nO+5ovI6HHoL162Hduvr6pQaMdGipQ1A7RkRszMyuRrZtu5vKan/Ll8MDDzR+L6Gvr3ot9vLl9fZL\nozSaX/KeKRwVDASN2vTp1QCU7u7Rh0JfH8ybV21/9tlHpn8agUb+xW8ojHsGghqyYAE8/DBcfnl1\n+efgwcOXP3CgKnfFFdV2vvq6hZq5/GMojGsGghq2YAG88kp1L+DMM2HpUnjhBXj33eovpr37bjW/\ndCmcdVZVbsMGw6Cl6rgXYCiMWwaCmjJ9enVj+JlnqgfN7roLpk6t/iDO1KnV/P791fp167xM1HIb\nNtRzY/hQKGzYUE+/1BYcZSRJ44ijjCRJTTMQJEmAgSBJKgwESRJgIEiSCgNBkgQYCJKkwkCQJAEG\ngiSpMBAkSYCBIEkqDARJEmAgSJIKA0GSBBgIkqTCQJAkAQaCJKkwECRJgIEgSSoMBEkSYCBIkooJ\nzWwcEZ8A/hM4C9gJXJOZ/ztIuZ3AfuAj4GBmdjXTriSpfs2eISwBnsvMGcBzZX4oczLzfMNAktpT\ns4EwH3iqTD8FfLnJ+iRJLdJsIJyamXvK9O+AU4col8D6iNgYEQubbFOSdAQMew8hItYDpw2y6hv9\nZzIzIyKHqOaLmbk7Ij4JPBsRv8rMF4dobyGwEGDatGnDdU+SVJNhAyEzrxhqXUT8PiKmZOaeiJgC\nvD1EHbvL99sR8WNgFjBoIGTmCmAFQFdX11ABI0mqWbOXjFYDN5TpG4CfDiwQEZMi4sRD08BcYGuT\n7UqSatZsIDwAXBkRvwGuKPNExKciYk0pcyrwi4jYBLwC/Cwzn2myXUlSzZp6DiEz3wEuH2T5/wDz\nyvQOYGYz7UiSjjyfVJYkAQaCJKkwECRJgIEgSSoMBEkSYCBIkgoDQZIEGAiSpMJAkCQBBoIkqTAQ\nJEmAgSBJKgwESRJgIEiSCgNBkgQYCJKkwkCQJAEGgiSpMBAkSYCBIEkqDARJEmAgSJIKA0GSBBgI\nkqTCQJAkAQaCJKkwECRJgIEgSSqaCoSIWBARr0XE/0VE12HKXR0Rb0TE9ohY0kybkqQjo9kzhK3A\nPwEvDlUgIo4FvgN0A+cA10XEOU22K0mq2YRmNs7M1wEi4nDFZgHbM3NHKfsDYD6wrZm2JUn1aioQ\nRuh04K1+87uALwxVOCIWAgvL7J8jYusR7FsrnQL8odWdOILcv87m/nWuTze64bCBEBHrgdMGWfWN\nzPxpow0PJTNXACtK272ZOeS9iU42nvcN3L9O5/51rojobXTbYQMhM69otPJiN3BGv/mpZZkkqY2M\nxbDTDcCMiDg7Io4DrgVWj0G7kqRRaHbY6VciYhdwEfCziFhXln8qItYAZOZB4BZgHfA6sDIzXxth\nEyua6V+bG8/7Bu5fp3P/OlfD+xaZWWdHJEkdyieVJUmAgSBJKtomEMb7azAi4hMR8WxE/KZ8/80Q\n5XZGxJaI6Gtm+NhYGe54ROWRsn5zRFzQin42agT7d2lEvFeOV19E3NOKfjYiIp6IiLeHetZnHBy7\n4favk4/dGRHRExHbyu/NWwcpM/rjl5lt8QH+juqBiheAriHKHAv8FpgOHAdsAs5pdd9HuH8PAkvK\n9BJg2RDldgKntLq/I9ynYY8HMA9YCwQwG/hlq/td8/5dCvxXq/va4P79I3ABsHWI9R177Ea4f518\n7KYAF5TpE4Ff1/Gz1zZnCJn5ema+MUyxj1+DkZl/AQ69BqMTzAeeKtNPAV9uYV/qMpLjMR94Oisv\nAydHxJSx7miDOvn/t2Fl5ovAHw9TpJOP3Uj2r2Nl5p7MfLVM76cawXn6gGKjPn5tEwgjNNhrMAb+\nR2hXp2bmnjL9O+DUIcolsD4iNpbXeLSzkRyPTj5mI+37xeWUfG1EfHZsujYmOvnYjVTHH7uIOAv4\nPPDLAatGffzG4l1GHxvr12CMtcPtX/+ZzMyIGGq87xczc3dEfBJ4NiJ+Vf6lo/b0KjAtM9+PiHnA\nT4AZLe6TRqbjj11EnAD8ELgtM/c1W9+YBkKO89dgHG7/IuL3ETElM/eU07a3h6hjd/l+OyJ+THXZ\nol0DYSTHo62P2TCG7Xv/H8LMXBMR/x4Rp2TmeHhxWicfu2F1+rGLiIlUYfC9zPzRIEVGffw67ZJR\nJ78GYzVwQ5m+AfirM6KImBQRJx6aBuZS/c2JdjWS47EauL6MeJgNvNfv0lm7G3b/IuK0iOr97xEx\ni+pn6p0x7+mR0cnHblidfOxKvx8HXs/Mbw5RbPTHr9V3y/vdEf8K1TWuPwO/B9aV5Z8C1gy4c/5r\nqtEf32h1v0exf38LPAf8BlgPfGLg/lGNZtlUPq91wv4NdjyARcCiMh1UfyDpt8AWhhhB1q6fEezf\nLeVYbQJeBi5udZ9HsW/fB/YAB8rP3k3j7NgNt3+dfOy+SHW/cTPQVz7zmj1+vrpCkgR03iUjSdIR\nYiBIkgADQZJUGAiSJMBAkCQVBoIkCTAQJEnF/wPMtsAPpyFp9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116cd06d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    'x',\n",
    "    color = 'red',\n",
    "    markersize = 20,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    'o',\n",
    "    color = 'blue',\n",
    "    markerfacecolor = 'none',\n",
    "    markersize = 20,\n",
    ")\n",
    "\n",
    "plt.ylim(ymin = -1, ymax = +2)\n",
    "plt.xlim(xmin = -1, xmax = +2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above, you see a two red 'x's where the XOR function should be one, and two blue 'o's where the XOR function should be zero.\n",
    "\n",
    "You see that no line can have the 'x's on one side and 'o's on the other.\n",
    "\n",
    "Consider any linear function $z = w_0 + w_1 x + w_2 y $. This is the equation for a plane. Imagine the $z$ axis is coming straight out of the screen at you. Then where the plane crosses the $x, y$ plane (where $z = 0$), this crossing is a line.\n",
    "\n",
    "I can show you algebra style:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "&z = w_0 + w_1 x+ w_2 y \\quad\\text{and}\\quad z = 0\n",
    "\\\\\n",
    "\\Rightarrow\\quad&\n",
    "0 = w_0 + w_1 x + w_2 y\n",
    "\\\\\n",
    "\\Rightarrow\\quad&\n",
    "-w_2 y = w_0 + w_1 x\n",
    "\\\\\n",
    "\\Rightarrow\\quad&\n",
    "y = -\\frac{w_0}{w_2} + -\\frac{w_1}{w_2} x\n",
    "\\end{align}\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on one side of this line of intersection in the $x, y$ plane is where the $z$ values are positive, and on the other side are where the $z$ values are negative.\n",
    "\n",
    "Together, this shows that no linear function of $x$ and $y$ can properly seperate the red 'x's and the blue 'o's on different sides of the dividing line.\n",
    "\n",
    "This shows that no linear model can properly learn to distinguish points where the XOR function takes a positive value from points where the XOR function takes a zero value. In jargon, we say that the red 'x's and blue 'o's are not *linearly separable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Decision Boundary\n",
    "\n",
    "There is a simple nonlinear function that will seperate the XOR examples properly. It is:\n",
    "\n",
    "\\\\[\n",
    "z = x + y - 2xy - 0.1\n",
    "\\\\]\n",
    "\n",
    "You may want to check out the *decision boundary*:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "&z = x + y - 2xy - 0.33 \\quad\\text{and}\\quad z = 0\n",
    "\\\\\n",
    "\\Rightarrow\\quad&\n",
    "2xy - y = x - 0.33\n",
    "\\\\\n",
    "\\Rightarrow\\quad&\n",
    "(2x - 1)y = x - 0.33\n",
    "\\\\\n",
    "\\Rightarrow\\quad&\n",
    "y = \\frac{x - 0.33}{2x - 1}\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "This is not a linear function, because it has $x$ values on top and bottom of an irreducible fraction. We can graph this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfHeV8AqBCU0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116c38b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    'x',\n",
    "    color = 'red',\n",
    "    markersize = 20,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    'o',\n",
    "    color = 'blue',\n",
    "    markerfacecolor = 'none',\n",
    "    markersize = 20,\n",
    ")\n",
    "\n",
    "\n",
    "# Because the hyperbola graph is discontinuous, I graph it\n",
    "# in two parts.\n",
    "x = np.arange(-1, 1 / 2.0 - 0.01, 0.01)\n",
    "plt.plot(\n",
    "    x,\n",
    "    (x - 0.33) / (2.0 * x - 1),\n",
    "    color = 'purple'\n",
    ")\n",
    "\n",
    "x = np.arange(1 / 2.00 + 0.01, 2, 0.01)\n",
    "plt.plot(\n",
    "    x,\n",
    "    (x - 0.33) / (2.0 * x - 1),\n",
    "    color = 'purple'\n",
    ")\n",
    "\n",
    "plt.ylim(ymin = -1, ymax = +2)\n",
    "plt.xlim(xmin = -1, xmax = +2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a *nonlinear* decision that separates the red 'x's and blue 'o's. But a nonlinear decision boundary has to come from a nonlinear function. That's why we needed the $xy$ *cross term.* This cross term is only positive when $x$ and $y$ are *both* positive, or *both* negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Cross Terms\n",
    "\n",
    "One way to improve our linear regression and logistic regression techniques is to add in these cross terms. The easiest way to do ths is add new \"synthetic\" features. If I want to predict a target variable $Y$ from predictor variables $X_1, X_2, X_3, \\ldots X_N$, I can add a new feature called $X_{i, j} := X_i \\cdot X_j$ for every (unordered) pair of $i$ and $j$.\n",
    "\n",
    "Having added in these synthetic variables, I can then just do normal linear or logistic regression. The fundamental *statistical problem* is still linear. As far as the learner is concerned, it is still learning a linear function, it just now has a bunch of new variables, which the learner doesn't know are resulting in a nonlinear decision boundary.\n",
    "\n",
    "Some people call this extended form of linear regression: *polynomial regression*. My point is that even when using higher-order polynomials with square powers or cross terms or more, the underlying learning technique is still learning a linear model.\n",
    "\n",
    "In other words: the model is only nonlinear because you fed it features that you transformed nonlinearly. That was based on *your* choice or prior knowledge, not the model's choice or anything it \"learned.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Computation Required By Adding Higher Order Terms Isn't Scalable\n",
    "\n",
    "Because some variables might have interactions effects you want to capture, you can add in cross terms. But if you have $N$ dimensions of input dimensions, then that implies you will add in $\\frac{n(n - 1)}{2}$ cross terms.\n",
    "\n",
    "If you want to capture interactions of three variables, you'd have to add in $\\frac{n(n - 1)(n-2)}{6}$.\n",
    "\n",
    "In general, if you want $k$-way interactions, you need to add $\\bigoh{n^k}$ terms. This grows exponentially in $k$.\n",
    "\n",
    "What is the problem with more features? First of all, the more features you have, the more data you have, which means calculations are slower. If you are using gradient descent, each update requires work that is linear in the number of features. But if you start adding 2-way interactions, each update is now taking $\\bigoh{n^2}$ time. It has to take that long because every update step needs to update the $\\bigoh{n^2}$ *parameters*: one for each feature. In terms of both speed and memory usage (because you need to store all the parameters in memory), this will hit a brick wall very fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Order Terms Often Lead To Overfitting\n",
    "\n",
    "The other part that doesn't scale is that the more parameters you have, the more datapoints you will need to stop the model from *overfitting*.\n",
    "\n",
    "Let's remember what overfitting is. Overfitting is the model learning patterns that aren't really \"there.\" It is basically the model thinking that coincidences are a general rule. This \"knowledge\" will have no predictive value in the future.\n",
    "\n",
    "When do we avoid overfitting? When we have a lot of data and a simple model. For any \"fake\" knowledge, if I get more and more data, that fake knowledge will be eventually be exposed as wrong. That means it is harder to learn a spurious relationship with more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple model is important as well. The more complex my model, the more baroque a sham theory I can spin. The more complexity I have to work with, the more I can try to explain away data that would otherwise contradict a simpler version of my fake theory.\n",
    "\n",
    "Let me give a silly example. Say I propose that the Earth is flat. You ask: \"Why do the tops of ships appear at the horizon first?\" I say \"well, that is explained by a complicated theory of perspective.\" You ask: but why, at the summer solstice, was the sun directly overhead in Syene, but still cast a shadow in Alexandria? I say something about a complicated theory of refraction. You ask: why is it that if I travel east continuously, that I end up where I started? I say something about how you don't notice you're turning.\n",
    "\n",
    "My point is this: to sustain a false belief in the face of more and more evidence to the contrary, I need to add new twists to explain away that evidence. Adding more parameters to a model makes the model better able to capture complex relationships, but also more able to come up with complicated wrong theories that are hard to disprove.\n",
    "\n",
    "When we start considering adding $\\bigoh{n^k}$ features, we're talking about *a lot* of new features. All these new features will greatly increase the model complexity. So I better have *a lot* of data if I'm going to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useless Capacity\n",
    "\n",
    "Ultimately, if you really do have interaction effects, then you should have terms in your model to account for that. So adding some cross terms in may be the right thing to do.\n",
    "\n",
    "However: should you be adding *all* the cross terms? Some pairs of features will have strong interaction effects, but other pairs of features will have no interaction effect at all.\n",
    "\n",
    "Adding an interaction feature for a feature pair which don't interact at all is bad. This won't help your model do better. The additional capacity cannot be used to improve your predictions. The only thing this capacity can be used for is overfitting. It is burdensome: all else equal, you're going to need to have more data to prove that you shouldn't use this interaction feature.\n",
    "\n",
    "Even if there are weak pairwise interactions between some features, the ability to learn these weak interactions may be swamped by the perceived utility in using the feature for overfitting. In that case, it would be better to leave this term out.\n",
    "\n",
    "Therefore we come to two rules:\n",
    "\n",
    "1. If we have more data, we can add more interaction terms. If we have less data, we should add fewer interaction terms.\n",
    "2. We want to add an interaction term when there is a strong interaction effect. In that case, the term can improve future accuracy significantly. If the interaction effect is very small, then there is less benefit to adding the term. There is cost to adding any new interaction term: the ability to use the term for overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Domain Specific Knowledge vs Learned Knowledge\n",
    "\n",
    "If you are careful to avoid adding interaction terms that only add useless capacity: more power to you. That is a good thing and you should do it. This kind of knowledge you are injecting is called *prior domain specific knowledge*. Domain specific knowledge is what you know about the specific problem you are trying to solve. *Prior* knowledge is knowledge you already have about your problem, before you start trying to have an algorithm learn new things.\n",
    "\n",
    "You should exploit the prior knowledge you have; there is no point fighting with a hand tied behind your back. But domain specific knowledge is not general, so it can't always help you.\n",
    "\n",
    "We would like the model to be able to learn domain specific knowledge for itself. Instead of *you* injecting knowledge by choosing interaction pairs to include, we want the *model* to learn that, without going nuts and seeing spurious relationships that don't really exist.\n",
    "\n",
    "That's what motivates us to consider new kinds of models that can capture nonlinear relationships. Enter the neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
