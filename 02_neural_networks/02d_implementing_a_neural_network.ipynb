{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing A Neural Network\n",
    "\n",
    "**(Run this cell to define useful Latex macros)**\n",
    "\\\\[\n",
    "\\newcommand{\\bigoh}[1]{\\mathcal{O}\\left(#1\\right)}\n",
    "\\newcommand{\\card}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\condbar}[0]{\\,\\big|\\,}\n",
    "\\newcommand{\\eprob}[1]{\\widehat{\\text{Pr}}\\left[#1\\right]}\n",
    "\\newcommand{\\norm}[1]{\\left\\lvert\\left\\lvert#1\\right\\rvert\\right\\rvert}\n",
    "\\newcommand{\\prob}[1]{\\text{Pr}\\left[#1\\right]}\n",
    "\\newcommand{\\pprob}[2]{\\text{Pr}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\fpartial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/neural_network_diagram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculating Activations In A Neural Network\n",
    "\n",
    "We will denote the inputs to a neural network as $x$. These are the activations of the first layer in the network. In my example, $x$ is an $m$-dimensional vector.\n",
    "\n",
    "The second layer is the first *hidden layer*. In my example there are $n$ units in the second layer.\n",
    "\n",
    "Each unit of the second layer is effectively a linear regression model. For each unit, we need to do a weighted sum of the previous layer's activations to calculate a $z$ value, then we run this value through the logistic function to calculate $a = \\sigma(z)$.\n",
    "\n",
    "Consider the $i$th unit in the second layer. Then:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "z_i^2 &:= \\theta_i^1 x\n",
    "\\\\\n",
    "a_i^2 &:= \\sigma\\left(z_i^2\\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "$\\theta_i^1$ is the list of $m$ weights to use to calculate the preactivation of the $i$th unit from a weighted sum of the $m$ values of $x$. Every unit in the layer will use a different $\\theta_i^1$ weight vector.\n",
    "\n",
    "To make things compact, we write all each $\\theta_i^1$ as the $i$th row of $n$-by-$m$ matrix denoted $\\Theta^1$. Using this matrix we can calculate $\\Theta^1 x$, which transforms the $m$-dimensional input into an $n$-dimensional output. This output is exactly $z^2$, the vector of individual $z_i^2$ values.\n",
    "\n",
    "It is common to denote the $i$-th column of $\\Theta^1$ by $\\Theta_{i, :}^1$. Here the $:$ symbol means \"all the columns.\"\n",
    "\n",
    "To calculate the $a^2$ values, the activations of the second layer, we just calculate $\\sigma\\left(z^2\\right)$, which applies the sigmoid to each individual coordinate of $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These activations are then used for the next step of the process, which proceeds the same way.\n",
    "\n",
    "In this case, the third layer happens to be the *output layer*. In my example it looks like there is only one output value. In principle there could be many outputs, or there could be more hidden layers.\n",
    "\n",
    "Everything works the same. There is a $1$-by-$n$ matrix $\\Theta^2$ that calculates 1-dimensional $z^3$ preactivation for the third layer by $\\Theta^2 a^2$. We again calculate $a^3 = \\sigma\\left(z^3\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
