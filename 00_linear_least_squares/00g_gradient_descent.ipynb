{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The problem with the methods we've look at thus far is that they all require inverting the Hessian matrix. This can be very slow.\n",
    "\n",
    "Here is a simpler approach. Consider $\\nabla E(\\hat\\theta_{i, 0}, \\hat\\theta_{i, 1})$. This is the vector of partial derivatives. Let's consider a change $(\\Delta \\hat\\theta_0, \\Delta \\hat\\theta_1)$. Then, using the tangent line approximation to the quadratic surface, we should have:\n",
    "\n",
    "\\\\[\n",
    "\\Delta E\n",
    "\\approx\n",
    "    \\nabla E(\\hat\\theta_{i, 0}, \\hat\\theta_{i, 1}) \\cdot (\\Delta \\hat\\theta_0, \\Delta \\hat\\theta_1)\n",
    "= \\frac{\\partial E}{\\partial \\hat\\theta_0} \\Delta\\hat\\theta_0\n",
    "  + \\frac{\\partial E}{\\partial \\hat\\theta_1} \\Delta\\hat\\theta_1\n",
    "\\\\]\n",
    "\n",
    "Now, this is only an approximation to $\\Delta E$, because the error surface is quadratic, not linear. Therefore, as we change $\\hat\\theta_0, \\hat\\theta_1$, the partial derivatives will change. Still, just like $f'(x)$ is the slope of the line tangent to $f$ at $x$, $\\nabla E(\\hat\\theta)$ is the gradient of the linear surface tangent to $E$ at $\\hat\\theta$. (I wrote $\\hat\\theta$ which is the vector version of $(\\hat\\theta_0, \\hat\\theta_1)$.)\n",
    "\n",
    "In other words, $\\nabla E(\\theta)$ gives us the best linear approximation to the quadratic surface. For small $\\Delta\\theta$ the approximation should stay pretty good.\n",
    "\n",
    "Now, remember what we want to do: we want to find a minimum of the error surface $E$. But instead of trying to make a big jump to try to zero the partial derivatives, why don't we just try to make a small step in the downhill direction of the quadratic surface?\n",
    "\n",
    "For instance, any update $\\Delta\\theta$ where we have $\\nabla E(\\theta) \\cdot \\Delta\\theta < 0$ should reduce the error, provided $\\Delta\\theta$ is small enough that the approximation to $\\Delta E$ is still good.\n",
    "\n",
    "Let's say we want to take a step of length $\\epsilon$ units. Since $||u|| = \\sqrt{\\sum u_i^2}$, let's look for a $u$ where $||u|| = 1$, and take a step of $\\Delta\\theta = \\epsilon u$. This will have the right length.\n",
    "\n",
    "What direction $u$ should we move in? Well, first note that any direction where $\\nabla E(\\theta) \\cdot u > 0$ is heading *uphill* on the error surface.\n",
    "\n",
    "On the other hand, $\\nabla E(\\theta) \\cdot ||u|| = 0$ means you are traveling sideways on the error surface. A $u$ like this is *parallel to the contour* at $\\theta$. That's because when you're on a contour line, if you move along that line, you don't change your height. Since moving in the direction $u$ doesn't change your height, it must be the direction of the contour line.\n",
    "\n",
    "So we know you want $\\nabla E(\\theta) \\cdot u < 0$. In fact, we want this to be as negative as possible: that would be the direction most downhill.\n",
    "\n",
    "* Show them how constrained optimization on a circle is easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
