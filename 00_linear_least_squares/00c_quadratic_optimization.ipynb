{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.10: Univariate quadratic is easy!\n",
    "\n",
    "Let's go back to our squared error. It's a super ideal case for Newton's method. A quadratic function always has a single stationary point. That's because the derivative of a quadratic equation is a linear equation, and a line will cross the $x$-axis exactly once!\n",
    "\n",
    "Since the squared error is always positive, then this point must be a global minimum. So we don't have to worry about converging to a local maximum, or a local minimum that isn't a global minimum.\n",
    "\n",
    "In fact, if we're optimizing a univariate quadratic function, this is ridiculously simple. Note:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "f(x) &= a x^2 + bx + c\\\\\n",
    "\\frac{\\partial f}{\\partial x} &= 2a x + b\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x^2} &= 2a\\\\\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Now look at that. Newton's method in optimization says you should find the zero of $f'$. The way it does this is by approximating $f'$ with a tangent line at $x_i$ and seeing where that approximation hits zero.\n",
    "\n",
    "But in the case of a quadratic function, the first derivative truly *is* a line. So that means that the calculated stationary point isn't an approximation: it's perfectly accurate!\n",
    "\n",
    "Therefore, when optimizing a quadratic error function of a single variable, we will jump immediately to the global minimum on the first step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.11: Finding $\\hat\\theta_1$ given $\\hat\\theta_0$\n",
    "\n",
    "So we've learned how to minimize a univariate quadratic function. Let's do it to calculate the best $\\hat\\theta_1$, assuming we already know that $\\hat\\theta_0 = 100.0$. Now that would be cheating in real life, but I want to show a practical example where Newton's method works.\n",
    "\n",
    "To do this we need not only the first, but second partial derivative with respect to $\\hat\\theta_0$. Note:\n",
    "\n",
    "\\\\[\n",
    "\\begin{alignat*}{3}\n",
    "\\frac{\\partial E}{\\partial \\hat\\theta_1} &= \\sum_{i=1}^N 2 \\left(\n",
    "                                              \\left( \\hat\\theta_0 + \\hat\\theta_1 x_i \\right) - y_i\n",
    "                                            \\right) \\big( x_i \\big) &&= 0\\\\\n",
    "\\frac{\\partial^2 E}{\\partial \\hat\\theta_1^2} &= \\sum_{i=1}^N 2 \\big( x_i \\big) \\big( x_i \\big) &&= 0\n",
    "\\end{alignat*}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-be37170d3128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDifferentiableLinearModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearModel' is not defined"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "import matplotlib.animation\n",
    "import time\n",
    "\n",
    "class DifferentiableLinearModel(LinearModel):\n",
    "    def __init__(self, theta0, theta1):\n",
    "        super().__init__(theta0, theta1)\n",
    "\n",
    "    def improve_theta1(self, dataset):\n",
    "        first_error_deriv_wrt_theta1 = np.sum(\n",
    "            2 * (dataset.y - self(dataset.x)) * (-dataset.x)\n",
    "        )\n",
    "        second_error_derivative_wrt_theta1 = np.sum(2 * (-dataset.x) * (-dataset.x))\n",
    "\n",
    "        self.theta1 = self.theta1 - (first_error_deriv_wrt_theta1 / second_error_derivative_wrt_theta1)\n",
    "\n",
    "class FindTheta0Animation:\n",
    "    NUM_STEPS = 4\n",
    "    SLEEP = 2000\n",
    "    \n",
    "    @staticmethod\n",
    "    def frame(ax, dataset, generator_model, step_idx):\n",
    "        ax.clear()\n",
    "\n",
    "        x_values = np.arange(0, 100, 1.0)\n",
    "        average_sse = generator_model.average_sse(dataset)\n",
    "\n",
    "        dataset.plot(ax)\n",
    "        generator_model.plot(ax, x_values, \"g-\")\n",
    "        ax.set_title(f\"Step #{step_idx} | theta0: {generator_model.theta1:0.2f} | Avg SSE: {average_sse:0.2f}\")\n",
    "\n",
    "        generator_model.improve_theta1(dataset)\n",
    "\n",
    "    @classmethod\n",
    "    def run(cls, dataset, fixed_theta0):\n",
    "        figure = plt.figure()\n",
    "        ax = figure.add_subplot(1, 1, 1)\n",
    "\n",
    "        generator_model = DifferentiableLinearModel(theta0 = fixed_theta0, theta1 = 0.0)\n",
    "        frame_fn = lambda step_idx: cls.frame(ax, dataset, generator_model, step_idx)\n",
    "        animation = matplotlib.animation.FuncAnimation(\n",
    "            figure,\n",
    "            frame_fn,\n",
    "            frames = cls.NUM_STEPS,\n",
    "            interval = cls.SLEEP,\n",
    "            init_func = lambda: None\n",
    "        )\n",
    "        \n",
    "        plt.close(figure)\n",
    "\n",
    "        IPython.display.display(IPython.display.HTML(animation.to_html5_video()))\n",
    "\n",
    "FindTheta0Animation.run(DATASET, fixed_theta0 = 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
